{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSYwm77YoUBY",
        "outputId": "ac4d9bd1-5771-4b58-caf5-42ca9e992ee1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cloning into 'DAWN_WACV2020'...\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/mxbastidasr/DAWN_WACV2020.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOSWOeTMoWqT",
        "outputId": "acfbf060-b534-42a9-a725-f78b72e23b52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/DAWN_WACV2020\n"
          ]
        }
      ],
      "source": [
        "%cd /content/DAWN_WACV2020"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvKbh6suob4f",
        "outputId": "d8539f2b-a84b-40d4-d38a-c778e08c8451"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 169M/169M [00:01<00:00, 87.4MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 50000 images\n",
            "Test: 10000 images\n"
          ]
        }
      ],
      "source": [
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "testset  = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "print(\"Train:\", len(trainset), \"images\")\n",
        "print(\"Test:\", len(testset), \"images\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "H4dZlbUJpCHn",
        "outputId": "72a35772-7d59-456d-a730-b7a2a4f91e51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ignore tensorboard logger\n",
            "Launch...\n",
            "['train.py', '--epochs', '150', '-b', '64', '--lr', '0.03', '--name', 'dwnn_cifar-100_32', '--lrdecay', '150', '225', '--database', 'cifar-100', '--tempdir', '/root/tmp', 'dawn', '--regu_detail', '0.1', '--regu_approx', '0.1', '--levels', '3', '--first_conv', '128']\n",
            "DAWN:\n",
            "- first conv: 128\n",
            "- image size: 32\n",
            "- nb levels : 3\n",
            "- levels U/P: [2, 1]\n",
            "- channels:  3\n",
            "Final channel: 1280\n",
            "Final size   : 4\n",
            "Number of model parameters            : 2,646,500\n",
            "Number of *trainable* model parameters: 2,646,500\n",
            "Learning rate:  0.03\n",
            "Epoch: [0][0/782]\tTime 0.890 (0.890)\tLoss (Class) 5.4526 (5.4526)\tLoss (Regu) 0.8434 (0.8434)\tPrec@1 0.000 (0.000)\tPrec@5 7.812 (7.812)\n",
            "Epoch: [0][50/782]\tTime 0.024 (0.043)\tLoss (Class) 4.8828 (5.2249)\tLoss (Regu) 0.6114 (0.6912)\tPrec@1 4.688 (3.615)\tPrec@5 20.312 (14.124)\n",
            "Epoch: [0][100/782]\tTime 0.032 (0.034)\tLoss (Class) 4.6778 (4.9706)\tLoss (Regu) 0.5631 (0.6387)\tPrec@1 4.688 (5.167)\tPrec@5 20.312 (18.502)\n",
            "Epoch: [0][150/782]\tTime 0.023 (0.031)\tLoss (Class) 4.7089 (4.8262)\tLoss (Regu) 0.5163 (0.6055)\tPrec@1 6.250 (6.612)\tPrec@5 28.125 (22.092)\n",
            "Epoch: [0][200/782]\tTime 0.037 (0.029)\tLoss (Class) 4.2436 (4.7217)\tLoss (Regu) 0.4744 (0.5787)\tPrec@1 6.250 (7.400)\tPrec@5 32.812 (23.966)\n",
            "Epoch: [0][250/782]\tTime 0.022 (0.029)\tLoss (Class) 4.3268 (4.6351)\tLoss (Regu) 0.4592 (0.5562)\tPrec@1 10.938 (8.093)\tPrec@5 34.375 (25.809)\n",
            "Epoch: [0][300/782]\tTime 0.023 (0.028)\tLoss (Class) 4.0445 (4.5618)\tLoss (Regu) 0.4291 (0.5377)\tPrec@1 14.062 (8.830)\tPrec@5 43.750 (27.471)\n",
            "Epoch: [0][350/782]\tTime 0.023 (0.028)\tLoss (Class) 4.3535 (4.4960)\tLoss (Regu) 0.4231 (0.5211)\tPrec@1 9.375 (9.482)\tPrec@5 32.812 (28.846)\n",
            "Epoch: [0][400/782]\tTime 0.024 (0.027)\tLoss (Class) 3.8553 (4.4412)\tLoss (Regu) 0.3993 (0.5069)\tPrec@1 18.750 (10.119)\tPrec@5 43.750 (29.960)\n",
            "Epoch: [0][450/782]\tTime 0.022 (0.027)\tLoss (Class) 3.9110 (4.3875)\tLoss (Regu) 0.3871 (0.4942)\tPrec@1 18.750 (10.705)\tPrec@5 37.500 (31.170)\n",
            "Epoch: [0][500/782]\tTime 0.023 (0.027)\tLoss (Class) 3.6922 (4.3463)\tLoss (Regu) 0.3678 (0.4824)\tPrec@1 17.188 (11.206)\tPrec@5 56.250 (32.048)\n",
            "Epoch: [0][550/782]\tTime 0.022 (0.027)\tLoss (Class) 3.8237 (4.3026)\tLoss (Regu) 0.3608 (0.4718)\tPrec@1 14.062 (11.675)\tPrec@5 40.625 (32.937)\n",
            "Epoch: [0][600/782]\tTime 0.032 (0.027)\tLoss (Class) 3.4104 (4.2607)\tLoss (Regu) 0.3521 (0.4620)\tPrec@1 29.688 (12.196)\tPrec@5 54.688 (33.915)\n",
            "Epoch: [0][650/782]\tTime 0.024 (0.027)\tLoss (Class) 3.8697 (4.2210)\tLoss (Regu) 0.3439 (0.4531)\tPrec@1 14.062 (12.651)\tPrec@5 40.625 (34.733)\n",
            "Epoch: [0][700/782]\tTime 0.033 (0.026)\tLoss (Class) 3.5664 (4.1851)\tLoss (Regu) 0.3357 (0.4449)\tPrec@1 21.875 (13.109)\tPrec@5 45.312 (35.552)\n",
            "Epoch: [0][750/782]\tTime 0.023 (0.026)\tLoss (Class) 3.7974 (4.1544)\tLoss (Regu) 0.3302 (0.4371)\tPrec@1 17.188 (13.449)\tPrec@5 40.625 (36.189)\n",
            "Test: [0/157]\tTime 0.109 (0.109)\tLoss (Class) 3.6905 (3.6905)\tLoss (Regu) 0.3125 (0.3125)\tPrec@1 18.750 (18.750)\tPrec@5 46.875 (46.875)\n",
            "Test: [50/157]\tTime 0.008 (0.010)\tLoss (Class) 3.7968 (3.5748)\tLoss (Regu) 0.3093 (0.3148)\tPrec@1 15.625 (20.895)\tPrec@5 34.375 (48.958)\n",
            "Test: [100/157]\tTime 0.008 (0.009)\tLoss (Class) 3.4359 (3.6138)\tLoss (Regu) 0.3128 (0.3150)\tPrec@1 21.875 (20.514)\tPrec@5 50.000 (47.757)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 3.5382 (3.5927)\tLoss (Regu) 0.3154 (0.3161)\tPrec@1 28.125 (20.644)\tPrec@5 48.438 (48.520)\n",
            " * Train[13.688 %, 36.638 %, 4.134 loss] Val [20.670 %, 48.600%, 3.591 loss] Best: 20.670 %\n",
            "Time for 0 / 150 22.283833503723145\n",
            "Learning rate:  0.03\n",
            "Epoch: [1][0/782]\tTime 0.142 (0.142)\tLoss (Class) 3.4425 (3.4425)\tLoss (Regu) 0.3178 (0.3178)\tPrec@1 17.188 (17.188)\tPrec@5 53.125 (53.125)\n",
            "Epoch: [1][50/782]\tTime 0.023 (0.027)\tLoss (Class) 3.7542 (3.5781)\tLoss (Regu) 0.3093 (0.3200)\tPrec@1 17.188 (21.170)\tPrec@5 54.688 (49.112)\n",
            "Epoch: [1][100/782]\tTime 0.022 (0.026)\tLoss (Class) 3.2607 (3.5621)\tLoss (Regu) 0.3073 (0.3151)\tPrec@1 28.125 (20.838)\tPrec@5 64.062 (48.731)\n",
            "Epoch: [1][150/782]\tTime 0.023 (0.026)\tLoss (Class) 3.6255 (3.5379)\tLoss (Regu) 0.2994 (0.3120)\tPrec@1 21.875 (21.285)\tPrec@5 45.312 (49.524)\n",
            "Epoch: [1][200/782]\tTime 0.023 (0.025)\tLoss (Class) 3.3207 (3.5355)\tLoss (Regu) 0.2994 (0.3080)\tPrec@1 23.438 (21.377)\tPrec@5 54.688 (49.782)\n",
            "Epoch: [1][250/782]\tTime 0.023 (0.025)\tLoss (Class) 3.5091 (3.5137)\tLoss (Regu) 0.2885 (0.3048)\tPrec@1 23.438 (21.750)\tPrec@5 51.562 (50.255)\n",
            "Epoch: [1][300/782]\tTime 0.023 (0.024)\tLoss (Class) 3.3604 (3.4970)\tLoss (Regu) 0.2804 (0.3017)\tPrec@1 25.000 (21.968)\tPrec@5 50.000 (50.571)\n",
            "Epoch: [1][350/782]\tTime 0.022 (0.024)\tLoss (Class) 3.4179 (3.4787)\tLoss (Regu) 0.2798 (0.2988)\tPrec@1 20.312 (22.151)\tPrec@5 42.188 (51.011)\n",
            "Epoch: [1][400/782]\tTime 0.025 (0.024)\tLoss (Class) 3.4306 (3.4612)\tLoss (Regu) 0.2790 (0.2962)\tPrec@1 17.188 (22.362)\tPrec@5 56.250 (51.344)\n",
            "Epoch: [1][450/782]\tTime 0.023 (0.024)\tLoss (Class) 3.3179 (3.4424)\tLoss (Regu) 0.2647 (0.2940)\tPrec@1 25.000 (22.648)\tPrec@5 54.688 (51.826)\n",
            "Epoch: [1][500/782]\tTime 0.023 (0.024)\tLoss (Class) 3.2287 (3.4289)\tLoss (Regu) 0.2671 (0.2915)\tPrec@1 21.875 (22.867)\tPrec@5 54.688 (52.049)\n",
            "Epoch: [1][550/782]\tTime 0.023 (0.024)\tLoss (Class) 3.4621 (3.4178)\tLoss (Regu) 0.2614 (0.2889)\tPrec@1 20.312 (23.009)\tPrec@5 51.562 (52.237)\n",
            "Epoch: [1][600/782]\tTime 0.023 (0.024)\tLoss (Class) 3.1503 (3.4046)\tLoss (Regu) 0.2562 (0.2865)\tPrec@1 28.125 (23.217)\tPrec@5 59.375 (52.493)\n",
            "Epoch: [1][650/782]\tTime 0.023 (0.024)\tLoss (Class) 3.0825 (3.3946)\tLoss (Regu) 0.2550 (0.2839)\tPrec@1 26.562 (23.370)\tPrec@5 64.062 (52.712)\n",
            "Epoch: [1][700/782]\tTime 0.022 (0.024)\tLoss (Class) 3.2472 (3.3772)\tLoss (Regu) 0.2551 (0.2818)\tPrec@1 31.250 (23.636)\tPrec@5 56.250 (53.043)\n",
            "Epoch: [1][750/782]\tTime 0.023 (0.024)\tLoss (Class) 3.6339 (3.3668)\tLoss (Regu) 0.2473 (0.2796)\tPrec@1 23.438 (23.797)\tPrec@5 46.875 (53.300)\n",
            "Test: [0/157]\tTime 0.109 (0.109)\tLoss (Class) 2.9636 (2.9636)\tLoss (Regu) 0.2509 (0.2509)\tPrec@1 32.812 (32.812)\tPrec@5 56.250 (56.250)\n",
            "Test: [50/157]\tTime 0.007 (0.010)\tLoss (Class) 2.9211 (3.0662)\tLoss (Regu) 0.2533 (0.2532)\tPrec@1 32.812 (29.075)\tPrec@5 60.938 (59.559)\n",
            "Test: [100/157]\tTime 0.008 (0.009)\tLoss (Class) 3.2562 (3.0750)\tLoss (Regu) 0.2483 (0.2525)\tPrec@1 29.688 (29.378)\tPrec@5 48.438 (59.483)\n",
            "Test: [150/157]\tTime 0.007 (0.008)\tLoss (Class) 2.9584 (3.0683)\tLoss (Regu) 0.2557 (0.2527)\tPrec@1 32.812 (29.025)\tPrec@5 56.250 (59.613)\n",
            " * Train[23.936 %, 53.468 %, 3.359 loss] Val [28.940 %, 59.630%, 3.069 loss] Best: 28.940 %\n",
            "Time for 1 / 150 20.328980207443237\n",
            "Learning rate:  0.03\n",
            "Epoch: [2][0/782]\tTime 0.145 (0.145)\tLoss (Class) 2.9444 (2.9444)\tLoss (Regu) 0.2483 (0.2483)\tPrec@1 21.875 (21.875)\tPrec@5 68.750 (68.750)\n",
            "Epoch: [2][50/782]\tTime 0.022 (0.027)\tLoss (Class) 3.1057 (3.1326)\tLoss (Regu) 0.2418 (0.2473)\tPrec@1 26.562 (27.574)\tPrec@5 56.250 (58.732)\n",
            "Epoch: [2][100/782]\tTime 0.024 (0.025)\tLoss (Class) 2.8471 (3.0900)\tLoss (Regu) 0.2421 (0.2453)\tPrec@1 34.375 (28.233)\tPrec@5 64.062 (59.499)\n",
            "Epoch: [2][150/782]\tTime 0.022 (0.025)\tLoss (Class) 3.0293 (3.0636)\tLoss (Regu) 0.2435 (0.2446)\tPrec@1 25.000 (29.015)\tPrec@5 60.938 (60.130)\n",
            "Epoch: [2][200/782]\tTime 0.023 (0.024)\tLoss (Class) 2.9986 (3.0637)\tLoss (Regu) 0.2328 (0.2421)\tPrec@1 31.250 (29.237)\tPrec@5 60.938 (59.958)\n",
            "Epoch: [2][250/782]\tTime 0.023 (0.024)\tLoss (Class) 2.9903 (3.0546)\tLoss (Regu) 0.2339 (0.2407)\tPrec@1 28.125 (29.177)\tPrec@5 54.688 (60.153)\n",
            "Epoch: [2][300/782]\tTime 0.023 (0.024)\tLoss (Class) 3.1139 (3.0464)\tLoss (Regu) 0.2302 (0.2392)\tPrec@1 26.562 (29.329)\tPrec@5 60.938 (60.268)\n",
            "Epoch: [2][350/782]\tTime 0.023 (0.024)\tLoss (Class) 3.1391 (3.0339)\tLoss (Regu) 0.2331 (0.2381)\tPrec@1 32.812 (29.501)\tPrec@5 57.812 (60.590)\n",
            "Epoch: [2][400/782]\tTime 0.023 (0.024)\tLoss (Class) 2.7044 (3.0238)\tLoss (Regu) 0.2275 (0.2371)\tPrec@1 40.625 (29.543)\tPrec@5 65.625 (60.844)\n",
            "Epoch: [2][450/782]\tTime 0.023 (0.023)\tLoss (Class) 3.1412 (3.0091)\tLoss (Regu) 0.2247 (0.2362)\tPrec@1 26.562 (29.812)\tPrec@5 65.625 (61.152)\n",
            "Epoch: [2][500/782]\tTime 0.023 (0.023)\tLoss (Class) 3.0077 (3.0056)\tLoss (Regu) 0.2244 (0.2350)\tPrec@1 21.875 (29.722)\tPrec@5 56.250 (61.224)\n",
            "Epoch: [2][550/782]\tTime 0.023 (0.023)\tLoss (Class) 3.1970 (2.9943)\tLoss (Regu) 0.2255 (0.2339)\tPrec@1 21.875 (29.855)\tPrec@5 53.125 (61.502)\n",
            "Epoch: [2][600/782]\tTime 0.023 (0.023)\tLoss (Class) 2.7309 (2.9850)\tLoss (Regu) 0.2246 (0.2330)\tPrec@1 35.938 (30.067)\tPrec@5 70.312 (61.647)\n",
            "Epoch: [2][650/782]\tTime 0.023 (0.023)\tLoss (Class) 2.5737 (2.9764)\tLoss (Regu) 0.2154 (0.2320)\tPrec@1 34.375 (30.196)\tPrec@5 71.875 (61.869)\n",
            "Epoch: [2][700/782]\tTime 0.023 (0.023)\tLoss (Class) 2.7005 (2.9703)\tLoss (Regu) 0.2161 (0.2307)\tPrec@1 40.625 (30.318)\tPrec@5 70.312 (61.996)\n",
            "Epoch: [2][750/782]\tTime 0.023 (0.023)\tLoss (Class) 3.1757 (2.9593)\tLoss (Regu) 0.2115 (0.2298)\tPrec@1 26.562 (30.526)\tPrec@5 50.000 (62.211)\n",
            "Test: [0/157]\tTime 0.108 (0.108)\tLoss (Class) 2.7278 (2.7278)\tLoss (Regu) 0.2310 (0.2310)\tPrec@1 34.375 (34.375)\tPrec@5 64.062 (64.062)\n",
            "Test: [50/157]\tTime 0.008 (0.011)\tLoss (Class) 2.5854 (2.8590)\tLoss (Regu) 0.2252 (0.2265)\tPrec@1 29.688 (32.047)\tPrec@5 75.000 (64.001)\n",
            "Test: [100/157]\tTime 0.008 (0.009)\tLoss (Class) 2.9177 (2.8362)\tLoss (Regu) 0.2164 (0.2261)\tPrec@1 28.125 (32.952)\tPrec@5 62.500 (64.449)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 2.7838 (2.8613)\tLoss (Regu) 0.2233 (0.2262)\tPrec@1 25.000 (32.647)\tPrec@5 64.062 (63.866)\n",
            " * Train[30.616 %, 62.330 %, 2.955 loss] Val [32.680 %, 63.910%, 2.860 loss] Best: 32.680 %\n",
            "Time for 2 / 150 19.77706551551819\n",
            "Learning rate:  0.03\n",
            "Epoch: [3][0/782]\tTime 0.157 (0.157)\tLoss (Class) 2.7175 (2.7175)\tLoss (Regu) 0.2179 (0.2179)\tPrec@1 34.375 (34.375)\tPrec@5 65.625 (65.625)\n",
            "Epoch: [3][50/782]\tTime 0.023 (0.026)\tLoss (Class) 2.6523 (2.7187)\tLoss (Regu) 0.2158 (0.2149)\tPrec@1 37.500 (34.283)\tPrec@5 67.188 (67.708)\n",
            "Epoch: [3][100/782]\tTime 0.023 (0.024)\tLoss (Class) 2.7238 (2.7306)\tLoss (Regu) 0.2160 (0.2150)\tPrec@1 29.688 (34.545)\tPrec@5 67.188 (66.940)\n",
            "Epoch: [3][150/782]\tTime 0.022 (0.024)\tLoss (Class) 2.2351 (2.7212)\tLoss (Regu) 0.2131 (0.2144)\tPrec@1 42.188 (34.696)\tPrec@5 81.250 (67.312)\n",
            "Epoch: [3][200/782]\tTime 0.027 (0.024)\tLoss (Class) 2.1269 (2.7223)\tLoss (Regu) 0.2098 (0.2138)\tPrec@1 42.188 (34.663)\tPrec@5 85.938 (67.156)\n",
            "Epoch: [3][250/782]\tTime 0.023 (0.024)\tLoss (Class) 2.7248 (2.7237)\tLoss (Regu) 0.2075 (0.2130)\tPrec@1 35.938 (34.624)\tPrec@5 70.312 (67.094)\n",
            "Epoch: [3][300/782]\tTime 0.023 (0.024)\tLoss (Class) 2.8414 (2.7227)\tLoss (Regu) 0.2055 (0.2122)\tPrec@1 26.562 (34.609)\tPrec@5 60.938 (67.104)\n",
            "Epoch: [3][350/782]\tTime 0.024 (0.024)\tLoss (Class) 2.6439 (2.7206)\tLoss (Regu) 0.2030 (0.2115)\tPrec@1 40.625 (34.669)\tPrec@5 64.062 (67.067)\n",
            "Epoch: [3][400/782]\tTime 0.022 (0.024)\tLoss (Class) 2.6241 (2.7106)\tLoss (Regu) 0.2026 (0.2107)\tPrec@1 31.250 (34.815)\tPrec@5 67.188 (67.145)\n",
            "Epoch: [3][450/782]\tTime 0.023 (0.024)\tLoss (Class) 2.6868 (2.7072)\tLoss (Regu) 0.2091 (0.2100)\tPrec@1 29.688 (35.012)\tPrec@5 68.750 (67.201)\n",
            "Epoch: [3][500/782]\tTime 0.022 (0.024)\tLoss (Class) 2.2500 (2.7025)\tLoss (Regu) 0.1998 (0.2096)\tPrec@1 39.062 (35.161)\tPrec@5 81.250 (67.328)\n",
            "Epoch: [3][550/782]\tTime 0.023 (0.024)\tLoss (Class) 2.3295 (2.6945)\tLoss (Regu) 0.2089 (0.2090)\tPrec@1 45.312 (35.325)\tPrec@5 79.688 (67.519)\n",
            "Epoch: [3][600/782]\tTime 0.022 (0.024)\tLoss (Class) 3.0337 (2.6921)\tLoss (Regu) 0.2032 (0.2086)\tPrec@1 34.375 (35.384)\tPrec@5 60.938 (67.541)\n",
            "Epoch: [3][650/782]\tTime 0.030 (0.024)\tLoss (Class) 2.6374 (2.6851)\tLoss (Regu) 0.2003 (0.2082)\tPrec@1 31.250 (35.534)\tPrec@5 75.000 (67.696)\n",
            "Epoch: [3][700/782]\tTime 0.023 (0.024)\tLoss (Class) 2.7675 (2.6786)\tLoss (Regu) 0.2051 (0.2078)\tPrec@1 37.500 (35.652)\tPrec@5 56.250 (67.865)\n",
            "Epoch: [3][750/782]\tTime 0.030 (0.024)\tLoss (Class) 2.9656 (2.6733)\tLoss (Regu) 0.2016 (0.2075)\tPrec@1 35.938 (35.752)\tPrec@5 68.750 (67.945)\n",
            "Test: [0/157]\tTime 0.108 (0.108)\tLoss (Class) 2.7182 (2.7182)\tLoss (Regu) 0.2141 (0.2141)\tPrec@1 28.125 (28.125)\tPrec@5 68.750 (68.750)\n",
            "Test: [50/157]\tTime 0.009 (0.010)\tLoss (Class) 2.6793 (2.6238)\tLoss (Regu) 0.2051 (0.2122)\tPrec@1 39.062 (36.642)\tPrec@5 68.750 (70.098)\n",
            "Test: [100/157]\tTime 0.007 (0.009)\tLoss (Class) 2.8151 (2.6494)\tLoss (Regu) 0.2121 (0.2128)\tPrec@1 39.062 (37.237)\tPrec@5 65.625 (69.137)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 2.7512 (2.6464)\tLoss (Regu) 0.2135 (0.2128)\tPrec@1 37.500 (37.148)\tPrec@5 64.062 (69.309)\n",
            " * Train[35.842 %, 68.012 %, 2.670 loss] Val [37.200 %, 69.270%, 2.649 loss] Best: 37.200 %\n",
            "Time for 3 / 150 20.195064544677734\n",
            "Learning rate:  0.03\n",
            "Epoch: [4][0/782]\tTime 0.151 (0.151)\tLoss (Class) 2.4248 (2.4248)\tLoss (Regu) 0.2003 (0.2003)\tPrec@1 37.500 (37.500)\tPrec@5 70.312 (70.312)\n",
            "Epoch: [4][50/782]\tTime 0.023 (0.026)\tLoss (Class) 2.6806 (2.5638)\tLoss (Regu) 0.1997 (0.2018)\tPrec@1 32.812 (37.224)\tPrec@5 67.188 (69.975)\n",
            "Epoch: [4][100/782]\tTime 0.022 (0.025)\tLoss (Class) 2.7567 (2.5140)\tLoss (Regu) 0.2058 (0.2023)\tPrec@1 39.062 (39.047)\tPrec@5 68.750 (71.024)\n",
            "Epoch: [4][150/782]\tTime 0.022 (0.025)\tLoss (Class) 2.6681 (2.5053)\tLoss (Regu) 0.1990 (0.2010)\tPrec@1 37.500 (39.166)\tPrec@5 60.938 (71.264)\n",
            "Epoch: [4][200/782]\tTime 0.022 (0.024)\tLoss (Class) 2.3222 (2.5086)\tLoss (Regu) 0.2003 (0.2008)\tPrec@1 37.500 (39.062)\tPrec@5 73.438 (71.168)\n",
            "Epoch: [4][250/782]\tTime 0.023 (0.024)\tLoss (Class) 2.4048 (2.5037)\tLoss (Regu) 0.1989 (0.2010)\tPrec@1 43.750 (39.343)\tPrec@5 73.438 (71.271)\n",
            "Epoch: [4][300/782]\tTime 0.023 (0.024)\tLoss (Class) 2.3367 (2.4959)\tLoss (Regu) 0.1983 (0.2004)\tPrec@1 42.188 (39.395)\tPrec@5 68.750 (71.346)\n",
            "Epoch: [4][350/782]\tTime 0.024 (0.024)\tLoss (Class) 2.8942 (2.4996)\tLoss (Regu) 0.1964 (0.2002)\tPrec@1 35.938 (39.343)\tPrec@5 65.625 (71.457)\n",
            "Epoch: [4][400/782]\tTime 0.022 (0.024)\tLoss (Class) 2.5727 (2.4945)\tLoss (Regu) 0.1989 (0.2002)\tPrec@1 39.062 (39.394)\tPrec@5 70.312 (71.567)\n",
            "Epoch: [4][450/782]\tTime 0.027 (0.024)\tLoss (Class) 2.3059 (2.4835)\tLoss (Regu) 0.1988 (0.2001)\tPrec@1 39.062 (39.638)\tPrec@5 78.125 (71.795)\n",
            "Epoch: [4][500/782]\tTime 0.023 (0.024)\tLoss (Class) 3.0752 (2.4821)\tLoss (Regu) 0.1970 (0.2000)\tPrec@1 31.250 (39.711)\tPrec@5 59.375 (71.760)\n",
            "Epoch: [4][550/782]\tTime 0.023 (0.024)\tLoss (Class) 2.3491 (2.4833)\tLoss (Regu) 0.1950 (0.1994)\tPrec@1 40.625 (39.638)\tPrec@5 78.125 (71.762)\n",
            "Epoch: [4][600/782]\tTime 0.022 (0.024)\tLoss (Class) 2.4411 (2.4816)\tLoss (Regu) 0.1912 (0.1991)\tPrec@1 35.938 (39.551)\tPrec@5 70.312 (71.784)\n",
            "Epoch: [4][650/782]\tTime 0.024 (0.024)\tLoss (Class) 2.3129 (2.4783)\tLoss (Regu) 0.1928 (0.1988)\tPrec@1 48.438 (39.603)\tPrec@5 73.438 (71.841)\n",
            "Epoch: [4][700/782]\tTime 0.023 (0.024)\tLoss (Class) 3.0584 (2.4742)\tLoss (Regu) 0.1968 (0.1986)\tPrec@1 29.688 (39.691)\tPrec@5 62.500 (71.969)\n",
            "Epoch: [4][750/782]\tTime 0.023 (0.024)\tLoss (Class) 2.3206 (2.4682)\tLoss (Regu) 0.1946 (0.1985)\tPrec@1 48.438 (39.849)\tPrec@5 76.562 (72.131)\n",
            "Test: [0/157]\tTime 0.117 (0.117)\tLoss (Class) 2.4729 (2.4729)\tLoss (Regu) 0.2147 (0.2147)\tPrec@1 39.062 (39.062)\tPrec@5 73.438 (73.438)\n",
            "Test: [50/157]\tTime 0.008 (0.011)\tLoss (Class) 2.3841 (2.5255)\tLoss (Regu) 0.2245 (0.2161)\tPrec@1 46.875 (39.614)\tPrec@5 78.125 (71.232)\n",
            "Test: [100/157]\tTime 0.008 (0.010)\tLoss (Class) 2.7647 (2.5075)\tLoss (Regu) 0.2156 (0.2165)\tPrec@1 34.375 (39.929)\tPrec@5 65.625 (71.751)\n",
            "Test: [150/157]\tTime 0.007 (0.010)\tLoss (Class) 2.5206 (2.4878)\tLoss (Regu) 0.2180 (0.2165)\tPrec@1 42.188 (40.553)\tPrec@5 71.875 (72.558)\n",
            " * Train[39.848 %, 72.142 %, 2.467 loss] Val [40.530 %, 72.500%, 2.489 loss] Best: 40.530 %\n",
            "Time for 4 / 150 20.190163612365723\n",
            "Learning rate:  0.03\n",
            "Epoch: [5][0/782]\tTime 0.157 (0.157)\tLoss (Class) 2.2549 (2.2549)\tLoss (Regu) 0.1940 (0.1940)\tPrec@1 42.188 (42.188)\tPrec@5 73.438 (73.438)\n",
            "Epoch: [5][50/782]\tTime 0.022 (0.027)\tLoss (Class) 2.6977 (2.3539)\tLoss (Regu) 0.1973 (0.1946)\tPrec@1 39.062 (42.188)\tPrec@5 68.750 (74.020)\n",
            "Epoch: [5][100/782]\tTime 0.022 (0.026)\tLoss (Class) 2.2586 (2.3406)\tLoss (Regu) 0.1962 (0.1956)\tPrec@1 46.875 (42.002)\tPrec@5 76.562 (74.582)\n",
            "Epoch: [5][150/782]\tTime 0.023 (0.025)\tLoss (Class) 1.9599 (2.3287)\tLoss (Regu) 0.1986 (0.1952)\tPrec@1 56.250 (42.488)\tPrec@5 78.125 (74.928)\n",
            "Epoch: [5][200/782]\tTime 0.023 (0.024)\tLoss (Class) 2.6035 (2.3364)\tLoss (Regu) 0.1954 (0.1948)\tPrec@1 35.938 (42.436)\tPrec@5 73.438 (74.876)\n",
            "Epoch: [5][250/782]\tTime 0.022 (0.024)\tLoss (Class) 2.1668 (2.3361)\tLoss (Regu) 0.1954 (0.1950)\tPrec@1 42.188 (42.430)\tPrec@5 78.125 (74.844)\n",
            "Epoch: [5][300/782]\tTime 0.023 (0.024)\tLoss (Class) 2.4440 (2.3295)\tLoss (Regu) 0.1972 (0.1950)\tPrec@1 45.312 (42.759)\tPrec@5 71.875 (75.036)\n",
            "Epoch: [5][350/782]\tTime 0.021 (0.024)\tLoss (Class) 2.2293 (2.3284)\tLoss (Regu) 0.1923 (0.1948)\tPrec@1 45.312 (42.797)\tPrec@5 84.375 (75.067)\n",
            "Epoch: [5][400/782]\tTime 0.022 (0.024)\tLoss (Class) 2.0819 (2.3214)\tLoss (Regu) 0.1906 (0.1947)\tPrec@1 50.000 (43.056)\tPrec@5 81.250 (75.152)\n",
            "Epoch: [5][450/782]\tTime 0.022 (0.024)\tLoss (Class) 2.1482 (2.3174)\tLoss (Regu) 0.1913 (0.1944)\tPrec@1 46.875 (43.140)\tPrec@5 81.250 (75.152)\n",
            "Epoch: [5][500/782]\tTime 0.023 (0.024)\tLoss (Class) 1.9504 (2.3150)\tLoss (Regu) 0.1909 (0.1941)\tPrec@1 59.375 (43.263)\tPrec@5 81.250 (75.156)\n",
            "Epoch: [5][550/782]\tTime 0.021 (0.024)\tLoss (Class) 1.9637 (2.3156)\tLoss (Regu) 0.1893 (0.1939)\tPrec@1 53.125 (43.240)\tPrec@5 82.812 (75.139)\n",
            "Epoch: [5][600/782]\tTime 0.022 (0.024)\tLoss (Class) 2.3401 (2.3118)\tLoss (Regu) 0.1909 (0.1936)\tPrec@1 46.875 (43.347)\tPrec@5 73.438 (75.244)\n",
            "Epoch: [5][650/782]\tTime 0.023 (0.024)\tLoss (Class) 2.7234 (2.3125)\tLoss (Regu) 0.1932 (0.1934)\tPrec@1 34.375 (43.361)\tPrec@5 76.562 (75.264)\n",
            "Epoch: [5][700/782]\tTime 0.027 (0.024)\tLoss (Class) 2.4072 (2.3092)\tLoss (Regu) 0.1905 (0.1932)\tPrec@1 46.875 (43.433)\tPrec@5 68.750 (75.386)\n",
            "Epoch: [5][750/782]\tTime 0.022 (0.024)\tLoss (Class) 2.0230 (2.3076)\tLoss (Regu) 0.1856 (0.1929)\tPrec@1 48.438 (43.351)\tPrec@5 82.812 (75.410)\n",
            "Test: [0/157]\tTime 0.114 (0.114)\tLoss (Class) 2.3190 (2.3190)\tLoss (Regu) 0.2130 (0.2130)\tPrec@1 40.625 (40.625)\tPrec@5 78.125 (78.125)\n",
            "Test: [50/157]\tTime 0.009 (0.010)\tLoss (Class) 2.0970 (2.2991)\tLoss (Regu) 0.2054 (0.2057)\tPrec@1 50.000 (45.527)\tPrec@5 84.375 (75.643)\n",
            "Test: [100/157]\tTime 0.008 (0.009)\tLoss (Class) 1.9674 (2.3237)\tLoss (Regu) 0.2031 (0.2054)\tPrec@1 53.125 (44.678)\tPrec@5 81.250 (75.371)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 2.1114 (2.3123)\tLoss (Regu) 0.2054 (0.2058)\tPrec@1 45.312 (44.557)\tPrec@5 79.688 (75.735)\n",
            " * Train[43.468 %, 75.432 %, 2.305 loss] Val [44.610 %, 75.890%, 2.306 loss] Best: 44.610 %\n",
            "Time for 5 / 150 19.89849352836609\n",
            "Learning rate:  0.03\n",
            "Epoch: [6][0/782]\tTime 0.156 (0.156)\tLoss (Class) 2.0925 (2.0925)\tLoss (Regu) 0.1849 (0.1849)\tPrec@1 53.125 (53.125)\tPrec@5 79.688 (79.688)\n",
            "Epoch: [6][50/782]\tTime 0.033 (0.029)\tLoss (Class) 2.3656 (2.2406)\tLoss (Regu) 0.1915 (0.1888)\tPrec@1 42.188 (45.190)\tPrec@5 73.438 (77.267)\n",
            "Epoch: [6][100/782]\tTime 0.023 (0.027)\tLoss (Class) 1.9595 (2.2357)\tLoss (Regu) 0.1918 (0.1896)\tPrec@1 53.125 (44.616)\tPrec@5 81.250 (77.166)\n",
            "Epoch: [6][150/782]\tTime 0.023 (0.026)\tLoss (Class) 2.1553 (2.2143)\tLoss (Regu) 0.1947 (0.1908)\tPrec@1 43.750 (44.857)\tPrec@5 75.000 (77.701)\n",
            "Epoch: [6][200/782]\tTime 0.022 (0.025)\tLoss (Class) 2.0735 (2.2037)\tLoss (Regu) 0.1921 (0.1909)\tPrec@1 45.312 (44.970)\tPrec@5 79.688 (77.791)\n",
            "Epoch: [6][250/782]\tTime 0.022 (0.025)\tLoss (Class) 2.2492 (2.1983)\tLoss (Regu) 0.1905 (0.1915)\tPrec@1 39.062 (45.207)\tPrec@5 70.312 (77.783)\n",
            "Epoch: [6][300/782]\tTime 0.022 (0.025)\tLoss (Class) 2.2203 (2.2015)\tLoss (Regu) 0.1908 (0.1910)\tPrec@1 42.188 (45.271)\tPrec@5 81.250 (77.715)\n",
            "Epoch: [6][350/782]\tTime 0.023 (0.025)\tLoss (Class) 2.1449 (2.2002)\tLoss (Regu) 0.1878 (0.1907)\tPrec@1 40.625 (45.201)\tPrec@5 79.688 (77.764)\n",
            "Epoch: [6][400/782]\tTime 0.022 (0.025)\tLoss (Class) 2.1277 (2.2015)\tLoss (Regu) 0.1890 (0.1904)\tPrec@1 46.875 (45.200)\tPrec@5 79.688 (77.720)\n",
            "Epoch: [6][450/782]\tTime 0.023 (0.024)\tLoss (Class) 2.2029 (2.1989)\tLoss (Regu) 0.1880 (0.1904)\tPrec@1 46.875 (45.399)\tPrec@5 76.562 (77.668)\n",
            "Epoch: [6][500/782]\tTime 0.023 (0.024)\tLoss (Class) 1.9597 (2.1952)\tLoss (Regu) 0.1905 (0.1904)\tPrec@1 50.000 (45.531)\tPrec@5 84.375 (77.735)\n",
            "Epoch: [6][550/782]\tTime 0.023 (0.024)\tLoss (Class) 1.9270 (2.1953)\tLoss (Regu) 0.1898 (0.1903)\tPrec@1 50.000 (45.537)\tPrec@5 85.938 (77.705)\n",
            "Epoch: [6][600/782]\tTime 0.023 (0.024)\tLoss (Class) 2.3712 (2.1939)\tLoss (Regu) 0.1882 (0.1901)\tPrec@1 39.062 (45.559)\tPrec@5 75.000 (77.706)\n",
            "Epoch: [6][650/782]\tTime 0.022 (0.024)\tLoss (Class) 2.3235 (2.1908)\tLoss (Regu) 0.1879 (0.1900)\tPrec@1 50.000 (45.692)\tPrec@5 76.562 (77.787)\n",
            "Epoch: [6][700/782]\tTime 0.027 (0.024)\tLoss (Class) 2.4754 (2.1872)\tLoss (Regu) 0.1908 (0.1897)\tPrec@1 42.188 (45.843)\tPrec@5 75.000 (77.824)\n",
            "Epoch: [6][750/782]\tTime 0.023 (0.024)\tLoss (Class) 2.3117 (2.1882)\tLoss (Regu) 0.1898 (0.1896)\tPrec@1 43.750 (45.899)\tPrec@5 81.250 (77.753)\n",
            "Test: [0/157]\tTime 0.109 (0.109)\tLoss (Class) 2.5816 (2.5816)\tLoss (Regu) 0.2116 (0.2116)\tPrec@1 32.812 (32.812)\tPrec@5 67.188 (67.188)\n",
            "Test: [50/157]\tTime 0.008 (0.011)\tLoss (Class) 2.3526 (2.3284)\tLoss (Regu) 0.2098 (0.2132)\tPrec@1 42.188 (44.730)\tPrec@5 79.688 (75.797)\n",
            "Test: [100/157]\tTime 0.009 (0.010)\tLoss (Class) 2.1964 (2.3467)\tLoss (Regu) 0.2125 (0.2134)\tPrec@1 39.062 (44.446)\tPrec@5 75.000 (75.232)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 2.1997 (2.3365)\tLoss (Regu) 0.2099 (0.2132)\tPrec@1 45.312 (44.423)\tPrec@5 76.562 (75.528)\n",
            " * Train[45.940 %, 77.752 %, 2.188 loss] Val [44.360 %, 75.510%, 2.338 loss] Best: 44.610 %\n",
            "Time for 6 / 150 20.433414936065674\n",
            "Learning rate:  0.03\n",
            "Epoch: [7][0/782]\tTime 0.155 (0.155)\tLoss (Class) 1.6906 (1.6906)\tLoss (Regu) 0.1913 (0.1913)\tPrec@1 59.375 (59.375)\tPrec@5 84.375 (84.375)\n",
            "Epoch: [7][50/782]\tTime 0.023 (0.026)\tLoss (Class) 2.2918 (2.1421)\tLoss (Regu) 0.1890 (0.1891)\tPrec@1 39.062 (46.844)\tPrec@5 79.688 (77.941)\n",
            "Epoch: [7][100/782]\tTime 0.022 (0.025)\tLoss (Class) 2.3841 (2.1362)\tLoss (Regu) 0.1939 (0.1894)\tPrec@1 42.188 (46.906)\tPrec@5 67.188 (78.125)\n",
            "Epoch: [7][150/782]\tTime 0.023 (0.024)\tLoss (Class) 2.2949 (2.1143)\tLoss (Regu) 0.1862 (0.1900)\tPrec@1 42.188 (47.351)\tPrec@5 75.000 (78.384)\n",
            "Epoch: [7][200/782]\tTime 0.023 (0.024)\tLoss (Class) 1.8894 (2.0994)\tLoss (Regu) 0.1944 (0.1899)\tPrec@1 46.875 (47.816)\tPrec@5 81.250 (78.708)\n",
            "Epoch: [7][250/782]\tTime 0.023 (0.024)\tLoss (Class) 2.1503 (2.0911)\tLoss (Regu) 0.1901 (0.1901)\tPrec@1 48.438 (48.076)\tPrec@5 79.688 (78.997)\n",
            "Epoch: [7][300/782]\tTime 0.022 (0.024)\tLoss (Class) 1.8696 (2.0983)\tLoss (Regu) 0.1886 (0.1899)\tPrec@1 48.438 (48.007)\tPrec@5 79.688 (78.888)\n",
            "Epoch: [7][350/782]\tTime 0.022 (0.024)\tLoss (Class) 1.9726 (2.0911)\tLoss (Regu) 0.1876 (0.1901)\tPrec@1 51.562 (48.064)\tPrec@5 84.375 (78.966)\n",
            "Epoch: [7][400/782]\tTime 0.022 (0.024)\tLoss (Class) 2.0173 (2.0902)\tLoss (Regu) 0.1848 (0.1900)\tPrec@1 53.125 (48.176)\tPrec@5 81.250 (79.045)\n",
            "Epoch: [7][450/782]\tTime 0.022 (0.024)\tLoss (Class) 2.0036 (2.0854)\tLoss (Regu) 0.1887 (0.1899)\tPrec@1 51.562 (48.271)\tPrec@5 78.125 (79.185)\n",
            "Epoch: [7][500/782]\tTime 0.021 (0.024)\tLoss (Class) 1.9817 (2.0845)\tLoss (Regu) 0.1891 (0.1895)\tPrec@1 42.188 (48.260)\tPrec@5 81.250 (79.295)\n",
            "Epoch: [7][550/782]\tTime 0.025 (0.024)\tLoss (Class) 1.9311 (2.0854)\tLoss (Regu) 0.1884 (0.1892)\tPrec@1 59.375 (48.194)\tPrec@5 78.125 (79.268)\n",
            "Epoch: [7][600/782]\tTime 0.022 (0.024)\tLoss (Class) 1.8066 (2.0862)\tLoss (Regu) 0.1844 (0.1891)\tPrec@1 56.250 (48.183)\tPrec@5 87.500 (79.287)\n",
            "Epoch: [7][650/782]\tTime 0.022 (0.024)\tLoss (Class) 1.9713 (2.0846)\tLoss (Regu) 0.1847 (0.1889)\tPrec@1 51.562 (48.157)\tPrec@5 84.375 (79.411)\n",
            "Epoch: [7][700/782]\tTime 0.023 (0.024)\tLoss (Class) 2.6415 (2.0857)\tLoss (Regu) 0.1840 (0.1887)\tPrec@1 34.375 (48.110)\tPrec@5 71.875 (79.436)\n",
            "Epoch: [7][750/782]\tTime 0.023 (0.024)\tLoss (Class) 2.4043 (2.0867)\tLoss (Regu) 0.1815 (0.1886)\tPrec@1 43.750 (48.092)\tPrec@5 68.750 (79.396)\n",
            "Test: [0/157]\tTime 0.114 (0.114)\tLoss (Class) 2.4013 (2.4013)\tLoss (Regu) 0.2075 (0.2075)\tPrec@1 43.750 (43.750)\tPrec@5 70.312 (70.312)\n",
            "Test: [50/157]\tTime 0.008 (0.010)\tLoss (Class) 2.1849 (2.2181)\tLoss (Regu) 0.2050 (0.2097)\tPrec@1 53.125 (46.875)\tPrec@5 75.000 (78.186)\n",
            "Test: [100/157]\tTime 0.009 (0.009)\tLoss (Class) 2.0320 (2.2130)\tLoss (Regu) 0.2012 (0.2095)\tPrec@1 46.875 (46.287)\tPrec@5 82.812 (77.877)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 2.5427 (2.2091)\tLoss (Regu) 0.2075 (0.2094)\tPrec@1 34.375 (46.513)\tPrec@5 76.562 (77.835)\n",
            " * Train[48.094 %, 79.352 %, 2.088 loss] Val [46.390 %, 77.690%, 2.213 loss] Best: 46.390 %\n",
            "Time for 7 / 150 20.20393395423889\n",
            "Learning rate:  0.03\n",
            "Epoch: [8][0/782]\tTime 0.147 (0.147)\tLoss (Class) 1.8403 (1.8403)\tLoss (Regu) 0.1853 (0.1853)\tPrec@1 53.125 (53.125)\tPrec@5 84.375 (84.375)\n",
            "Epoch: [8][50/782]\tTime 0.021 (0.027)\tLoss (Class) 2.1639 (1.9937)\tLoss (Regu) 0.1848 (0.1898)\tPrec@1 43.750 (49.295)\tPrec@5 75.000 (82.322)\n",
            "Epoch: [8][100/782]\tTime 0.022 (0.026)\tLoss (Class) 2.1523 (1.9691)\tLoss (Regu) 0.1845 (0.1897)\tPrec@1 34.375 (50.882)\tPrec@5 79.688 (81.761)\n",
            "Epoch: [8][150/782]\tTime 0.028 (0.026)\tLoss (Class) 1.8392 (1.9752)\tLoss (Regu) 0.1927 (0.1886)\tPrec@1 59.375 (51.066)\tPrec@5 87.500 (81.550)\n",
            "Epoch: [8][200/782]\tTime 0.023 (0.025)\tLoss (Class) 1.6946 (1.9661)\tLoss (Regu) 0.1943 (0.1890)\tPrec@1 60.938 (51.228)\tPrec@5 79.688 (81.740)\n",
            "Epoch: [8][250/782]\tTime 0.032 (0.025)\tLoss (Class) 2.0234 (1.9590)\tLoss (Regu) 0.1914 (0.1892)\tPrec@1 51.562 (51.550)\tPrec@5 78.125 (81.904)\n",
            "Epoch: [8][300/782]\tTime 0.027 (0.025)\tLoss (Class) 2.1294 (1.9614)\tLoss (Regu) 0.1936 (0.1892)\tPrec@1 46.875 (51.562)\tPrec@5 79.688 (81.821)\n",
            "Epoch: [8][350/782]\tTime 0.023 (0.025)\tLoss (Class) 2.0731 (1.9619)\tLoss (Regu) 0.1816 (0.1891)\tPrec@1 48.438 (51.433)\tPrec@5 75.000 (81.735)\n",
            "Epoch: [8][400/782]\tTime 0.024 (0.025)\tLoss (Class) 1.8659 (1.9723)\tLoss (Regu) 0.1857 (0.1885)\tPrec@1 56.250 (51.270)\tPrec@5 81.250 (81.472)\n",
            "Epoch: [8][450/782]\tTime 0.023 (0.025)\tLoss (Class) 2.0977 (1.9790)\tLoss (Regu) 0.1838 (0.1883)\tPrec@1 34.375 (51.164)\tPrec@5 81.250 (81.340)\n",
            "Epoch: [8][500/782]\tTime 0.025 (0.024)\tLoss (Class) 1.9429 (1.9867)\tLoss (Regu) 0.1877 (0.1879)\tPrec@1 57.812 (50.914)\tPrec@5 81.250 (81.241)\n",
            "Epoch: [8][550/782]\tTime 0.024 (0.024)\tLoss (Class) 2.2276 (1.9927)\tLoss (Regu) 0.1854 (0.1876)\tPrec@1 46.875 (50.749)\tPrec@5 76.562 (81.165)\n",
            "Epoch: [8][600/782]\tTime 0.023 (0.024)\tLoss (Class) 2.1248 (1.9926)\tLoss (Regu) 0.1863 (0.1875)\tPrec@1 43.750 (50.749)\tPrec@5 79.688 (81.154)\n",
            "Epoch: [8][650/782]\tTime 0.023 (0.024)\tLoss (Class) 1.8982 (1.9929)\tLoss (Regu) 0.1857 (0.1874)\tPrec@1 45.312 (50.660)\tPrec@5 81.250 (81.178)\n",
            "Epoch: [8][700/782]\tTime 0.023 (0.024)\tLoss (Class) 1.8497 (1.9991)\tLoss (Regu) 0.1892 (0.1874)\tPrec@1 53.125 (50.446)\tPrec@5 82.812 (81.054)\n",
            "Epoch: [8][750/782]\tTime 0.023 (0.024)\tLoss (Class) 1.8881 (1.9979)\tLoss (Regu) 0.1901 (0.1874)\tPrec@1 48.438 (50.441)\tPrec@5 84.375 (81.092)\n",
            "Test: [0/157]\tTime 0.113 (0.113)\tLoss (Class) 2.2748 (2.2748)\tLoss (Regu) 0.1983 (0.1983)\tPrec@1 42.188 (42.188)\tPrec@5 78.125 (78.125)\n",
            "Test: [50/157]\tTime 0.008 (0.011)\tLoss (Class) 2.4460 (2.3203)\tLoss (Regu) 0.2017 (0.2003)\tPrec@1 40.625 (44.332)\tPrec@5 75.000 (74.786)\n",
            "Test: [100/157]\tTime 0.008 (0.010)\tLoss (Class) 2.2910 (2.2969)\tLoss (Regu) 0.1975 (0.2002)\tPrec@1 46.875 (44.663)\tPrec@5 70.312 (75.186)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 2.5081 (2.2893)\tLoss (Regu) 0.1994 (0.1999)\tPrec@1 40.625 (45.219)\tPrec@5 68.750 (75.600)\n",
            " * Train[50.362 %, 81.160 %, 1.998 loss] Val [45.350 %, 75.620%, 2.285 loss] Best: 46.390 %\n",
            "Time for 8 / 150 20.18637752532959\n",
            "Learning rate:  0.03\n",
            "Epoch: [9][0/782]\tTime 0.147 (0.147)\tLoss (Class) 1.5546 (1.5546)\tLoss (Regu) 0.1879 (0.1879)\tPrec@1 65.625 (65.625)\tPrec@5 87.500 (87.500)\n",
            "Epoch: [9][50/782]\tTime 0.023 (0.027)\tLoss (Class) 1.6720 (1.8857)\tLoss (Regu) 0.1928 (0.1904)\tPrec@1 57.812 (51.808)\tPrec@5 81.250 (83.058)\n",
            "Epoch: [9][100/782]\tTime 0.022 (0.025)\tLoss (Class) 1.8318 (1.9056)\tLoss (Regu) 0.1941 (0.1891)\tPrec@1 50.000 (51.872)\tPrec@5 87.500 (82.689)\n",
            "Epoch: [9][150/782]\tTime 0.029 (0.025)\tLoss (Class) 1.8796 (1.9240)\tLoss (Regu) 0.1820 (0.1887)\tPrec@1 59.375 (51.738)\tPrec@5 82.812 (82.368)\n",
            "Epoch: [9][200/782]\tTime 0.033 (0.025)\tLoss (Class) 2.2047 (1.9216)\tLoss (Regu) 0.1901 (0.1886)\tPrec@1 40.625 (51.772)\tPrec@5 79.688 (82.354)\n",
            "Epoch: [9][250/782]\tTime 0.030 (0.025)\tLoss (Class) 2.0260 (1.9255)\tLoss (Regu) 0.1814 (0.1883)\tPrec@1 43.750 (51.650)\tPrec@5 81.250 (82.171)\n",
            "Epoch: [9][300/782]\tTime 0.032 (0.025)\tLoss (Class) 2.0762 (1.9274)\tLoss (Regu) 0.1842 (0.1881)\tPrec@1 53.125 (51.583)\tPrec@5 79.688 (82.283)\n",
            "Epoch: [9][350/782]\tTime 0.022 (0.025)\tLoss (Class) 1.8387 (1.9238)\tLoss (Regu) 0.1826 (0.1879)\tPrec@1 51.562 (51.727)\tPrec@5 84.375 (82.430)\n",
            "Epoch: [9][400/782]\tTime 0.032 (0.025)\tLoss (Class) 2.2623 (1.9311)\tLoss (Regu) 0.1895 (0.1875)\tPrec@1 39.062 (51.625)\tPrec@5 75.000 (82.286)\n",
            "Epoch: [9][450/782]\tTime 0.023 (0.025)\tLoss (Class) 1.9019 (1.9302)\tLoss (Regu) 0.1828 (0.1873)\tPrec@1 43.750 (51.708)\tPrec@5 85.938 (82.185)\n",
            "Epoch: [9][500/782]\tTime 0.023 (0.025)\tLoss (Class) 1.6101 (1.9305)\tLoss (Regu) 0.1817 (0.1870)\tPrec@1 62.500 (51.647)\tPrec@5 90.625 (82.151)\n",
            "Epoch: [9][550/782]\tTime 0.033 (0.025)\tLoss (Class) 1.9910 (1.9305)\tLoss (Regu) 0.1842 (0.1867)\tPrec@1 46.875 (51.676)\tPrec@5 81.250 (82.140)\n",
            "Epoch: [9][600/782]\tTime 0.023 (0.025)\tLoss (Class) 2.0211 (1.9258)\tLoss (Regu) 0.1856 (0.1866)\tPrec@1 46.875 (51.742)\tPrec@5 82.812 (82.251)\n",
            "Epoch: [9][650/782]\tTime 0.022 (0.025)\tLoss (Class) 1.5799 (1.9246)\tLoss (Regu) 0.1828 (0.1864)\tPrec@1 60.938 (51.812)\tPrec@5 85.938 (82.256)\n",
            "Epoch: [9][700/782]\tTime 0.023 (0.025)\tLoss (Class) 2.0250 (1.9252)\tLoss (Regu) 0.1871 (0.1861)\tPrec@1 51.562 (51.725)\tPrec@5 76.562 (82.240)\n",
            "Epoch: [9][750/782]\tTime 0.023 (0.024)\tLoss (Class) 1.7551 (1.9243)\tLoss (Regu) 0.1861 (0.1862)\tPrec@1 53.125 (51.737)\tPrec@5 81.250 (82.282)\n",
            "Test: [0/157]\tTime 0.118 (0.118)\tLoss (Class) 2.2987 (2.2987)\tLoss (Regu) 0.2016 (0.2016)\tPrec@1 54.688 (54.688)\tPrec@5 81.250 (81.250)\n",
            "Test: [50/157]\tTime 0.008 (0.010)\tLoss (Class) 1.9660 (2.1774)\tLoss (Regu) 0.2071 (0.2020)\tPrec@1 57.812 (47.304)\tPrec@5 79.688 (78.768)\n",
            "Test: [100/157]\tTime 0.008 (0.009)\tLoss (Class) 1.9905 (2.1537)\tLoss (Regu) 0.2075 (0.2021)\tPrec@1 57.812 (48.252)\tPrec@5 79.688 (79.084)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 2.3392 (2.1747)\tLoss (Regu) 0.2085 (0.2020)\tPrec@1 45.312 (47.993)\tPrec@5 75.000 (78.322)\n",
            " * Train[51.722 %, 82.292 %, 1.925 loss] Val [47.990 %, 78.430%, 2.172 loss] Best: 47.990 %\n",
            "Time for 9 / 150 20.56900691986084\n",
            "Learning rate:  0.03\n",
            "Epoch: [10][0/782]\tTime 0.148 (0.148)\tLoss (Class) 1.6559 (1.6559)\tLoss (Regu) 0.1876 (0.1876)\tPrec@1 54.688 (54.688)\tPrec@5 82.812 (82.812)\n",
            "Epoch: [10][50/782]\tTime 0.023 (0.025)\tLoss (Class) 1.8637 (1.8720)\tLoss (Regu) 0.1866 (0.1865)\tPrec@1 50.000 (52.237)\tPrec@5 87.500 (83.456)\n",
            "Epoch: [10][100/782]\tTime 0.023 (0.024)\tLoss (Class) 2.0398 (1.8474)\tLoss (Regu) 0.1894 (0.1874)\tPrec@1 48.438 (53.465)\tPrec@5 84.375 (83.509)\n",
            "Epoch: [10][150/782]\tTime 0.024 (0.024)\tLoss (Class) 1.6748 (1.8412)\tLoss (Regu) 0.1901 (0.1880)\tPrec@1 56.250 (53.611)\tPrec@5 85.938 (83.495)\n",
            "Epoch: [10][200/782]\tTime 0.030 (0.024)\tLoss (Class) 1.8871 (1.8460)\tLoss (Regu) 0.1849 (0.1880)\tPrec@1 45.312 (53.436)\tPrec@5 84.375 (83.535)\n",
            "Epoch: [10][250/782]\tTime 0.023 (0.025)\tLoss (Class) 1.6055 (1.8430)\tLoss (Regu) 0.1865 (0.1877)\tPrec@1 60.938 (53.467)\tPrec@5 89.062 (83.740)\n",
            "Epoch: [10][300/782]\tTime 0.038 (0.025)\tLoss (Class) 1.6093 (1.8450)\tLoss (Regu) 0.1834 (0.1876)\tPrec@1 57.812 (53.478)\tPrec@5 87.500 (83.674)\n",
            "Epoch: [10][350/782]\tTime 0.023 (0.024)\tLoss (Class) 1.7624 (1.8418)\tLoss (Regu) 0.1851 (0.1874)\tPrec@1 45.312 (53.579)\tPrec@5 87.500 (83.694)\n",
            "Epoch: [10][400/782]\tTime 0.022 (0.024)\tLoss (Class) 1.8523 (1.8460)\tLoss (Regu) 0.1836 (0.1872)\tPrec@1 48.438 (53.569)\tPrec@5 87.500 (83.635)\n",
            "Epoch: [10][450/782]\tTime 0.022 (0.024)\tLoss (Class) 1.9429 (1.8455)\tLoss (Regu) 0.1867 (0.1871)\tPrec@1 53.125 (53.506)\tPrec@5 79.688 (83.665)\n",
            "Epoch: [10][500/782]\tTime 0.022 (0.024)\tLoss (Class) 1.5907 (1.8506)\tLoss (Regu) 0.1832 (0.1869)\tPrec@1 54.688 (53.378)\tPrec@5 81.250 (83.480)\n",
            "Epoch: [10][550/782]\tTime 0.022 (0.024)\tLoss (Class) 1.5705 (1.8515)\tLoss (Regu) 0.1820 (0.1866)\tPrec@1 60.938 (53.471)\tPrec@5 92.188 (83.465)\n",
            "Epoch: [10][600/782]\tTime 0.023 (0.024)\tLoss (Class) 2.1938 (1.8554)\tLoss (Regu) 0.1876 (0.1864)\tPrec@1 42.188 (53.401)\tPrec@5 73.438 (83.403)\n",
            "Epoch: [10][650/782]\tTime 0.022 (0.024)\tLoss (Class) 1.7830 (1.8565)\tLoss (Regu) 0.1825 (0.1864)\tPrec@1 56.250 (53.317)\tPrec@5 82.812 (83.403)\n",
            "Epoch: [10][700/782]\tTime 0.022 (0.024)\tLoss (Class) 2.2262 (1.8549)\tLoss (Regu) 0.1894 (0.1862)\tPrec@1 48.438 (53.417)\tPrec@5 78.125 (83.401)\n",
            "Epoch: [10][750/782]\tTime 0.022 (0.024)\tLoss (Class) 1.6194 (1.8511)\tLoss (Regu) 0.1846 (0.1862)\tPrec@1 57.812 (53.543)\tPrec@5 84.375 (83.466)\n",
            "Test: [0/157]\tTime 0.107 (0.107)\tLoss (Class) 2.2454 (2.2454)\tLoss (Regu) 0.2105 (0.2105)\tPrec@1 51.562 (51.562)\tPrec@5 78.125 (78.125)\n",
            "Test: [50/157]\tTime 0.008 (0.010)\tLoss (Class) 2.2173 (2.1474)\tLoss (Regu) 0.2090 (0.2078)\tPrec@1 48.438 (48.775)\tPrec@5 78.125 (79.412)\n",
            "Test: [100/157]\tTime 0.008 (0.009)\tLoss (Class) 2.0641 (2.1306)\tLoss (Regu) 0.2092 (0.2075)\tPrec@1 53.125 (48.994)\tPrec@5 73.438 (79.626)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 2.1003 (2.1366)\tLoss (Regu) 0.2067 (0.2076)\tPrec@1 48.438 (49.203)\tPrec@5 81.250 (79.232)\n",
            " * Train[53.588 %, 83.466 %, 1.850 loss] Val [49.240 %, 79.230%, 2.139 loss] Best: 49.240 %\n",
            "Time for 10 / 150 20.192588806152344\n",
            "Learning rate:  0.03\n",
            "Epoch: [11][0/782]\tTime 0.150 (0.150)\tLoss (Class) 1.8354 (1.8354)\tLoss (Regu) 0.1836 (0.1836)\tPrec@1 48.438 (48.438)\tPrec@5 82.812 (82.812)\n",
            "Epoch: [11][50/782]\tTime 0.023 (0.025)\tLoss (Class) 1.8794 (1.7780)\tLoss (Regu) 0.1834 (0.1862)\tPrec@1 53.125 (55.545)\tPrec@5 85.938 (84.314)\n",
            "Epoch: [11][100/782]\tTime 0.022 (0.024)\tLoss (Class) 1.7538 (1.7930)\tLoss (Regu) 0.1857 (0.1857)\tPrec@1 59.375 (54.688)\tPrec@5 89.062 (84.390)\n",
            "Epoch: [11][150/782]\tTime 0.023 (0.024)\tLoss (Class) 2.1650 (1.7889)\tLoss (Regu) 0.1850 (0.1860)\tPrec@1 51.562 (54.946)\tPrec@5 78.125 (84.727)\n",
            "Epoch: [11][200/782]\tTime 0.023 (0.024)\tLoss (Class) 2.1375 (1.7972)\tLoss (Regu) 0.1869 (0.1860)\tPrec@1 45.312 (54.905)\tPrec@5 75.000 (84.523)\n",
            "Epoch: [11][250/782]\tTime 0.024 (0.024)\tLoss (Class) 2.3089 (1.7948)\tLoss (Regu) 0.1897 (0.1859)\tPrec@1 37.500 (54.775)\tPrec@5 73.438 (84.587)\n",
            "Epoch: [11][300/782]\tTime 0.023 (0.024)\tLoss (Class) 2.3086 (1.7952)\tLoss (Regu) 0.1893 (0.1859)\tPrec@1 43.750 (54.776)\tPrec@5 73.438 (84.515)\n",
            "Epoch: [11][350/782]\tTime 0.023 (0.024)\tLoss (Class) 1.5523 (1.8037)\tLoss (Regu) 0.1854 (0.1859)\tPrec@1 57.812 (54.483)\tPrec@5 85.938 (84.446)\n",
            "Epoch: [11][400/782]\tTime 0.030 (0.024)\tLoss (Class) 1.8856 (1.8027)\tLoss (Regu) 0.1864 (0.1860)\tPrec@1 53.125 (54.571)\tPrec@5 81.250 (84.453)\n",
            "Epoch: [11][450/782]\tTime 0.023 (0.023)\tLoss (Class) 1.7578 (1.8041)\tLoss (Regu) 0.1858 (0.1860)\tPrec@1 54.688 (54.435)\tPrec@5 85.938 (84.489)\n",
            "Epoch: [11][500/782]\tTime 0.022 (0.023)\tLoss (Class) 1.9152 (1.8009)\tLoss (Regu) 0.1871 (0.1859)\tPrec@1 46.875 (54.569)\tPrec@5 82.812 (84.534)\n",
            "Epoch: [11][550/782]\tTime 0.023 (0.023)\tLoss (Class) 1.5549 (1.8004)\tLoss (Regu) 0.1835 (0.1859)\tPrec@1 56.250 (54.523)\tPrec@5 90.625 (84.588)\n",
            "Epoch: [11][600/782]\tTime 0.023 (0.023)\tLoss (Class) 2.2712 (1.8029)\tLoss (Regu) 0.1880 (0.1859)\tPrec@1 43.750 (54.513)\tPrec@5 76.562 (84.547)\n",
            "Epoch: [11][650/782]\tTime 0.023 (0.023)\tLoss (Class) 1.7943 (1.8026)\tLoss (Regu) 0.1883 (0.1857)\tPrec@1 53.125 (54.582)\tPrec@5 84.375 (84.469)\n",
            "Epoch: [11][700/782]\tTime 0.024 (0.023)\tLoss (Class) 1.6685 (1.7977)\tLoss (Regu) 0.1882 (0.1855)\tPrec@1 64.062 (54.739)\tPrec@5 82.812 (84.509)\n",
            "Epoch: [11][750/782]\tTime 0.024 (0.023)\tLoss (Class) 2.0757 (1.7977)\tLoss (Regu) 0.1853 (0.1855)\tPrec@1 48.438 (54.717)\tPrec@5 75.000 (84.521)\n",
            "Test: [0/157]\tTime 0.112 (0.112)\tLoss (Class) 2.0592 (2.0592)\tLoss (Regu) 0.2002 (0.2002)\tPrec@1 51.562 (51.562)\tPrec@5 81.250 (81.250)\n",
            "Test: [50/157]\tTime 0.013 (0.012)\tLoss (Class) 1.7778 (1.9927)\tLoss (Regu) 0.2080 (0.2064)\tPrec@1 60.938 (53.125)\tPrec@5 82.812 (81.189)\n",
            "Test: [100/157]\tTime 0.008 (0.011)\tLoss (Class) 1.5890 (1.9999)\tLoss (Regu) 0.2030 (0.2063)\tPrec@1 56.250 (52.058)\tPrec@5 87.500 (81.281)\n",
            "Test: [150/157]\tTime 0.007 (0.010)\tLoss (Class) 2.2178 (1.9994)\tLoss (Regu) 0.2069 (0.2061)\tPrec@1 45.312 (51.800)\tPrec@5 76.562 (81.374)\n",
            " * Train[54.718 %, 84.516 %, 1.799 loss] Val [51.820 %, 81.340%, 1.999 loss] Best: 51.820 %\n",
            "Time for 11 / 150 19.855413675308228\n",
            "Learning rate:  0.03\n",
            "Epoch: [12][0/782]\tTime 0.148 (0.148)\tLoss (Class) 1.6712 (1.6712)\tLoss (Regu) 0.1816 (0.1816)\tPrec@1 59.375 (59.375)\tPrec@5 89.062 (89.062)\n",
            "Epoch: [12][50/782]\tTime 0.022 (0.028)\tLoss (Class) 1.7336 (1.7530)\tLoss (Regu) 0.1839 (0.1846)\tPrec@1 53.125 (55.729)\tPrec@5 84.375 (85.539)\n",
            "Epoch: [12][100/782]\tTime 0.030 (0.026)\tLoss (Class) 1.5905 (1.7476)\tLoss (Regu) 0.1844 (0.1855)\tPrec@1 64.062 (56.064)\tPrec@5 89.062 (85.566)\n",
            "Epoch: [12][150/782]\tTime 0.022 (0.026)\tLoss (Class) 1.7258 (1.7459)\tLoss (Regu) 0.1906 (0.1859)\tPrec@1 57.812 (55.774)\tPrec@5 81.250 (85.586)\n",
            "Epoch: [12][200/782]\tTime 0.023 (0.025)\tLoss (Class) 1.7657 (1.7492)\tLoss (Regu) 0.1868 (0.1866)\tPrec@1 53.125 (55.690)\tPrec@5 84.375 (85.533)\n",
            "Epoch: [12][250/782]\tTime 0.022 (0.025)\tLoss (Class) 1.8458 (1.7461)\tLoss (Regu) 0.1858 (0.1863)\tPrec@1 54.688 (55.802)\tPrec@5 82.812 (85.477)\n",
            "Epoch: [12][300/782]\tTime 0.022 (0.025)\tLoss (Class) 1.6711 (1.7413)\tLoss (Regu) 0.1816 (0.1860)\tPrec@1 57.812 (55.913)\tPrec@5 87.500 (85.527)\n",
            "Epoch: [12][350/782]\tTime 0.021 (0.025)\tLoss (Class) 1.7298 (1.7400)\tLoss (Regu) 0.1824 (0.1860)\tPrec@1 57.812 (55.987)\tPrec@5 81.250 (85.546)\n",
            "Epoch: [12][400/782]\tTime 0.023 (0.025)\tLoss (Class) 1.6664 (1.7433)\tLoss (Regu) 0.1831 (0.1853)\tPrec@1 65.625 (56.051)\tPrec@5 87.500 (85.493)\n",
            "Epoch: [12][450/782]\tTime 0.023 (0.025)\tLoss (Class) 1.8722 (1.7428)\tLoss (Regu) 0.1851 (0.1853)\tPrec@1 59.375 (55.990)\tPrec@5 81.250 (85.463)\n",
            "Epoch: [12][500/782]\tTime 0.023 (0.025)\tLoss (Class) 1.7831 (1.7442)\tLoss (Regu) 0.1855 (0.1853)\tPrec@1 54.688 (55.926)\tPrec@5 81.250 (85.389)\n",
            "Epoch: [12][550/782]\tTime 0.023 (0.025)\tLoss (Class) 1.9219 (1.7499)\tLoss (Regu) 0.1800 (0.1851)\tPrec@1 45.312 (55.757)\tPrec@5 82.812 (85.333)\n",
            "Epoch: [12][600/782]\tTime 0.023 (0.025)\tLoss (Class) 1.7561 (1.7489)\tLoss (Regu) 0.1896 (0.1850)\tPrec@1 46.875 (55.769)\tPrec@5 89.062 (85.358)\n",
            "Epoch: [12][650/782]\tTime 0.024 (0.025)\tLoss (Class) 1.6321 (1.7484)\tLoss (Regu) 0.1859 (0.1851)\tPrec@1 53.125 (55.753)\tPrec@5 87.500 (85.349)\n",
            "Epoch: [12][700/782]\tTime 0.023 (0.025)\tLoss (Class) 1.3675 (1.7450)\tLoss (Regu) 0.1846 (0.1850)\tPrec@1 68.750 (55.905)\tPrec@5 93.750 (85.429)\n",
            "Epoch: [12][750/782]\tTime 0.023 (0.025)\tLoss (Class) 1.8490 (1.7465)\tLoss (Regu) 0.1842 (0.1850)\tPrec@1 50.000 (55.823)\tPrec@5 84.375 (85.426)\n",
            "Test: [0/157]\tTime 0.111 (0.111)\tLoss (Class) 1.9307 (1.9307)\tLoss (Regu) 0.2049 (0.2049)\tPrec@1 51.562 (51.562)\tPrec@5 79.688 (79.688)\n",
            "Test: [50/157]\tTime 0.008 (0.010)\tLoss (Class) 2.0731 (2.0027)\tLoss (Regu) 0.2009 (0.2035)\tPrec@1 48.438 (51.808)\tPrec@5 79.688 (80.147)\n",
            "Test: [100/157]\tTime 0.007 (0.009)\tLoss (Class) 2.2247 (1.9921)\tLoss (Regu) 0.2002 (0.2032)\tPrec@1 40.625 (51.887)\tPrec@5 82.812 (81.064)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 2.5488 (1.9819)\tLoss (Regu) 0.2010 (0.2031)\tPrec@1 45.312 (51.987)\tPrec@5 73.438 (81.322)\n",
            " * Train[55.890 %, 85.458 %, 1.745 loss] Val [52.020 %, 81.360%, 1.982 loss] Best: 52.020 %\n",
            "Time for 12 / 150 21.04299783706665\n",
            "Learning rate:  0.03\n",
            "Epoch: [13][0/782]\tTime 0.164 (0.164)\tLoss (Class) 1.5085 (1.5085)\tLoss (Regu) 0.1813 (0.1813)\tPrec@1 57.812 (57.812)\tPrec@5 92.188 (92.188)\n",
            "Epoch: [13][50/782]\tTime 0.022 (0.028)\tLoss (Class) 2.0119 (1.6907)\tLoss (Regu) 0.1878 (0.1843)\tPrec@1 45.312 (56.924)\tPrec@5 81.250 (86.550)\n",
            "Epoch: [13][100/782]\tTime 0.022 (0.025)\tLoss (Class) 1.6579 (1.6888)\tLoss (Regu) 0.1846 (0.1847)\tPrec@1 62.500 (57.271)\tPrec@5 84.375 (86.433)\n",
            "Epoch: [13][150/782]\tTime 0.022 (0.024)\tLoss (Class) 2.0311 (1.6827)\tLoss (Regu) 0.1818 (0.1851)\tPrec@1 50.000 (57.430)\tPrec@5 78.125 (86.558)\n",
            "Epoch: [13][200/782]\tTime 0.022 (0.024)\tLoss (Class) 1.7011 (1.6826)\tLoss (Regu) 0.1853 (0.1848)\tPrec@1 54.688 (57.572)\tPrec@5 92.188 (86.497)\n",
            "Epoch: [13][250/782]\tTime 0.022 (0.023)\tLoss (Class) 1.4037 (1.6759)\tLoss (Regu) 0.1813 (0.1848)\tPrec@1 64.062 (57.694)\tPrec@5 87.500 (86.492)\n",
            "Epoch: [13][300/782]\tTime 0.022 (0.023)\tLoss (Class) 1.7168 (1.6727)\tLoss (Regu) 0.1859 (0.1849)\tPrec@1 57.812 (57.761)\tPrec@5 85.938 (86.457)\n",
            "Epoch: [13][350/782]\tTime 0.023 (0.023)\tLoss (Class) 1.6670 (1.6731)\tLoss (Regu) 0.1849 (0.1851)\tPrec@1 62.500 (57.870)\tPrec@5 87.500 (86.441)\n",
            "Epoch: [13][400/782]\tTime 0.022 (0.023)\tLoss (Class) 1.7365 (1.6769)\tLoss (Regu) 0.1858 (0.1851)\tPrec@1 53.125 (57.832)\tPrec@5 89.062 (86.378)\n",
            "Epoch: [13][450/782]\tTime 0.024 (0.023)\tLoss (Class) 1.8096 (1.6829)\tLoss (Regu) 0.1884 (0.1849)\tPrec@1 51.562 (57.615)\tPrec@5 81.250 (86.260)\n",
            "Epoch: [13][500/782]\tTime 0.021 (0.023)\tLoss (Class) 1.6048 (1.6857)\tLoss (Regu) 0.1837 (0.1849)\tPrec@1 56.250 (57.513)\tPrec@5 89.062 (86.156)\n",
            "Epoch: [13][550/782]\tTime 0.023 (0.023)\tLoss (Class) 1.8527 (1.6879)\tLoss (Regu) 0.1861 (0.1851)\tPrec@1 64.062 (57.492)\tPrec@5 81.250 (86.164)\n",
            "Epoch: [13][600/782]\tTime 0.024 (0.023)\tLoss (Class) 1.8924 (1.6931)\tLoss (Regu) 0.1866 (0.1851)\tPrec@1 57.812 (57.373)\tPrec@5 79.688 (86.039)\n",
            "Epoch: [13][650/782]\tTime 0.024 (0.023)\tLoss (Class) 1.6487 (1.6936)\tLoss (Regu) 0.1844 (0.1851)\tPrec@1 56.250 (57.366)\tPrec@5 85.938 (86.070)\n",
            "Epoch: [13][700/782]\tTime 0.023 (0.023)\tLoss (Class) 1.7107 (1.6948)\tLoss (Regu) 0.1869 (0.1850)\tPrec@1 56.250 (57.344)\tPrec@5 87.500 (86.107)\n",
            "Epoch: [13][750/782]\tTime 0.023 (0.023)\tLoss (Class) 1.6286 (1.6953)\tLoss (Regu) 0.1845 (0.1850)\tPrec@1 57.812 (57.332)\tPrec@5 85.938 (86.129)\n",
            "Test: [0/157]\tTime 0.113 (0.113)\tLoss (Class) 1.5727 (1.5727)\tLoss (Regu) 0.2117 (0.2117)\tPrec@1 65.625 (65.625)\tPrec@5 93.750 (93.750)\n",
            "Test: [50/157]\tTime 0.008 (0.011)\tLoss (Class) 1.8645 (1.8984)\tLoss (Regu) 0.1981 (0.2046)\tPrec@1 65.625 (55.025)\tPrec@5 79.688 (83.088)\n",
            "Test: [100/157]\tTime 0.008 (0.009)\tLoss (Class) 1.5108 (1.9232)\tLoss (Regu) 0.2050 (0.2049)\tPrec@1 62.500 (54.131)\tPrec@5 89.062 (83.261)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 2.2372 (1.9367)\tLoss (Regu) 0.2054 (0.2050)\tPrec@1 46.875 (53.508)\tPrec@5 75.000 (83.030)\n",
            " * Train[57.294 %, 86.108 %, 1.697 loss] Val [53.510 %, 83.040%, 1.938 loss] Best: 53.510 %\n",
            "Time for 13 / 150 19.630525588989258\n",
            "Learning rate:  0.03\n",
            "Epoch: [14][0/782]\tTime 0.156 (0.156)\tLoss (Class) 2.0716 (2.0716)\tLoss (Regu) 0.1824 (0.1824)\tPrec@1 48.438 (48.438)\tPrec@5 79.688 (79.688)\n",
            "Epoch: [14][50/782]\tTime 0.023 (0.026)\tLoss (Class) 1.8882 (1.6347)\tLoss (Regu) 0.1836 (0.1853)\tPrec@1 50.000 (58.517)\tPrec@5 82.812 (87.653)\n",
            "Epoch: [14][100/782]\tTime 0.023 (0.024)\tLoss (Class) 1.5953 (1.6564)\tLoss (Regu) 0.1844 (0.1849)\tPrec@1 59.375 (58.308)\tPrec@5 85.938 (87.082)\n",
            "Epoch: [14][150/782]\tTime 0.023 (0.024)\tLoss (Class) 1.6967 (1.6515)\tLoss (Regu) 0.1879 (0.1860)\tPrec@1 62.500 (58.547)\tPrec@5 89.062 (87.055)\n",
            "Epoch: [14][200/782]\tTime 0.022 (0.024)\tLoss (Class) 1.8470 (1.6495)\tLoss (Regu) 0.1880 (0.1862)\tPrec@1 48.438 (58.613)\tPrec@5 82.812 (86.971)\n",
            "Epoch: [14][250/782]\tTime 0.023 (0.024)\tLoss (Class) 1.5780 (1.6508)\tLoss (Regu) 0.1874 (0.1863)\tPrec@1 57.812 (58.528)\tPrec@5 92.188 (86.853)\n",
            "Epoch: [14][300/782]\tTime 0.022 (0.023)\tLoss (Class) 1.7610 (1.6471)\tLoss (Regu) 0.1864 (0.1863)\tPrec@1 53.125 (58.846)\tPrec@5 85.938 (86.810)\n",
            "Epoch: [14][350/782]\tTime 0.023 (0.023)\tLoss (Class) 2.0466 (1.6421)\tLoss (Regu) 0.1866 (0.1862)\tPrec@1 54.688 (58.930)\tPrec@5 81.250 (86.952)\n",
            "Epoch: [14][400/782]\tTime 0.022 (0.023)\tLoss (Class) 1.7431 (1.6497)\tLoss (Regu) 0.1817 (0.1860)\tPrec@1 54.688 (58.611)\tPrec@5 82.812 (86.760)\n",
            "Epoch: [14][450/782]\tTime 0.034 (0.024)\tLoss (Class) 1.6840 (1.6553)\tLoss (Regu) 0.1877 (0.1858)\tPrec@1 53.125 (58.350)\tPrec@5 84.375 (86.648)\n",
            "Epoch: [14][500/782]\tTime 0.022 (0.024)\tLoss (Class) 1.3827 (1.6532)\tLoss (Regu) 0.1830 (0.1857)\tPrec@1 64.062 (58.446)\tPrec@5 92.188 (86.664)\n",
            "Epoch: [14][550/782]\tTime 0.032 (0.024)\tLoss (Class) 1.5752 (1.6522)\tLoss (Regu) 0.1845 (0.1856)\tPrec@1 60.938 (58.411)\tPrec@5 90.625 (86.729)\n",
            "Epoch: [14][600/782]\tTime 0.023 (0.024)\tLoss (Class) 1.4839 (1.6527)\tLoss (Regu) 0.1855 (0.1857)\tPrec@1 54.688 (58.410)\tPrec@5 90.625 (86.754)\n",
            "Epoch: [14][650/782]\tTime 0.032 (0.024)\tLoss (Class) 1.7016 (1.6538)\tLoss (Regu) 0.1830 (0.1856)\tPrec@1 54.688 (58.333)\tPrec@5 85.938 (86.766)\n",
            "Epoch: [14][700/782]\tTime 0.023 (0.024)\tLoss (Class) 1.4660 (1.6511)\tLoss (Regu) 0.1835 (0.1854)\tPrec@1 62.500 (58.408)\tPrec@5 93.750 (86.756)\n",
            "Epoch: [14][750/782]\tTime 0.032 (0.024)\tLoss (Class) 1.5542 (1.6535)\tLoss (Regu) 0.1831 (0.1851)\tPrec@1 60.938 (58.351)\tPrec@5 89.062 (86.670)\n",
            "Test: [0/157]\tTime 0.115 (0.115)\tLoss (Class) 1.8194 (1.8194)\tLoss (Regu) 0.2117 (0.2117)\tPrec@1 53.125 (53.125)\tPrec@5 89.062 (89.062)\n",
            "Test: [50/157]\tTime 0.008 (0.010)\tLoss (Class) 2.0642 (2.0617)\tLoss (Regu) 0.2108 (0.2103)\tPrec@1 46.875 (52.420)\tPrec@5 75.000 (80.178)\n",
            "Test: [100/157]\tTime 0.008 (0.009)\tLoss (Class) 2.0576 (2.0451)\tLoss (Regu) 0.2093 (0.2101)\tPrec@1 57.812 (52.321)\tPrec@5 84.375 (80.801)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.7445 (2.0310)\tLoss (Regu) 0.2159 (0.2101)\tPrec@1 57.812 (52.132)\tPrec@5 82.812 (81.126)\n",
            " * Train[58.410 %, 86.684 %, 1.651 loss] Val [52.210 %, 81.160%, 2.028 loss] Best: 53.510 %\n",
            "Time for 14 / 150 20.243256330490112\n",
            "Learning rate:  0.03\n",
            "Epoch: [15][0/782]\tTime 0.146 (0.146)\tLoss (Class) 1.4671 (1.4671)\tLoss (Regu) 0.1836 (0.1836)\tPrec@1 64.062 (64.062)\tPrec@5 87.500 (87.500)\n",
            "Epoch: [15][50/782]\tTime 0.023 (0.026)\tLoss (Class) 1.5990 (1.5572)\tLoss (Regu) 0.1844 (0.1872)\tPrec@1 57.812 (60.999)\tPrec@5 85.938 (88.388)\n",
            "Epoch: [15][100/782]\tTime 0.023 (0.024)\tLoss (Class) 1.7558 (1.5812)\tLoss (Regu) 0.1847 (0.1866)\tPrec@1 56.250 (59.901)\tPrec@5 85.938 (88.103)\n",
            "Epoch: [15][150/782]\tTime 0.024 (0.024)\tLoss (Class) 1.6622 (1.5720)\tLoss (Regu) 0.1861 (0.1863)\tPrec@1 56.250 (60.099)\tPrec@5 90.625 (88.317)\n",
            "Epoch: [15][200/782]\tTime 0.024 (0.024)\tLoss (Class) 1.5842 (1.5866)\tLoss (Regu) 0.1902 (0.1864)\tPrec@1 62.500 (59.857)\tPrec@5 85.938 (88.060)\n",
            "Epoch: [15][250/782]\tTime 0.030 (0.024)\tLoss (Class) 1.8949 (1.5969)\tLoss (Regu) 0.1865 (0.1860)\tPrec@1 56.250 (59.599)\tPrec@5 84.375 (87.886)\n",
            "Epoch: [15][300/782]\tTime 0.032 (0.024)\tLoss (Class) 1.6149 (1.6013)\tLoss (Regu) 0.1824 (0.1859)\tPrec@1 59.375 (59.593)\tPrec@5 89.062 (87.754)\n",
            "Epoch: [15][350/782]\tTime 0.023 (0.025)\tLoss (Class) 1.7688 (1.5960)\tLoss (Regu) 0.1861 (0.1861)\tPrec@1 54.688 (59.789)\tPrec@5 92.188 (88.003)\n",
            "Epoch: [15][400/782]\tTime 0.032 (0.025)\tLoss (Class) 1.3020 (1.5999)\tLoss (Regu) 0.1826 (0.1857)\tPrec@1 62.500 (59.761)\tPrec@5 93.750 (87.882)\n",
            "Epoch: [15][450/782]\tTime 0.022 (0.025)\tLoss (Class) 1.4005 (1.6008)\tLoss (Regu) 0.1860 (0.1854)\tPrec@1 67.188 (59.701)\tPrec@5 89.062 (87.874)\n",
            "Epoch: [15][500/782]\tTime 0.022 (0.025)\tLoss (Class) 1.4190 (1.6043)\tLoss (Regu) 0.1846 (0.1853)\tPrec@1 64.062 (59.487)\tPrec@5 95.312 (87.793)\n",
            "Epoch: [15][550/782]\tTime 0.023 (0.025)\tLoss (Class) 2.0420 (1.6051)\tLoss (Regu) 0.1879 (0.1853)\tPrec@1 53.125 (59.369)\tPrec@5 84.375 (87.769)\n",
            "Epoch: [15][600/782]\tTime 0.023 (0.025)\tLoss (Class) 1.5703 (1.6064)\tLoss (Regu) 0.1843 (0.1851)\tPrec@1 59.375 (59.289)\tPrec@5 90.625 (87.718)\n",
            "Epoch: [15][650/782]\tTime 0.022 (0.025)\tLoss (Class) 1.9182 (1.6089)\tLoss (Regu) 0.1818 (0.1850)\tPrec@1 50.000 (59.269)\tPrec@5 84.375 (87.666)\n",
            "Epoch: [15][700/782]\tTime 0.022 (0.025)\tLoss (Class) 1.5302 (1.6095)\tLoss (Regu) 0.1838 (0.1848)\tPrec@1 54.688 (59.257)\tPrec@5 93.750 (87.654)\n",
            "Epoch: [15][750/782]\tTime 0.023 (0.025)\tLoss (Class) 1.5986 (1.6099)\tLoss (Regu) 0.1834 (0.1846)\tPrec@1 64.062 (59.269)\tPrec@5 85.938 (87.635)\n",
            "Test: [0/157]\tTime 0.114 (0.114)\tLoss (Class) 1.6482 (1.6482)\tLoss (Regu) 0.2043 (0.2043)\tPrec@1 57.812 (57.812)\tPrec@5 87.500 (87.500)\n",
            "Test: [50/157]\tTime 0.008 (0.010)\tLoss (Class) 1.8923 (1.9055)\tLoss (Regu) 0.1948 (0.2003)\tPrec@1 54.688 (54.044)\tPrec@5 82.812 (82.751)\n",
            "Test: [100/157]\tTime 0.008 (0.009)\tLoss (Class) 1.6945 (1.8898)\tLoss (Regu) 0.2036 (0.2004)\tPrec@1 57.812 (54.858)\tPrec@5 90.625 (83.184)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.9452 (1.8931)\tLoss (Regu) 0.2018 (0.2004)\tPrec@1 54.688 (54.656)\tPrec@5 82.812 (83.082)\n",
            " * Train[59.180 %, 87.576 %, 1.613 loss] Val [54.750 %, 83.050%, 1.892 loss] Best: 54.750 %\n",
            "Time for 15 / 150 20.932799577713013\n",
            "Learning rate:  0.03\n",
            "Epoch: [16][0/782]\tTime 0.151 (0.151)\tLoss (Class) 1.3453 (1.3453)\tLoss (Regu) 0.1814 (0.1814)\tPrec@1 68.750 (68.750)\tPrec@5 92.188 (92.188)\n",
            "Epoch: [16][50/782]\tTime 0.023 (0.028)\tLoss (Class) 1.5378 (1.5534)\tLoss (Regu) 0.1835 (0.1849)\tPrec@1 54.688 (61.305)\tPrec@5 93.750 (87.990)\n",
            "Epoch: [16][100/782]\tTime 0.032 (0.026)\tLoss (Class) 1.2706 (1.5311)\tLoss (Regu) 0.1866 (0.1859)\tPrec@1 71.875 (61.587)\tPrec@5 92.188 (88.537)\n",
            "Epoch: [16][150/782]\tTime 0.023 (0.026)\tLoss (Class) 1.3957 (1.5443)\tLoss (Regu) 0.1829 (0.1854)\tPrec@1 60.938 (61.186)\tPrec@5 87.500 (88.535)\n",
            "Epoch: [16][200/782]\tTime 0.022 (0.025)\tLoss (Class) 1.7534 (1.5475)\tLoss (Regu) 0.1869 (0.1855)\tPrec@1 56.250 (60.813)\tPrec@5 85.938 (88.604)\n",
            "Epoch: [16][250/782]\tTime 0.023 (0.024)\tLoss (Class) 2.0386 (1.5558)\tLoss (Regu) 0.1869 (0.1853)\tPrec@1 56.250 (60.570)\tPrec@5 81.250 (88.272)\n",
            "Epoch: [16][300/782]\tTime 0.022 (0.024)\tLoss (Class) 1.6385 (1.5620)\tLoss (Regu) 0.1836 (0.1849)\tPrec@1 54.688 (60.517)\tPrec@5 85.938 (88.170)\n",
            "Epoch: [16][350/782]\tTime 0.023 (0.024)\tLoss (Class) 1.7221 (1.5688)\tLoss (Regu) 0.1809 (0.1846)\tPrec@1 59.375 (60.390)\tPrec@5 87.500 (88.168)\n",
            "Epoch: [16][400/782]\tTime 0.030 (0.025)\tLoss (Class) 1.4686 (1.5728)\tLoss (Regu) 0.1828 (0.1843)\tPrec@1 62.500 (60.189)\tPrec@5 90.625 (88.155)\n",
            "Epoch: [16][450/782]\tTime 0.022 (0.025)\tLoss (Class) 1.6028 (1.5710)\tLoss (Regu) 0.1891 (0.1845)\tPrec@1 59.375 (60.227)\tPrec@5 89.062 (88.196)\n",
            "Epoch: [16][500/782]\tTime 0.030 (0.025)\tLoss (Class) 1.4789 (1.5704)\tLoss (Regu) 0.1861 (0.1847)\tPrec@1 57.812 (60.261)\tPrec@5 87.500 (88.171)\n",
            "Epoch: [16][550/782]\tTime 0.030 (0.025)\tLoss (Class) 1.6041 (1.5716)\tLoss (Regu) 0.1828 (0.1848)\tPrec@1 59.375 (60.183)\tPrec@5 90.625 (88.229)\n",
            "Epoch: [16][600/782]\tTime 0.023 (0.025)\tLoss (Class) 1.5361 (1.5691)\tLoss (Regu) 0.1824 (0.1847)\tPrec@1 60.938 (60.342)\tPrec@5 90.625 (88.264)\n",
            "Epoch: [16][650/782]\tTime 0.023 (0.025)\tLoss (Class) 1.6319 (1.5682)\tLoss (Regu) 0.1813 (0.1846)\tPrec@1 60.938 (60.426)\tPrec@5 89.062 (88.244)\n",
            "Epoch: [16][700/782]\tTime 0.023 (0.025)\tLoss (Class) 1.4918 (1.5720)\tLoss (Regu) 0.1764 (0.1845)\tPrec@1 60.938 (60.351)\tPrec@5 92.188 (88.198)\n",
            "Epoch: [16][750/782]\tTime 0.023 (0.025)\tLoss (Class) 1.5479 (1.5725)\tLoss (Regu) 0.1856 (0.1843)\tPrec@1 65.625 (60.405)\tPrec@5 87.500 (88.105)\n",
            "Test: [0/157]\tTime 0.110 (0.110)\tLoss (Class) 2.1286 (2.1286)\tLoss (Regu) 0.2067 (0.2067)\tPrec@1 48.438 (48.438)\tPrec@5 82.812 (82.812)\n",
            "Test: [50/157]\tTime 0.007 (0.011)\tLoss (Class) 1.8228 (1.8709)\tLoss (Regu) 0.2088 (0.2053)\tPrec@1 51.562 (55.576)\tPrec@5 90.625 (84.222)\n",
            "Test: [100/157]\tTime 0.007 (0.010)\tLoss (Class) 2.0780 (1.9047)\tLoss (Regu) 0.2012 (0.2051)\tPrec@1 42.188 (54.270)\tPrec@5 76.562 (83.400)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.6958 (1.9101)\tLoss (Regu) 0.1989 (0.2052)\tPrec@1 57.812 (54.677)\tPrec@5 90.625 (83.154)\n",
            " * Train[60.352 %, 88.080 %, 1.575 loss] Val [54.610 %, 83.220%, 1.910 loss] Best: 54.750 %\n",
            "Time for 16 / 150 20.67502999305725\n",
            "Learning rate:  0.03\n",
            "Epoch: [17][0/782]\tTime 0.160 (0.160)\tLoss (Class) 1.5265 (1.5265)\tLoss (Regu) 0.1805 (0.1805)\tPrec@1 57.812 (57.812)\tPrec@5 87.500 (87.500)\n",
            "Epoch: [17][50/782]\tTime 0.022 (0.027)\tLoss (Class) 1.4400 (1.5289)\tLoss (Regu) 0.1838 (0.1827)\tPrec@1 71.875 (62.224)\tPrec@5 89.062 (88.572)\n",
            "Epoch: [17][100/782]\tTime 0.022 (0.025)\tLoss (Class) 1.4571 (1.5012)\tLoss (Regu) 0.1838 (0.1842)\tPrec@1 64.062 (62.577)\tPrec@5 89.062 (89.047)\n",
            "Epoch: [17][150/782]\tTime 0.024 (0.024)\tLoss (Class) 1.1872 (1.4995)\tLoss (Regu) 0.1881 (0.1852)\tPrec@1 73.438 (62.428)\tPrec@5 92.188 (89.125)\n",
            "Epoch: [17][200/782]\tTime 0.032 (0.024)\tLoss (Class) 1.7997 (1.5137)\tLoss (Regu) 0.1834 (0.1854)\tPrec@1 57.812 (62.010)\tPrec@5 79.688 (89.008)\n",
            "Epoch: [17][250/782]\tTime 0.022 (0.024)\tLoss (Class) 1.5524 (1.5157)\tLoss (Regu) 0.1848 (0.1855)\tPrec@1 57.812 (61.902)\tPrec@5 84.375 (88.944)\n",
            "Epoch: [17][300/782]\tTime 0.022 (0.024)\tLoss (Class) 1.1270 (1.5205)\tLoss (Regu) 0.1839 (0.1854)\tPrec@1 68.750 (61.519)\tPrec@5 95.312 (88.959)\n",
            "Epoch: [17][350/782]\tTime 0.022 (0.024)\tLoss (Class) 1.6594 (1.5234)\tLoss (Regu) 0.1848 (0.1853)\tPrec@1 62.500 (61.467)\tPrec@5 87.500 (88.884)\n",
            "Epoch: [17][400/782]\tTime 0.023 (0.024)\tLoss (Class) 1.5373 (1.5216)\tLoss (Regu) 0.1831 (0.1854)\tPrec@1 62.500 (61.561)\tPrec@5 89.062 (88.856)\n",
            "Epoch: [17][450/782]\tTime 0.023 (0.024)\tLoss (Class) 1.7639 (1.5242)\tLoss (Regu) 0.1827 (0.1851)\tPrec@1 50.000 (61.471)\tPrec@5 85.938 (88.817)\n",
            "Epoch: [17][500/782]\tTime 0.022 (0.024)\tLoss (Class) 1.8209 (1.5260)\tLoss (Regu) 0.1846 (0.1851)\tPrec@1 53.125 (61.393)\tPrec@5 79.688 (88.838)\n",
            "Epoch: [17][550/782]\tTime 0.022 (0.024)\tLoss (Class) 1.5056 (1.5303)\tLoss (Regu) 0.1876 (0.1851)\tPrec@1 54.688 (61.326)\tPrec@5 90.625 (88.773)\n",
            "Epoch: [17][600/782]\tTime 0.023 (0.024)\tLoss (Class) 1.6932 (1.5337)\tLoss (Regu) 0.1856 (0.1850)\tPrec@1 60.938 (61.353)\tPrec@5 84.375 (88.691)\n",
            "Epoch: [17][650/782]\tTime 0.022 (0.024)\tLoss (Class) 1.6170 (1.5355)\tLoss (Regu) 0.1828 (0.1849)\tPrec@1 56.250 (61.312)\tPrec@5 87.500 (88.652)\n",
            "Epoch: [17][700/782]\tTime 0.022 (0.024)\tLoss (Class) 1.5255 (1.5350)\tLoss (Regu) 0.1839 (0.1850)\tPrec@1 60.938 (61.292)\tPrec@5 90.625 (88.648)\n",
            "Epoch: [17][750/782]\tTime 0.022 (0.024)\tLoss (Class) 1.6862 (1.5344)\tLoss (Regu) 0.1838 (0.1850)\tPrec@1 57.812 (61.283)\tPrec@5 90.625 (88.684)\n",
            "Test: [0/157]\tTime 0.113 (0.113)\tLoss (Class) 1.3886 (1.3886)\tLoss (Regu) 0.2026 (0.2026)\tPrec@1 65.625 (65.625)\tPrec@5 92.188 (92.188)\n",
            "Test: [50/157]\tTime 0.010 (0.011)\tLoss (Class) 1.6678 (1.7593)\tLoss (Regu) 0.2065 (0.2040)\tPrec@1 56.250 (57.782)\tPrec@5 87.500 (85.692)\n",
            "Test: [100/157]\tTime 0.008 (0.010)\tLoss (Class) 2.0048 (1.7824)\tLoss (Regu) 0.2114 (0.2036)\tPrec@1 53.125 (57.132)\tPrec@5 78.125 (85.303)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 2.0513 (1.7949)\tLoss (Regu) 0.2070 (0.2035)\tPrec@1 56.250 (56.850)\tPrec@5 79.688 (85.089)\n",
            " * Train[61.194 %, 88.640 %, 1.538 loss] Val [56.840 %, 85.080%, 1.796 loss] Best: 56.840 %\n",
            "Time for 17 / 150 20.41386127471924\n",
            "Learning rate:  0.03\n",
            "Epoch: [18][0/782]\tTime 0.148 (0.148)\tLoss (Class) 1.5009 (1.5009)\tLoss (Regu) 0.1840 (0.1840)\tPrec@1 56.250 (56.250)\tPrec@5 90.625 (90.625)\n",
            "Epoch: [18][50/782]\tTime 0.023 (0.025)\tLoss (Class) 1.4451 (1.4900)\tLoss (Regu) 0.1859 (0.1867)\tPrec@1 70.312 (62.653)\tPrec@5 89.062 (89.277)\n",
            "Epoch: [18][100/782]\tTime 0.024 (0.025)\tLoss (Class) 1.6285 (1.4752)\tLoss (Regu) 0.1906 (0.1880)\tPrec@1 62.500 (63.289)\tPrec@5 84.375 (89.635)\n",
            "Epoch: [18][150/782]\tTime 0.030 (0.025)\tLoss (Class) 1.9834 (1.4858)\tLoss (Regu) 0.1809 (0.1877)\tPrec@1 45.312 (62.779)\tPrec@5 87.500 (89.466)\n",
            "Epoch: [18][200/782]\tTime 0.021 (0.025)\tLoss (Class) 1.4975 (1.4918)\tLoss (Regu) 0.1874 (0.1867)\tPrec@1 56.250 (62.469)\tPrec@5 90.625 (89.272)\n",
            "Epoch: [18][250/782]\tTime 0.023 (0.025)\tLoss (Class) 2.1527 (1.4963)\tLoss (Regu) 0.1820 (0.1862)\tPrec@1 39.062 (62.245)\tPrec@5 84.375 (89.119)\n",
            "Epoch: [18][300/782]\tTime 0.023 (0.024)\tLoss (Class) 1.6224 (1.4966)\tLoss (Regu) 0.1817 (0.1856)\tPrec@1 57.812 (62.054)\tPrec@5 85.938 (89.208)\n",
            "Epoch: [18][350/782]\tTime 0.023 (0.024)\tLoss (Class) 1.6061 (1.4982)\tLoss (Regu) 0.1829 (0.1853)\tPrec@1 59.375 (62.006)\tPrec@5 81.250 (89.138)\n",
            "Epoch: [18][400/782]\tTime 0.022 (0.024)\tLoss (Class) 1.9356 (1.5010)\tLoss (Regu) 0.1864 (0.1850)\tPrec@1 51.562 (61.877)\tPrec@5 81.250 (89.082)\n",
            "Epoch: [18][450/782]\tTime 0.022 (0.024)\tLoss (Class) 1.6654 (1.4997)\tLoss (Regu) 0.1854 (0.1850)\tPrec@1 59.375 (62.060)\tPrec@5 90.625 (89.056)\n",
            "Epoch: [18][500/782]\tTime 0.022 (0.024)\tLoss (Class) 1.7542 (1.4992)\tLoss (Regu) 0.1804 (0.1850)\tPrec@1 60.938 (62.091)\tPrec@5 82.812 (89.066)\n",
            "Epoch: [18][550/782]\tTime 0.023 (0.024)\tLoss (Class) 1.2204 (1.5013)\tLoss (Regu) 0.1857 (0.1850)\tPrec@1 75.000 (62.157)\tPrec@5 92.188 (88.972)\n",
            "Epoch: [18][600/782]\tTime 0.023 (0.024)\tLoss (Class) 1.4340 (1.5011)\tLoss (Regu) 0.1826 (0.1850)\tPrec@1 60.938 (62.092)\tPrec@5 87.500 (88.979)\n",
            "Epoch: [18][650/782]\tTime 0.030 (0.024)\tLoss (Class) 1.4648 (1.5053)\tLoss (Regu) 0.1858 (0.1847)\tPrec@1 65.625 (61.974)\tPrec@5 93.750 (88.904)\n",
            "Epoch: [18][700/782]\tTime 0.022 (0.024)\tLoss (Class) 1.6342 (1.5062)\tLoss (Regu) 0.1846 (0.1846)\tPrec@1 54.688 (61.956)\tPrec@5 87.500 (88.864)\n",
            "Epoch: [18][750/782]\tTime 0.021 (0.024)\tLoss (Class) 1.4685 (1.5082)\tLoss (Regu) 0.1860 (0.1846)\tPrec@1 62.500 (61.917)\tPrec@5 90.625 (88.857)\n",
            "Test: [0/157]\tTime 0.118 (0.118)\tLoss (Class) 1.9151 (1.9151)\tLoss (Regu) 0.2104 (0.2104)\tPrec@1 56.250 (56.250)\tPrec@5 81.250 (81.250)\n",
            "Test: [50/157]\tTime 0.008 (0.011)\tLoss (Class) 1.4983 (1.8035)\tLoss (Regu) 0.2132 (0.2107)\tPrec@1 62.500 (56.985)\tPrec@5 84.375 (84.467)\n",
            "Test: [100/157]\tTime 0.008 (0.010)\tLoss (Class) 1.7498 (1.7915)\tLoss (Regu) 0.2133 (0.2108)\tPrec@1 59.375 (57.302)\tPrec@5 85.938 (84.916)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.9756 (1.7898)\tLoss (Regu) 0.2109 (0.2111)\tPrec@1 54.688 (57.440)\tPrec@5 81.250 (84.923)\n",
            " * Train[61.926 %, 88.858 %, 1.508 loss] Val [57.420 %, 84.840%, 1.792 loss] Best: 57.420 %\n",
            "Time for 18 / 150 20.335038661956787\n",
            "Learning rate:  0.03\n",
            "Epoch: [19][0/782]\tTime 0.161 (0.161)\tLoss (Class) 1.5011 (1.5011)\tLoss (Regu) 0.1879 (0.1879)\tPrec@1 62.500 (62.500)\tPrec@5 90.625 (90.625)\n",
            "Epoch: [19][50/782]\tTime 0.022 (0.028)\tLoss (Class) 1.4382 (1.4546)\tLoss (Regu) 0.1822 (0.1857)\tPrec@1 62.500 (63.634)\tPrec@5 92.188 (89.277)\n",
            "Epoch: [19][100/782]\tTime 0.021 (0.026)\tLoss (Class) 1.5486 (1.4433)\tLoss (Regu) 0.1877 (0.1850)\tPrec@1 62.500 (64.140)\tPrec@5 87.500 (89.449)\n",
            "Epoch: [19][150/782]\tTime 0.022 (0.026)\tLoss (Class) 1.5341 (1.4572)\tLoss (Regu) 0.1829 (0.1850)\tPrec@1 59.375 (63.721)\tPrec@5 89.062 (89.394)\n",
            "Epoch: [19][200/782]\tTime 0.030 (0.025)\tLoss (Class) 1.5025 (1.4545)\tLoss (Regu) 0.1815 (0.1849)\tPrec@1 64.062 (63.650)\tPrec@5 85.938 (89.638)\n",
            "Epoch: [19][250/782]\tTime 0.022 (0.025)\tLoss (Class) 1.6892 (1.4591)\tLoss (Regu) 0.1865 (0.1849)\tPrec@1 57.812 (63.322)\tPrec@5 89.062 (89.610)\n",
            "Epoch: [19][300/782]\tTime 0.021 (0.025)\tLoss (Class) 1.3377 (1.4597)\tLoss (Regu) 0.1804 (0.1847)\tPrec@1 70.312 (63.388)\tPrec@5 92.188 (89.566)\n",
            "Epoch: [19][350/782]\tTime 0.022 (0.025)\tLoss (Class) 1.6823 (1.4571)\tLoss (Regu) 0.1812 (0.1843)\tPrec@1 60.938 (63.301)\tPrec@5 81.250 (89.583)\n",
            "Epoch: [19][400/782]\tTime 0.022 (0.025)\tLoss (Class) 1.4604 (1.4625)\tLoss (Regu) 0.1878 (0.1841)\tPrec@1 64.062 (63.108)\tPrec@5 87.500 (89.472)\n",
            "Epoch: [19][450/782]\tTime 0.022 (0.025)\tLoss (Class) 1.6563 (1.4661)\tLoss (Regu) 0.1798 (0.1839)\tPrec@1 57.812 (62.950)\tPrec@5 87.500 (89.506)\n",
            "Epoch: [19][500/782]\tTime 0.022 (0.025)\tLoss (Class) 1.7596 (1.4717)\tLoss (Regu) 0.1810 (0.1839)\tPrec@1 51.562 (62.753)\tPrec@5 87.500 (89.409)\n",
            "Epoch: [19][550/782]\tTime 0.030 (0.025)\tLoss (Class) 1.3964 (1.4727)\tLoss (Regu) 0.1858 (0.1838)\tPrec@1 65.625 (62.613)\tPrec@5 90.625 (89.445)\n",
            "Epoch: [19][600/782]\tTime 0.022 (0.025)\tLoss (Class) 1.5750 (1.4762)\tLoss (Regu) 0.1867 (0.1837)\tPrec@1 65.625 (62.562)\tPrec@5 87.500 (89.429)\n",
            "Epoch: [19][650/782]\tTime 0.023 (0.025)\tLoss (Class) 1.6503 (1.4789)\tLoss (Regu) 0.1827 (0.1836)\tPrec@1 60.938 (62.495)\tPrec@5 90.625 (89.427)\n",
            "Epoch: [19][700/782]\tTime 0.023 (0.025)\tLoss (Class) 1.4031 (1.4784)\tLoss (Regu) 0.1922 (0.1836)\tPrec@1 64.062 (62.549)\tPrec@5 89.062 (89.437)\n",
            "Epoch: [19][750/782]\tTime 0.022 (0.025)\tLoss (Class) 1.4060 (1.4770)\tLoss (Regu) 0.1827 (0.1837)\tPrec@1 65.625 (62.554)\tPrec@5 89.062 (89.491)\n",
            "Test: [0/157]\tTime 0.104 (0.104)\tLoss (Class) 2.1984 (2.1984)\tLoss (Regu) 0.2067 (0.2067)\tPrec@1 50.000 (50.000)\tPrec@5 81.250 (81.250)\n",
            "Test: [50/157]\tTime 0.007 (0.010)\tLoss (Class) 2.1414 (1.8386)\tLoss (Regu) 0.2077 (0.2058)\tPrec@1 53.125 (55.882)\tPrec@5 82.812 (85.355)\n",
            "Test: [100/157]\tTime 0.008 (0.009)\tLoss (Class) 1.8167 (1.8370)\tLoss (Regu) 0.2022 (0.2061)\tPrec@1 57.812 (55.894)\tPrec@5 81.250 (85.149)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.7095 (1.8344)\tLoss (Regu) 0.2104 (0.2063)\tPrec@1 64.062 (56.519)\tPrec@5 89.062 (84.882)\n",
            " * Train[62.586 %, 89.460 %, 1.477 loss] Val [56.570 %, 84.870%, 1.831 loss] Best: 57.420 %\n",
            "Time for 19 / 150 20.67107081413269\n",
            "Learning rate:  0.03\n",
            "Epoch: [20][0/782]\tTime 0.147 (0.147)\tLoss (Class) 1.5240 (1.5240)\tLoss (Regu) 0.1823 (0.1823)\tPrec@1 59.375 (59.375)\tPrec@5 84.375 (84.375)\n",
            "Epoch: [20][50/782]\tTime 0.022 (0.025)\tLoss (Class) 1.2979 (1.4067)\tLoss (Regu) 0.1871 (0.1851)\tPrec@1 71.875 (65.441)\tPrec@5 92.188 (90.349)\n",
            "Epoch: [20][100/782]\tTime 0.022 (0.024)\tLoss (Class) 1.4891 (1.4025)\tLoss (Regu) 0.1855 (0.1861)\tPrec@1 64.062 (64.975)\tPrec@5 92.188 (90.532)\n",
            "Epoch: [20][150/782]\tTime 0.022 (0.023)\tLoss (Class) 1.3859 (1.4126)\tLoss (Regu) 0.1841 (0.1861)\tPrec@1 70.312 (64.683)\tPrec@5 92.188 (90.315)\n",
            "Epoch: [20][200/782]\tTime 0.023 (0.023)\tLoss (Class) 1.2603 (1.4268)\tLoss (Regu) 0.1791 (0.1854)\tPrec@1 67.188 (64.280)\tPrec@5 95.312 (90.267)\n",
            "Epoch: [20][250/782]\tTime 0.022 (0.023)\tLoss (Class) 1.5539 (1.4288)\tLoss (Regu) 0.1842 (0.1851)\tPrec@1 64.062 (63.994)\tPrec@5 90.625 (90.332)\n",
            "Epoch: [20][300/782]\tTime 0.030 (0.024)\tLoss (Class) 1.7161 (1.4274)\tLoss (Regu) 0.1811 (0.1850)\tPrec@1 53.125 (64.016)\tPrec@5 85.938 (90.360)\n",
            "Epoch: [20][350/782]\tTime 0.022 (0.024)\tLoss (Class) 1.5038 (1.4231)\tLoss (Regu) 0.1830 (0.1850)\tPrec@1 65.625 (64.098)\tPrec@5 89.062 (90.411)\n",
            "Epoch: [20][400/782]\tTime 0.023 (0.024)\tLoss (Class) 1.3293 (1.4219)\tLoss (Regu) 0.1877 (0.1849)\tPrec@1 59.375 (64.168)\tPrec@5 95.312 (90.333)\n",
            "Epoch: [20][450/782]\tTime 0.022 (0.024)\tLoss (Class) 1.5837 (1.4237)\tLoss (Regu) 0.1858 (0.1852)\tPrec@1 56.250 (64.170)\tPrec@5 85.938 (90.199)\n",
            "Epoch: [20][500/782]\tTime 0.023 (0.024)\tLoss (Class) 1.3917 (1.4270)\tLoss (Regu) 0.1813 (0.1852)\tPrec@1 65.625 (64.022)\tPrec@5 95.312 (90.154)\n",
            "Epoch: [20][550/782]\tTime 0.023 (0.024)\tLoss (Class) 1.5377 (1.4286)\tLoss (Regu) 0.1850 (0.1852)\tPrec@1 60.938 (63.980)\tPrec@5 87.500 (90.132)\n",
            "Epoch: [20][600/782]\tTime 0.023 (0.024)\tLoss (Class) 1.3707 (1.4339)\tLoss (Regu) 0.1822 (0.1851)\tPrec@1 70.312 (63.948)\tPrec@5 87.500 (89.996)\n",
            "Epoch: [20][650/782]\tTime 0.023 (0.024)\tLoss (Class) 1.3840 (1.4349)\tLoss (Regu) 0.1860 (0.1850)\tPrec@1 57.812 (63.909)\tPrec@5 90.625 (90.020)\n",
            "Epoch: [20][700/782]\tTime 0.023 (0.024)\tLoss (Class) 1.3629 (1.4375)\tLoss (Regu) 0.1814 (0.1849)\tPrec@1 60.938 (63.857)\tPrec@5 90.625 (89.914)\n",
            "Epoch: [20][750/782]\tTime 0.023 (0.023)\tLoss (Class) 1.7819 (1.4404)\tLoss (Regu) 0.1810 (0.1848)\tPrec@1 51.562 (63.694)\tPrec@5 82.812 (89.891)\n",
            "Test: [0/157]\tTime 0.117 (0.117)\tLoss (Class) 1.7959 (1.7959)\tLoss (Regu) 0.2145 (0.2145)\tPrec@1 59.375 (59.375)\tPrec@5 82.812 (82.812)\n",
            "Test: [50/157]\tTime 0.008 (0.011)\tLoss (Class) 2.1922 (1.8912)\tLoss (Regu) 0.2060 (0.2106)\tPrec@1 45.312 (55.576)\tPrec@5 79.688 (84.161)\n",
            "Test: [100/157]\tTime 0.008 (0.009)\tLoss (Class) 1.7707 (1.8721)\tLoss (Regu) 0.2081 (0.2103)\tPrec@1 57.812 (55.832)\tPrec@5 90.625 (84.499)\n",
            "Test: [150/157]\tTime 0.008 (0.009)\tLoss (Class) 1.5262 (1.8578)\tLoss (Regu) 0.2167 (0.2105)\tPrec@1 62.500 (56.053)\tPrec@5 90.625 (84.830)\n",
            " * Train[63.640 %, 89.876 %, 1.442 loss] Val [56.070 %, 84.770%, 1.861 loss] Best: 57.420 %\n",
            "Time for 20 / 150 19.81654453277588\n",
            "Learning rate:  0.03\n",
            "Epoch: [21][0/782]\tTime 0.147 (0.147)\tLoss (Class) 1.2580 (1.2580)\tLoss (Regu) 0.1866 (0.1866)\tPrec@1 73.438 (73.438)\tPrec@5 92.188 (92.188)\n",
            "Epoch: [21][50/782]\tTime 0.023 (0.026)\tLoss (Class) 1.2595 (1.3785)\tLoss (Regu) 0.1867 (0.1864)\tPrec@1 70.312 (65.165)\tPrec@5 92.188 (90.931)\n",
            "Epoch: [21][100/782]\tTime 0.023 (0.024)\tLoss (Class) 1.2900 (1.3925)\tLoss (Regu) 0.1846 (0.1855)\tPrec@1 67.188 (64.728)\tPrec@5 90.625 (90.610)\n",
            "Epoch: [21][150/782]\tTime 0.023 (0.024)\tLoss (Class) 1.3988 (1.3917)\tLoss (Regu) 0.1854 (0.1860)\tPrec@1 67.188 (64.808)\tPrec@5 87.500 (90.749)\n",
            "Epoch: [21][200/782]\tTime 0.023 (0.024)\tLoss (Class) 1.6898 (1.3878)\tLoss (Regu) 0.1899 (0.1861)\tPrec@1 56.250 (65.127)\tPrec@5 84.375 (90.742)\n",
            "Epoch: [21][250/782]\tTime 0.022 (0.023)\tLoss (Class) 1.2233 (1.3845)\tLoss (Regu) 0.1930 (0.1864)\tPrec@1 67.188 (65.152)\tPrec@5 93.750 (90.868)\n",
            "Epoch: [21][300/782]\tTime 0.022 (0.023)\tLoss (Class) 1.2047 (1.3910)\tLoss (Regu) 0.1829 (0.1863)\tPrec@1 70.312 (64.976)\tPrec@5 95.312 (90.791)\n",
            "Epoch: [21][350/782]\tTime 0.022 (0.023)\tLoss (Class) 1.1838 (1.3924)\tLoss (Regu) 0.1910 (0.1861)\tPrec@1 67.188 (64.922)\tPrec@5 96.875 (90.745)\n",
            "Epoch: [21][400/782]\tTime 0.032 (0.023)\tLoss (Class) 1.4165 (1.3996)\tLoss (Regu) 0.1816 (0.1861)\tPrec@1 65.625 (64.729)\tPrec@5 87.500 (90.590)\n",
            "Epoch: [21][450/782]\tTime 0.022 (0.023)\tLoss (Class) 1.5167 (1.4028)\tLoss (Regu) 0.1839 (0.1859)\tPrec@1 59.375 (64.658)\tPrec@5 92.188 (90.504)\n",
            "Epoch: [21][500/782]\tTime 0.024 (0.023)\tLoss (Class) 1.6171 (1.4074)\tLoss (Regu) 0.1854 (0.1857)\tPrec@1 62.500 (64.562)\tPrec@5 87.500 (90.435)\n",
            "Epoch: [21][550/782]\tTime 0.023 (0.023)\tLoss (Class) 1.4536 (1.4098)\tLoss (Regu) 0.1884 (0.1857)\tPrec@1 64.062 (64.460)\tPrec@5 92.188 (90.449)\n",
            "Epoch: [21][600/782]\tTime 0.023 (0.023)\tLoss (Class) 1.6245 (1.4075)\tLoss (Regu) 0.1904 (0.1856)\tPrec@1 59.375 (64.546)\tPrec@5 82.812 (90.433)\n",
            "Epoch: [21][650/782]\tTime 0.023 (0.023)\tLoss (Class) 1.5543 (1.4079)\tLoss (Regu) 0.1839 (0.1857)\tPrec@1 56.250 (64.483)\tPrec@5 87.500 (90.464)\n",
            "Epoch: [21][700/782]\tTime 0.023 (0.023)\tLoss (Class) 1.7139 (1.4127)\tLoss (Regu) 0.1857 (0.1854)\tPrec@1 56.250 (64.346)\tPrec@5 92.188 (90.404)\n",
            "Epoch: [21][750/782]\tTime 0.023 (0.023)\tLoss (Class) 1.2421 (1.4170)\tLoss (Regu) 0.1868 (0.1854)\tPrec@1 71.875 (64.275)\tPrec@5 87.500 (90.342)\n",
            "Test: [0/157]\tTime 0.111 (0.111)\tLoss (Class) 1.9366 (1.9366)\tLoss (Regu) 0.2099 (0.2099)\tPrec@1 54.688 (54.688)\tPrec@5 79.688 (79.688)\n",
            "Test: [50/157]\tTime 0.010 (0.010)\tLoss (Class) 1.8159 (1.8447)\tLoss (Regu) 0.1982 (0.2053)\tPrec@1 54.688 (56.771)\tPrec@5 82.812 (84.130)\n",
            "Test: [100/157]\tTime 0.013 (0.010)\tLoss (Class) 1.9366 (1.8140)\tLoss (Regu) 0.2107 (0.2048)\tPrec@1 56.250 (57.039)\tPrec@5 79.688 (84.994)\n",
            "Test: [150/157]\tTime 0.007 (0.010)\tLoss (Class) 1.6790 (1.7980)\tLoss (Regu) 0.2073 (0.2048)\tPrec@1 65.625 (57.368)\tPrec@5 85.938 (85.348)\n",
            " * Train[64.230 %, 90.296 %, 1.418 loss] Val [57.410 %, 85.410%, 1.795 loss] Best: 57.420 %\n",
            "Time for 21 / 150 19.797163009643555\n",
            "Learning rate:  0.03\n",
            "Epoch: [22][0/782]\tTime 0.144 (0.144)\tLoss (Class) 1.2755 (1.2755)\tLoss (Regu) 0.1864 (0.1864)\tPrec@1 62.500 (62.500)\tPrec@5 93.750 (93.750)\n",
            "Epoch: [22][50/782]\tTime 0.023 (0.025)\tLoss (Class) 1.0773 (1.3508)\tLoss (Regu) 0.1879 (0.1870)\tPrec@1 68.750 (64.890)\tPrec@5 98.438 (91.513)\n",
            "Epoch: [22][100/782]\tTime 0.032 (0.025)\tLoss (Class) 1.5821 (1.3552)\tLoss (Regu) 0.1832 (0.1866)\tPrec@1 60.938 (65.424)\tPrec@5 87.500 (91.166)\n",
            "Epoch: [22][150/782]\tTime 0.023 (0.025)\tLoss (Class) 1.5080 (1.3752)\tLoss (Regu) 0.1807 (0.1859)\tPrec@1 57.812 (64.952)\tPrec@5 90.625 (90.946)\n",
            "Epoch: [22][200/782]\tTime 0.031 (0.025)\tLoss (Class) 1.3342 (1.3732)\tLoss (Regu) 0.1894 (0.1857)\tPrec@1 65.625 (64.941)\tPrec@5 93.750 (91.154)\n",
            "Epoch: [22][250/782]\tTime 0.022 (0.024)\tLoss (Class) 1.4466 (1.3768)\tLoss (Regu) 0.1854 (0.1859)\tPrec@1 60.938 (64.934)\tPrec@5 90.625 (90.886)\n",
            "Epoch: [22][300/782]\tTime 0.030 (0.024)\tLoss (Class) 1.6131 (1.3896)\tLoss (Regu) 0.1823 (0.1853)\tPrec@1 65.625 (64.711)\tPrec@5 87.500 (90.718)\n",
            "Epoch: [22][350/782]\tTime 0.022 (0.024)\tLoss (Class) 1.1760 (1.3885)\tLoss (Regu) 0.1877 (0.1853)\tPrec@1 75.000 (64.775)\tPrec@5 92.188 (90.656)\n",
            "Epoch: [22][400/782]\tTime 0.033 (0.024)\tLoss (Class) 1.2968 (1.3893)\tLoss (Regu) 0.1845 (0.1852)\tPrec@1 67.188 (64.721)\tPrec@5 93.750 (90.629)\n",
            "Epoch: [22][450/782]\tTime 0.022 (0.024)\tLoss (Class) 1.5316 (1.3879)\tLoss (Regu) 0.1864 (0.1854)\tPrec@1 64.062 (64.780)\tPrec@5 87.500 (90.674)\n",
            "Epoch: [22][500/782]\tTime 0.022 (0.024)\tLoss (Class) 1.6256 (1.3956)\tLoss (Regu) 0.1821 (0.1853)\tPrec@1 62.500 (64.530)\tPrec@5 87.500 (90.619)\n",
            "Epoch: [22][550/782]\tTime 0.023 (0.024)\tLoss (Class) 1.3491 (1.3979)\tLoss (Regu) 0.1876 (0.1852)\tPrec@1 67.188 (64.442)\tPrec@5 89.062 (90.616)\n",
            "Epoch: [22][600/782]\tTime 0.032 (0.024)\tLoss (Class) 1.7775 (1.3966)\tLoss (Regu) 0.1833 (0.1852)\tPrec@1 56.250 (64.525)\tPrec@5 85.938 (90.659)\n",
            "Epoch: [22][650/782]\tTime 0.022 (0.024)\tLoss (Class) 1.5350 (1.3985)\tLoss (Regu) 0.1806 (0.1851)\tPrec@1 56.250 (64.451)\tPrec@5 87.500 (90.632)\n",
            "Epoch: [22][700/782]\tTime 0.023 (0.024)\tLoss (Class) 1.3110 (1.3972)\tLoss (Regu) 0.1787 (0.1850)\tPrec@1 65.625 (64.508)\tPrec@5 90.625 (90.703)\n",
            "Epoch: [22][750/782]\tTime 0.022 (0.024)\tLoss (Class) 1.3251 (1.3973)\tLoss (Regu) 0.1866 (0.1849)\tPrec@1 68.750 (64.506)\tPrec@5 87.500 (90.667)\n",
            "Test: [0/157]\tTime 0.107 (0.107)\tLoss (Class) 1.9285 (1.9285)\tLoss (Regu) 0.2088 (0.2088)\tPrec@1 53.125 (53.125)\tPrec@5 82.812 (82.812)\n",
            "Test: [50/157]\tTime 0.008 (0.010)\tLoss (Class) 1.3886 (1.7821)\tLoss (Regu) 0.2093 (0.2075)\tPrec@1 68.750 (57.230)\tPrec@5 89.062 (85.049)\n",
            "Test: [100/157]\tTime 0.009 (0.009)\tLoss (Class) 2.1925 (1.7787)\tLoss (Regu) 0.2043 (0.2076)\tPrec@1 50.000 (57.534)\tPrec@5 79.688 (85.102)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 2.5130 (1.8013)\tLoss (Regu) 0.2032 (0.2074)\tPrec@1 43.750 (56.695)\tPrec@5 71.875 (85.130)\n",
            " * Train[64.532 %, 90.658 %, 1.397 loss] Val [56.570 %, 85.140%, 1.801 loss] Best: 57.420 %\n",
            "Time for 22 / 150 20.442058563232422\n",
            "Learning rate:  0.03\n",
            "Epoch: [23][0/782]\tTime 0.147 (0.147)\tLoss (Class) 1.5217 (1.5217)\tLoss (Regu) 0.1848 (0.1848)\tPrec@1 70.312 (70.312)\tPrec@5 85.938 (85.938)\n",
            "Epoch: [23][50/782]\tTime 0.032 (0.028)\tLoss (Class) 1.1554 (1.3587)\tLoss (Regu) 0.1798 (0.1838)\tPrec@1 70.312 (65.778)\tPrec@5 93.750 (90.962)\n",
            "Epoch: [23][100/782]\tTime 0.022 (0.027)\tLoss (Class) 1.4741 (1.3557)\tLoss (Regu) 0.1846 (0.1838)\tPrec@1 68.750 (65.888)\tPrec@5 89.062 (91.151)\n",
            "Epoch: [23][150/782]\tTime 0.023 (0.025)\tLoss (Class) 1.0061 (1.3326)\tLoss (Regu) 0.1875 (0.1849)\tPrec@1 73.438 (66.184)\tPrec@5 92.188 (91.546)\n",
            "Epoch: [23][200/782]\tTime 0.023 (0.025)\tLoss (Class) 1.2848 (1.3241)\tLoss (Regu) 0.1836 (0.1860)\tPrec@1 65.625 (66.387)\tPrec@5 90.625 (91.651)\n",
            "Epoch: [23][250/782]\tTime 0.023 (0.025)\tLoss (Class) 1.2948 (1.3343)\tLoss (Regu) 0.1873 (0.1858)\tPrec@1 65.625 (66.360)\tPrec@5 90.625 (91.472)\n",
            "Epoch: [23][300/782]\tTime 0.033 (0.025)\tLoss (Class) 1.2623 (1.3412)\tLoss (Regu) 0.1864 (0.1860)\tPrec@1 68.750 (66.014)\tPrec@5 92.188 (91.373)\n",
            "Epoch: [23][350/782]\tTime 0.023 (0.024)\tLoss (Class) 1.1403 (1.3403)\tLoss (Regu) 0.1861 (0.1861)\tPrec@1 75.000 (66.017)\tPrec@5 92.188 (91.391)\n",
            "Epoch: [23][400/782]\tTime 0.023 (0.024)\tLoss (Class) 1.6027 (1.3438)\tLoss (Regu) 0.1820 (0.1858)\tPrec@1 59.375 (65.890)\tPrec@5 87.500 (91.377)\n",
            "Epoch: [23][450/782]\tTime 0.023 (0.024)\tLoss (Class) 1.2437 (1.3496)\tLoss (Regu) 0.1879 (0.1855)\tPrec@1 65.625 (65.646)\tPrec@5 89.062 (91.266)\n",
            "Epoch: [23][500/782]\tTime 0.022 (0.024)\tLoss (Class) 1.1933 (1.3484)\tLoss (Regu) 0.1806 (0.1854)\tPrec@1 67.188 (65.731)\tPrec@5 93.750 (91.233)\n",
            "Epoch: [23][550/782]\tTime 0.022 (0.024)\tLoss (Class) 1.5859 (1.3530)\tLoss (Regu) 0.1820 (0.1854)\tPrec@1 62.500 (65.682)\tPrec@5 90.625 (91.181)\n",
            "Epoch: [23][600/782]\tTime 0.022 (0.024)\tLoss (Class) 1.5144 (1.3573)\tLoss (Regu) 0.1846 (0.1851)\tPrec@1 57.812 (65.656)\tPrec@5 89.062 (91.077)\n",
            "Epoch: [23][650/782]\tTime 0.032 (0.024)\tLoss (Class) 1.3416 (1.3603)\tLoss (Regu) 0.1834 (0.1850)\tPrec@1 68.750 (65.692)\tPrec@5 92.188 (91.007)\n",
            "Epoch: [23][700/782]\tTime 0.022 (0.024)\tLoss (Class) 1.1568 (1.3640)\tLoss (Regu) 0.1810 (0.1850)\tPrec@1 71.875 (65.589)\tPrec@5 95.312 (90.901)\n",
            "Epoch: [23][750/782]\tTime 0.023 (0.024)\tLoss (Class) 1.5715 (1.3674)\tLoss (Regu) 0.1795 (0.1849)\tPrec@1 64.062 (65.434)\tPrec@5 85.938 (90.900)\n",
            "Test: [0/157]\tTime 0.112 (0.112)\tLoss (Class) 1.6214 (1.6214)\tLoss (Regu) 0.1944 (0.1944)\tPrec@1 65.625 (65.625)\tPrec@5 89.062 (89.062)\n",
            "Test: [50/157]\tTime 0.008 (0.010)\tLoss (Class) 1.8001 (1.7385)\tLoss (Regu) 0.2042 (0.1981)\tPrec@1 57.812 (58.150)\tPrec@5 87.500 (86.336)\n",
            "Test: [100/157]\tTime 0.009 (0.009)\tLoss (Class) 1.8458 (1.7519)\tLoss (Regu) 0.1964 (0.1981)\tPrec@1 51.562 (57.302)\tPrec@5 87.500 (86.123)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.3636 (1.7542)\tLoss (Regu) 0.1998 (0.1978)\tPrec@1 67.188 (57.461)\tPrec@5 93.750 (85.834)\n",
            " * Train[65.376 %, 90.856 %, 1.371 loss] Val [57.530 %, 85.780%, 1.755 loss] Best: 57.530 %\n",
            "Time for 23 / 150 20.39983582496643\n",
            "Learning rate:  0.03\n",
            "Epoch: [24][0/782]\tTime 0.157 (0.157)\tLoss (Class) 1.4949 (1.4949)\tLoss (Regu) 0.1810 (0.1810)\tPrec@1 54.688 (54.688)\tPrec@5 90.625 (90.625)\n",
            "Epoch: [24][50/782]\tTime 0.023 (0.026)\tLoss (Class) 1.0389 (1.3073)\tLoss (Regu) 0.1886 (0.1837)\tPrec@1 79.688 (66.636)\tPrec@5 96.875 (92.739)\n",
            "Epoch: [24][100/782]\tTime 0.023 (0.024)\tLoss (Class) 1.5994 (1.3079)\tLoss (Regu) 0.1846 (0.1850)\tPrec@1 53.125 (66.801)\tPrec@5 85.938 (92.265)\n",
            "Epoch: [24][150/782]\tTime 0.023 (0.024)\tLoss (Class) 1.3012 (1.3090)\tLoss (Regu) 0.1851 (0.1851)\tPrec@1 67.188 (66.649)\tPrec@5 90.625 (92.229)\n",
            "Epoch: [24][200/782]\tTime 0.022 (0.024)\tLoss (Class) 1.5955 (1.3190)\tLoss (Regu) 0.1862 (0.1848)\tPrec@1 62.500 (66.535)\tPrec@5 87.500 (92.009)\n",
            "Epoch: [24][250/782]\tTime 0.023 (0.024)\tLoss (Class) 1.4973 (1.3185)\tLoss (Regu) 0.1841 (0.1848)\tPrec@1 59.375 (66.646)\tPrec@5 90.625 (91.963)\n",
            "Epoch: [24][300/782]\tTime 0.022 (0.023)\tLoss (Class) 1.1012 (1.3204)\tLoss (Regu) 0.1809 (0.1847)\tPrec@1 71.875 (66.783)\tPrec@5 96.875 (91.772)\n",
            "Epoch: [24][350/782]\tTime 0.023 (0.023)\tLoss (Class) 1.5165 (1.3258)\tLoss (Regu) 0.1835 (0.1846)\tPrec@1 64.062 (66.667)\tPrec@5 87.500 (91.636)\n",
            "Epoch: [24][400/782]\tTime 0.036 (0.024)\tLoss (Class) 1.4279 (1.3324)\tLoss (Regu) 0.1820 (0.1844)\tPrec@1 62.500 (66.322)\tPrec@5 93.750 (91.611)\n",
            "Epoch: [24][450/782]\tTime 0.022 (0.023)\tLoss (Class) 1.4543 (1.3334)\tLoss (Regu) 0.1861 (0.1846)\tPrec@1 68.750 (66.155)\tPrec@5 87.500 (91.602)\n",
            "Epoch: [24][500/782]\tTime 0.031 (0.024)\tLoss (Class) 1.0420 (1.3369)\tLoss (Regu) 0.1844 (0.1847)\tPrec@1 75.000 (66.177)\tPrec@5 95.312 (91.545)\n",
            "Epoch: [24][550/782]\tTime 0.022 (0.024)\tLoss (Class) 1.0996 (1.3385)\tLoss (Regu) 0.1847 (0.1846)\tPrec@1 71.875 (66.067)\tPrec@5 96.875 (91.538)\n",
            "Epoch: [24][600/782]\tTime 0.022 (0.024)\tLoss (Class) 1.2711 (1.3396)\tLoss (Regu) 0.1854 (0.1845)\tPrec@1 68.750 (66.062)\tPrec@5 90.625 (91.480)\n",
            "Epoch: [24][650/782]\tTime 0.023 (0.024)\tLoss (Class) 1.2055 (1.3454)\tLoss (Regu) 0.1854 (0.1843)\tPrec@1 73.438 (65.855)\tPrec@5 95.312 (91.415)\n",
            "Epoch: [24][700/782]\tTime 0.022 (0.024)\tLoss (Class) 1.5489 (1.3464)\tLoss (Regu) 0.1813 (0.1843)\tPrec@1 70.312 (65.855)\tPrec@5 89.062 (91.407)\n",
            "Epoch: [24][750/782]\tTime 0.023 (0.024)\tLoss (Class) 1.2920 (1.3481)\tLoss (Regu) 0.1868 (0.1842)\tPrec@1 65.625 (65.754)\tPrec@5 95.312 (91.386)\n",
            "Test: [0/157]\tTime 0.115 (0.115)\tLoss (Class) 1.5804 (1.5804)\tLoss (Regu) 0.2022 (0.2022)\tPrec@1 56.250 (56.250)\tPrec@5 89.062 (89.062)\n",
            "Test: [50/157]\tTime 0.014 (0.013)\tLoss (Class) 2.0998 (1.8784)\tLoss (Regu) 0.2049 (0.2035)\tPrec@1 46.875 (55.668)\tPrec@5 78.125 (84.099)\n",
            "Test: [100/157]\tTime 0.008 (0.011)\tLoss (Class) 2.1241 (1.8359)\tLoss (Regu) 0.2067 (0.2031)\tPrec@1 51.562 (56.033)\tPrec@5 82.812 (84.267)\n",
            "Test: [150/157]\tTime 0.007 (0.010)\tLoss (Class) 1.8838 (1.8298)\tLoss (Regu) 0.2033 (0.2028)\tPrec@1 67.188 (56.147)\tPrec@5 82.812 (84.716)\n",
            " * Train[65.672 %, 91.298 %, 1.352 loss] Val [56.110 %, 84.670%, 1.833 loss] Best: 57.530 %\n",
            "Time for 24 / 150 20.163410902023315\n",
            "Learning rate:  0.03\n",
            "Epoch: [25][0/782]\tTime 0.148 (0.148)\tLoss (Class) 1.4199 (1.4199)\tLoss (Regu) 0.1817 (0.1817)\tPrec@1 64.062 (64.062)\tPrec@5 89.062 (89.062)\n",
            "Epoch: [25][50/782]\tTime 0.032 (0.027)\tLoss (Class) 1.1006 (1.3046)\tLoss (Regu) 0.1878 (0.1845)\tPrec@1 70.312 (67.371)\tPrec@5 93.750 (91.575)\n",
            "Epoch: [25][100/782]\tTime 0.025 (0.025)\tLoss (Class) 1.2889 (1.3055)\tLoss (Regu) 0.1841 (0.1857)\tPrec@1 65.625 (67.389)\tPrec@5 92.188 (91.739)\n",
            "Epoch: [25][150/782]\tTime 0.022 (0.024)\tLoss (Class) 1.2316 (1.2807)\tLoss (Regu) 0.1900 (0.1863)\tPrec@1 70.312 (67.736)\tPrec@5 92.188 (92.291)\n",
            "Epoch: [25][200/782]\tTime 0.024 (0.024)\tLoss (Class) 1.4937 (1.2872)\tLoss (Regu) 0.1867 (0.1861)\tPrec@1 59.375 (67.421)\tPrec@5 87.500 (92.125)\n",
            "Epoch: [25][250/782]\tTime 0.023 (0.024)\tLoss (Class) 1.2811 (1.2944)\tLoss (Regu) 0.1804 (0.1858)\tPrec@1 75.000 (67.219)\tPrec@5 90.625 (92.038)\n",
            "Epoch: [25][300/782]\tTime 0.022 (0.024)\tLoss (Class) 1.1296 (1.2925)\tLoss (Regu) 0.1857 (0.1855)\tPrec@1 75.000 (67.162)\tPrec@5 93.750 (92.110)\n",
            "Epoch: [25][350/782]\tTime 0.023 (0.024)\tLoss (Class) 1.6209 (1.3012)\tLoss (Regu) 0.1830 (0.1851)\tPrec@1 64.062 (66.956)\tPrec@5 87.500 (91.987)\n",
            "Epoch: [25][400/782]\tTime 0.024 (0.024)\tLoss (Class) 1.2875 (1.3040)\tLoss (Regu) 0.1839 (0.1849)\tPrec@1 67.188 (66.876)\tPrec@5 92.188 (91.899)\n",
            "Epoch: [25][450/782]\tTime 0.022 (0.024)\tLoss (Class) 1.4069 (1.3072)\tLoss (Regu) 0.1806 (0.1850)\tPrec@1 65.625 (66.782)\tPrec@5 82.812 (91.851)\n",
            "Epoch: [25][500/782]\tTime 0.024 (0.024)\tLoss (Class) 1.3712 (1.3134)\tLoss (Regu) 0.1821 (0.1848)\tPrec@1 71.875 (66.707)\tPrec@5 92.188 (91.779)\n",
            "Epoch: [25][550/782]\tTime 0.022 (0.024)\tLoss (Class) 1.2181 (1.3169)\tLoss (Regu) 0.1843 (0.1848)\tPrec@1 68.750 (66.669)\tPrec@5 92.188 (91.714)\n",
            "Epoch: [25][600/782]\tTime 0.032 (0.024)\tLoss (Class) 1.3626 (1.3198)\tLoss (Regu) 0.1833 (0.1847)\tPrec@1 68.750 (66.634)\tPrec@5 89.062 (91.634)\n",
            "Epoch: [25][650/782]\tTime 0.022 (0.024)\tLoss (Class) 1.4468 (1.3247)\tLoss (Regu) 0.1806 (0.1845)\tPrec@1 65.625 (66.537)\tPrec@5 92.188 (91.571)\n",
            "Epoch: [25][700/782]\tTime 0.021 (0.024)\tLoss (Class) 1.3644 (1.3279)\tLoss (Regu) 0.1820 (0.1845)\tPrec@1 62.500 (66.488)\tPrec@5 89.062 (91.530)\n",
            "Epoch: [25][750/782]\tTime 0.022 (0.024)\tLoss (Class) 1.2204 (1.3313)\tLoss (Regu) 0.1764 (0.1842)\tPrec@1 70.312 (66.438)\tPrec@5 93.750 (91.478)\n",
            "Test: [0/157]\tTime 0.125 (0.125)\tLoss (Class) 1.3402 (1.3402)\tLoss (Regu) 0.2110 (0.2110)\tPrec@1 70.312 (70.312)\tPrec@5 90.625 (90.625)\n",
            "Test: [50/157]\tTime 0.009 (0.013)\tLoss (Class) 2.1829 (1.6981)\tLoss (Regu) 0.2110 (0.2085)\tPrec@1 54.688 (59.283)\tPrec@5 79.688 (87.316)\n",
            "Test: [100/157]\tTime 0.014 (0.012)\tLoss (Class) 1.8674 (1.7158)\tLoss (Regu) 0.2050 (0.2084)\tPrec@1 57.812 (59.282)\tPrec@5 87.500 (86.928)\n",
            "Test: [150/157]\tTime 0.007 (0.012)\tLoss (Class) 1.5992 (1.7232)\tLoss (Regu) 0.2056 (0.2083)\tPrec@1 62.500 (59.158)\tPrec@5 90.625 (86.548)\n",
            " * Train[66.396 %, 91.482 %, 1.332 loss] Val [59.160 %, 86.540%, 1.722 loss] Best: 59.160 %\n",
            "Time for 25 / 150 20.963953256607056\n",
            "Learning rate:  0.03\n",
            "Epoch: [26][0/782]\tTime 0.151 (0.151)\tLoss (Class) 1.0427 (1.0427)\tLoss (Regu) 0.1865 (0.1865)\tPrec@1 79.688 (79.688)\tPrec@5 93.750 (93.750)\n",
            "Epoch: [26][50/782]\tTime 0.023 (0.027)\tLoss (Class) 1.4701 (1.2867)\tLoss (Regu) 0.1881 (0.1863)\tPrec@1 59.375 (67.586)\tPrec@5 89.062 (92.249)\n",
            "Epoch: [26][100/782]\tTime 0.024 (0.026)\tLoss (Class) 1.8281 (1.2715)\tLoss (Regu) 0.1848 (0.1873)\tPrec@1 57.812 (68.085)\tPrec@5 84.375 (92.497)\n",
            "Epoch: [26][150/782]\tTime 0.023 (0.025)\tLoss (Class) 1.2262 (1.2744)\tLoss (Regu) 0.1858 (0.1862)\tPrec@1 64.062 (68.202)\tPrec@5 89.062 (92.177)\n",
            "Epoch: [26][200/782]\tTime 0.022 (0.025)\tLoss (Class) 1.1887 (1.2649)\tLoss (Regu) 0.1866 (0.1866)\tPrec@1 71.875 (68.299)\tPrec@5 95.312 (92.312)\n",
            "Epoch: [26][250/782]\tTime 0.030 (0.025)\tLoss (Class) 1.5583 (1.2775)\tLoss (Regu) 0.1867 (0.1859)\tPrec@1 60.938 (68.109)\tPrec@5 92.188 (92.156)\n",
            "Epoch: [26][300/782]\tTime 0.023 (0.025)\tLoss (Class) 1.2703 (1.2746)\tLoss (Regu) 0.1873 (0.1856)\tPrec@1 71.875 (67.961)\tPrec@5 93.750 (92.239)\n",
            "Epoch: [26][350/782]\tTime 0.024 (0.025)\tLoss (Class) 1.2532 (1.2796)\tLoss (Regu) 0.1903 (0.1856)\tPrec@1 75.000 (67.998)\tPrec@5 90.625 (92.174)\n",
            "Epoch: [26][400/782]\tTime 0.026 (0.024)\tLoss (Class) 1.3936 (1.2846)\tLoss (Regu) 0.1856 (0.1859)\tPrec@1 68.750 (67.943)\tPrec@5 89.062 (92.102)\n",
            "Epoch: [26][450/782]\tTime 0.023 (0.024)\tLoss (Class) 1.2109 (1.2898)\tLoss (Regu) 0.1835 (0.1856)\tPrec@1 71.875 (67.794)\tPrec@5 93.750 (92.087)\n",
            "Epoch: [26][500/782]\tTime 0.023 (0.024)\tLoss (Class) 1.6021 (1.2923)\tLoss (Regu) 0.1807 (0.1855)\tPrec@1 62.500 (67.699)\tPrec@5 85.938 (92.028)\n",
            "Epoch: [26][550/782]\tTime 0.030 (0.024)\tLoss (Class) 1.4464 (1.2944)\tLoss (Regu) 0.1865 (0.1852)\tPrec@1 68.750 (67.590)\tPrec@5 85.938 (91.975)\n",
            "Epoch: [26][600/782]\tTime 0.023 (0.024)\tLoss (Class) 1.3792 (1.2985)\tLoss (Regu) 0.1839 (0.1850)\tPrec@1 64.062 (67.440)\tPrec@5 92.188 (91.930)\n",
            "Epoch: [26][650/782]\tTime 0.022 (0.024)\tLoss (Class) 1.3405 (1.3020)\tLoss (Regu) 0.1860 (0.1849)\tPrec@1 68.750 (67.322)\tPrec@5 85.938 (91.895)\n",
            "Epoch: [26][700/782]\tTime 0.032 (0.024)\tLoss (Class) 1.1367 (1.3060)\tLoss (Regu) 0.1835 (0.1847)\tPrec@1 67.188 (67.219)\tPrec@5 93.750 (91.838)\n",
            "Epoch: [26][750/782]\tTime 0.022 (0.024)\tLoss (Class) 1.3747 (1.3069)\tLoss (Regu) 0.1816 (0.1846)\tPrec@1 62.500 (67.150)\tPrec@5 89.062 (91.838)\n",
            "Test: [0/157]\tTime 0.116 (0.116)\tLoss (Class) 2.1837 (2.1837)\tLoss (Regu) 0.2041 (0.2041)\tPrec@1 51.562 (51.562)\tPrec@5 78.125 (78.125)\n",
            "Test: [50/157]\tTime 0.007 (0.011)\tLoss (Class) 1.5163 (1.7559)\tLoss (Regu) 0.2094 (0.2063)\tPrec@1 62.500 (58.640)\tPrec@5 90.625 (85.539)\n",
            "Test: [100/157]\tTime 0.008 (0.010)\tLoss (Class) 1.7880 (1.7980)\tLoss (Regu) 0.2061 (0.2057)\tPrec@1 54.688 (57.766)\tPrec@5 87.500 (85.458)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.6395 (1.7836)\tLoss (Regu) 0.2031 (0.2060)\tPrec@1 62.500 (58.299)\tPrec@5 89.062 (85.648)\n",
            " * Train[67.054 %, 91.814 %, 1.309 loss] Val [58.350 %, 85.740%, 1.779 loss] Best: 59.160 %\n",
            "Time for 26 / 150 20.45542860031128\n",
            "Learning rate:  0.03\n",
            "Epoch: [27][0/782]\tTime 0.151 (0.151)\tLoss (Class) 1.0673 (1.0673)\tLoss (Regu) 0.1866 (0.1866)\tPrec@1 76.562 (76.562)\tPrec@5 93.750 (93.750)\n",
            "Epoch: [27][50/782]\tTime 0.033 (0.028)\tLoss (Class) 1.1662 (1.2255)\tLoss (Regu) 0.1891 (0.1854)\tPrec@1 71.875 (68.903)\tPrec@5 96.875 (92.647)\n",
            "Epoch: [27][100/782]\tTime 0.022 (0.026)\tLoss (Class) 1.1697 (1.2262)\tLoss (Regu) 0.1854 (0.1868)\tPrec@1 76.562 (68.905)\tPrec@5 93.750 (92.853)\n",
            "Epoch: [27][150/782]\tTime 0.023 (0.026)\tLoss (Class) 1.1736 (1.2178)\tLoss (Regu) 0.1884 (0.1866)\tPrec@1 71.875 (69.112)\tPrec@5 90.625 (92.912)\n",
            "Epoch: [27][200/782]\tTime 0.022 (0.025)\tLoss (Class) 1.3308 (1.2328)\tLoss (Regu) 0.1833 (0.1864)\tPrec@1 65.625 (68.797)\tPrec@5 89.062 (92.732)\n",
            "Epoch: [27][250/782]\tTime 0.034 (0.025)\tLoss (Class) 1.1932 (1.2415)\tLoss (Regu) 0.1845 (0.1861)\tPrec@1 68.750 (68.520)\tPrec@5 90.625 (92.611)\n",
            "Epoch: [27][300/782]\tTime 0.023 (0.025)\tLoss (Class) 1.2287 (1.2518)\tLoss (Regu) 0.1882 (0.1858)\tPrec@1 67.188 (68.272)\tPrec@5 90.625 (92.540)\n",
            "Epoch: [27][350/782]\tTime 0.023 (0.025)\tLoss (Class) 0.9508 (1.2572)\tLoss (Regu) 0.1880 (0.1859)\tPrec@1 81.250 (68.100)\tPrec@5 98.438 (92.481)\n",
            "Epoch: [27][400/782]\tTime 0.023 (0.024)\tLoss (Class) 1.1862 (1.2600)\tLoss (Regu) 0.1834 (0.1861)\tPrec@1 64.062 (67.994)\tPrec@5 95.312 (92.476)\n",
            "Epoch: [27][450/782]\tTime 0.025 (0.024)\tLoss (Class) 1.1495 (1.2635)\tLoss (Regu) 0.1858 (0.1861)\tPrec@1 67.188 (68.029)\tPrec@5 96.875 (92.461)\n",
            "Epoch: [27][500/782]\tTime 0.023 (0.024)\tLoss (Class) 1.1554 (1.2687)\tLoss (Regu) 0.1834 (0.1859)\tPrec@1 75.000 (68.033)\tPrec@5 92.188 (92.347)\n",
            "Epoch: [27][550/782]\tTime 0.023 (0.024)\tLoss (Class) 1.3429 (1.2745)\tLoss (Regu) 0.1868 (0.1859)\tPrec@1 62.500 (67.888)\tPrec@5 89.062 (92.227)\n",
            "Epoch: [27][600/782]\tTime 0.023 (0.024)\tLoss (Class) 1.1844 (1.2830)\tLoss (Regu) 0.1844 (0.1858)\tPrec@1 70.312 (67.577)\tPrec@5 93.750 (92.136)\n",
            "Epoch: [27][650/782]\tTime 0.023 (0.024)\tLoss (Class) 1.2530 (1.2860)\tLoss (Regu) 0.1893 (0.1857)\tPrec@1 67.188 (67.480)\tPrec@5 92.188 (92.113)\n",
            "Epoch: [27][700/782]\tTime 0.022 (0.024)\tLoss (Class) 1.5579 (1.2892)\tLoss (Regu) 0.1823 (0.1856)\tPrec@1 60.938 (67.379)\tPrec@5 82.812 (92.105)\n",
            "Epoch: [27][750/782]\tTime 0.024 (0.024)\tLoss (Class) 1.1331 (1.2928)\tLoss (Regu) 0.1898 (0.1855)\tPrec@1 71.875 (67.304)\tPrec@5 95.312 (92.077)\n",
            "Test: [0/157]\tTime 0.112 (0.112)\tLoss (Class) 1.8077 (1.8077)\tLoss (Regu) 0.2023 (0.2023)\tPrec@1 60.938 (60.938)\tPrec@5 85.938 (85.938)\n",
            "Test: [50/157]\tTime 0.008 (0.010)\tLoss (Class) 1.3617 (1.6875)\tLoss (Regu) 0.2112 (0.2051)\tPrec@1 70.312 (60.110)\tPrec@5 89.062 (86.857)\n",
            "Test: [100/157]\tTime 0.008 (0.009)\tLoss (Class) 1.6037 (1.7182)\tLoss (Regu) 0.2088 (0.2055)\tPrec@1 68.750 (59.267)\tPrec@5 84.375 (86.634)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.4280 (1.7214)\tLoss (Regu) 0.2074 (0.2054)\tPrec@1 65.625 (59.261)\tPrec@5 89.062 (86.672)\n",
            " * Train[67.252 %, 92.040 %, 1.295 loss] Val [59.290 %, 86.660%, 1.719 loss] Best: 59.290 %\n",
            "Time for 27 / 150 19.980720043182373\n",
            "Learning rate:  0.03\n",
            "Epoch: [28][0/782]\tTime 0.147 (0.147)\tLoss (Class) 1.0225 (1.0225)\tLoss (Regu) 0.1836 (0.1836)\tPrec@1 75.000 (75.000)\tPrec@5 92.188 (92.188)\n",
            "Epoch: [28][50/782]\tTime 0.023 (0.025)\tLoss (Class) 1.2680 (1.2207)\tLoss (Regu) 0.1812 (0.1849)\tPrec@1 68.750 (69.240)\tPrec@5 92.188 (92.923)\n",
            "Epoch: [28][100/782]\tTime 0.022 (0.024)\tLoss (Class) 1.2643 (1.2171)\tLoss (Regu) 0.1811 (0.1849)\tPrec@1 70.312 (69.585)\tPrec@5 90.625 (92.915)\n",
            "Epoch: [28][150/782]\tTime 0.021 (0.024)\tLoss (Class) 1.0396 (1.2033)\tLoss (Regu) 0.1852 (0.1845)\tPrec@1 73.438 (70.054)\tPrec@5 95.312 (93.036)\n",
            "Epoch: [28][200/782]\tTime 0.022 (0.024)\tLoss (Class) 1.1945 (1.2096)\tLoss (Regu) 0.1834 (0.1850)\tPrec@1 68.750 (70.064)\tPrec@5 95.312 (93.027)\n",
            "Epoch: [28][250/782]\tTime 0.021 (0.024)\tLoss (Class) 1.5089 (1.2209)\tLoss (Regu) 0.1874 (0.1846)\tPrec@1 67.188 (69.541)\tPrec@5 89.062 (92.984)\n",
            "Epoch: [28][300/782]\tTime 0.025 (0.024)\tLoss (Class) 1.1586 (1.2259)\tLoss (Regu) 0.1812 (0.1845)\tPrec@1 73.438 (69.461)\tPrec@5 90.625 (92.899)\n",
            "Epoch: [28][350/782]\tTime 0.023 (0.024)\tLoss (Class) 1.3071 (1.2359)\tLoss (Regu) 0.1848 (0.1846)\tPrec@1 68.750 (69.106)\tPrec@5 87.500 (92.793)\n",
            "Epoch: [28][400/782]\tTime 0.023 (0.024)\tLoss (Class) 1.0028 (1.2385)\tLoss (Regu) 0.1881 (0.1846)\tPrec@1 76.562 (69.066)\tPrec@5 92.188 (92.752)\n",
            "Epoch: [28][450/782]\tTime 0.030 (0.024)\tLoss (Class) 1.1380 (1.2486)\tLoss (Regu) 0.1853 (0.1844)\tPrec@1 67.188 (68.885)\tPrec@5 95.312 (92.624)\n",
            "Epoch: [28][500/782]\tTime 0.023 (0.024)\tLoss (Class) 1.2143 (1.2535)\tLoss (Regu) 0.1891 (0.1845)\tPrec@1 70.312 (68.725)\tPrec@5 93.750 (92.546)\n",
            "Epoch: [28][550/782]\tTime 0.030 (0.024)\tLoss (Class) 1.0660 (1.2621)\tLoss (Regu) 0.1865 (0.1845)\tPrec@1 75.000 (68.520)\tPrec@5 95.312 (92.440)\n",
            "Epoch: [28][600/782]\tTime 0.021 (0.024)\tLoss (Class) 1.3525 (1.2645)\tLoss (Regu) 0.1837 (0.1844)\tPrec@1 62.500 (68.391)\tPrec@5 93.750 (92.385)\n",
            "Epoch: [28][650/782]\tTime 0.030 (0.024)\tLoss (Class) 0.8817 (1.2659)\tLoss (Regu) 0.1862 (0.1844)\tPrec@1 85.938 (68.349)\tPrec@5 95.312 (92.353)\n",
            "Epoch: [28][700/782]\tTime 0.023 (0.024)\tLoss (Class) 0.9758 (1.2677)\tLoss (Regu) 0.1834 (0.1844)\tPrec@1 81.250 (68.182)\tPrec@5 93.750 (92.308)\n",
            "Epoch: [28][750/782]\tTime 0.031 (0.024)\tLoss (Class) 1.2985 (1.2702)\tLoss (Regu) 0.1844 (0.1844)\tPrec@1 65.625 (68.063)\tPrec@5 89.062 (92.283)\n",
            "Test: [0/157]\tTime 0.115 (0.115)\tLoss (Class) 1.2929 (1.2929)\tLoss (Regu) 0.2119 (0.2119)\tPrec@1 64.062 (64.062)\tPrec@5 93.750 (93.750)\n",
            "Test: [50/157]\tTime 0.009 (0.010)\tLoss (Class) 1.6705 (1.7263)\tLoss (Regu) 0.2094 (0.2083)\tPrec@1 60.938 (58.456)\tPrec@5 85.938 (86.765)\n",
            "Test: [100/157]\tTime 0.013 (0.010)\tLoss (Class) 1.6958 (1.7358)\tLoss (Regu) 0.2094 (0.2078)\tPrec@1 60.938 (58.803)\tPrec@5 84.375 (86.788)\n",
            "Test: [150/157]\tTime 0.007 (0.010)\tLoss (Class) 1.9351 (1.7457)\tLoss (Regu) 0.2074 (0.2078)\tPrec@1 54.688 (59.096)\tPrec@5 84.375 (86.372)\n",
            " * Train[68.010 %, 92.224 %, 1.273 loss] Val [59.160 %, 86.420%, 1.741 loss] Best: 59.290 %\n",
            "Time for 28 / 150 20.620332717895508\n",
            "Learning rate:  0.03\n",
            "Epoch: [29][0/782]\tTime 0.167 (0.167)\tLoss (Class) 1.0715 (1.0715)\tLoss (Regu) 0.1843 (0.1843)\tPrec@1 75.000 (75.000)\tPrec@5 96.875 (96.875)\n",
            "Epoch: [29][50/782]\tTime 0.023 (0.026)\tLoss (Class) 1.4082 (1.2004)\tLoss (Regu) 0.1859 (0.1838)\tPrec@1 62.500 (70.343)\tPrec@5 90.625 (93.168)\n",
            "Epoch: [29][100/782]\tTime 0.022 (0.024)\tLoss (Class) 1.2557 (1.2127)\tLoss (Regu) 0.1901 (0.1853)\tPrec@1 62.500 (69.431)\tPrec@5 93.750 (93.069)\n",
            "Epoch: [29][150/782]\tTime 0.023 (0.024)\tLoss (Class) 1.4098 (1.2339)\tLoss (Regu) 0.1873 (0.1852)\tPrec@1 67.188 (68.688)\tPrec@5 89.062 (92.684)\n",
            "Epoch: [29][200/782]\tTime 0.022 (0.024)\tLoss (Class) 1.3313 (1.2289)\tLoss (Regu) 0.1786 (0.1851)\tPrec@1 67.188 (69.076)\tPrec@5 96.875 (92.747)\n",
            "Epoch: [29][250/782]\tTime 0.022 (0.024)\tLoss (Class) 1.2749 (1.2305)\tLoss (Regu) 0.1825 (0.1849)\tPrec@1 67.188 (68.887)\tPrec@5 92.188 (92.847)\n",
            "Epoch: [29][300/782]\tTime 0.021 (0.024)\tLoss (Class) 1.0068 (1.2286)\tLoss (Regu) 0.1904 (0.1851)\tPrec@1 75.000 (69.072)\tPrec@5 96.875 (92.873)\n",
            "Epoch: [29][350/782]\tTime 0.023 (0.024)\tLoss (Class) 1.2546 (1.2319)\tLoss (Regu) 0.1852 (0.1851)\tPrec@1 71.875 (69.075)\tPrec@5 95.312 (92.726)\n",
            "Epoch: [29][400/782]\tTime 0.022 (0.024)\tLoss (Class) 1.0636 (1.2352)\tLoss (Regu) 0.1871 (0.1850)\tPrec@1 71.875 (68.894)\tPrec@5 93.750 (92.698)\n",
            "Epoch: [29][450/782]\tTime 0.023 (0.024)\tLoss (Class) 1.0889 (1.2402)\tLoss (Regu) 0.1859 (0.1851)\tPrec@1 75.000 (68.701)\tPrec@5 96.875 (92.673)\n",
            "Epoch: [29][500/782]\tTime 0.023 (0.024)\tLoss (Class) 1.0573 (1.2428)\tLoss (Regu) 0.1821 (0.1851)\tPrec@1 73.438 (68.663)\tPrec@5 93.750 (92.640)\n",
            "Epoch: [29][550/782]\tTime 0.022 (0.024)\tLoss (Class) 1.0766 (1.2441)\tLoss (Regu) 0.1849 (0.1853)\tPrec@1 73.438 (68.639)\tPrec@5 92.188 (92.658)\n",
            "Epoch: [29][600/782]\tTime 0.023 (0.024)\tLoss (Class) 1.3155 (1.2450)\tLoss (Regu) 0.1851 (0.1853)\tPrec@1 62.500 (68.584)\tPrec@5 96.875 (92.671)\n",
            "Epoch: [29][650/782]\tTime 0.023 (0.024)\tLoss (Class) 1.1476 (1.2499)\tLoss (Regu) 0.1851 (0.1852)\tPrec@1 62.500 (68.472)\tPrec@5 98.438 (92.615)\n",
            "Epoch: [29][700/782]\tTime 0.023 (0.024)\tLoss (Class) 1.3257 (1.2468)\tLoss (Regu) 0.1887 (0.1852)\tPrec@1 62.500 (68.545)\tPrec@5 93.750 (92.691)\n",
            "Epoch: [29][750/782]\tTime 0.022 (0.024)\tLoss (Class) 1.3480 (1.2482)\tLoss (Regu) 0.1853 (0.1852)\tPrec@1 62.500 (68.511)\tPrec@5 92.188 (92.697)\n",
            "Test: [0/157]\tTime 0.119 (0.119)\tLoss (Class) 1.6429 (1.6429)\tLoss (Regu) 0.2093 (0.2093)\tPrec@1 67.188 (67.188)\tPrec@5 87.500 (87.500)\n",
            "Test: [50/157]\tTime 0.009 (0.012)\tLoss (Class) 1.3425 (1.8272)\tLoss (Regu) 0.2158 (0.2123)\tPrec@1 68.750 (58.517)\tPrec@5 92.188 (84.804)\n",
            "Test: [100/157]\tTime 0.009 (0.010)\tLoss (Class) 1.8609 (1.8247)\tLoss (Regu) 0.2084 (0.2120)\tPrec@1 51.562 (58.075)\tPrec@5 84.375 (84.932)\n",
            "Test: [150/157]\tTime 0.007 (0.010)\tLoss (Class) 2.1601 (1.8067)\tLoss (Regu) 0.2091 (0.2123)\tPrec@1 53.125 (58.288)\tPrec@5 76.562 (85.234)\n",
            " * Train[68.478 %, 92.660 %, 1.249 loss] Val [58.220 %, 85.300%, 1.808 loss] Best: 59.290 %\n",
            "Time for 29 / 150 20.08253288269043\n",
            "Learning rate:  0.03\n",
            "Epoch: [30][0/782]\tTime 0.151 (0.151)\tLoss (Class) 1.4950 (1.4950)\tLoss (Regu) 0.1868 (0.1868)\tPrec@1 60.938 (60.938)\tPrec@5 92.188 (92.188)\n",
            "Epoch: [30][50/782]\tTime 0.023 (0.026)\tLoss (Class) 1.2830 (1.2316)\tLoss (Regu) 0.1877 (0.1867)\tPrec@1 67.188 (68.168)\tPrec@5 95.312 (93.168)\n",
            "Epoch: [30][100/782]\tTime 0.023 (0.025)\tLoss (Class) 1.2064 (1.2185)\tLoss (Regu) 0.1866 (0.1866)\tPrec@1 68.750 (68.905)\tPrec@5 92.188 (93.363)\n",
            "Epoch: [30][150/782]\tTime 0.023 (0.024)\tLoss (Class) 1.2891 (1.2057)\tLoss (Regu) 0.1885 (0.1865)\tPrec@1 70.312 (69.319)\tPrec@5 96.875 (93.450)\n",
            "Epoch: [30][200/782]\tTime 0.023 (0.024)\tLoss (Class) 0.9272 (1.2066)\tLoss (Regu) 0.1879 (0.1861)\tPrec@1 79.688 (69.512)\tPrec@5 98.438 (93.400)\n",
            "Epoch: [30][250/782]\tTime 0.021 (0.024)\tLoss (Class) 1.0659 (1.2192)\tLoss (Regu) 0.1817 (0.1858)\tPrec@1 67.188 (68.999)\tPrec@5 98.438 (93.215)\n",
            "Epoch: [30][300/782]\tTime 0.024 (0.024)\tLoss (Class) 0.9054 (1.2209)\tLoss (Regu) 0.1896 (0.1859)\tPrec@1 75.000 (68.999)\tPrec@5 98.438 (93.163)\n",
            "Epoch: [30][350/782]\tTime 0.023 (0.025)\tLoss (Class) 1.3816 (1.2200)\tLoss (Regu) 0.1845 (0.1859)\tPrec@1 60.938 (69.022)\tPrec@5 93.750 (93.136)\n",
            "Epoch: [30][400/782]\tTime 0.024 (0.025)\tLoss (Class) 1.4251 (1.2226)\tLoss (Regu) 0.1847 (0.1858)\tPrec@1 62.500 (69.062)\tPrec@5 92.188 (93.099)\n",
            "Epoch: [30][450/782]\tTime 0.022 (0.024)\tLoss (Class) 1.0298 (1.2215)\tLoss (Regu) 0.1852 (0.1856)\tPrec@1 68.750 (69.110)\tPrec@5 95.312 (93.078)\n",
            "Epoch: [30][500/782]\tTime 0.024 (0.024)\tLoss (Class) 1.3784 (1.2253)\tLoss (Regu) 0.1823 (0.1854)\tPrec@1 70.312 (69.006)\tPrec@5 93.750 (93.036)\n",
            "Epoch: [30][550/782]\tTime 0.023 (0.024)\tLoss (Class) 1.1287 (1.2272)\tLoss (Regu) 0.1856 (0.1853)\tPrec@1 70.312 (68.923)\tPrec@5 96.875 (93.041)\n",
            "Epoch: [30][600/782]\tTime 0.023 (0.024)\tLoss (Class) 1.3817 (1.2257)\tLoss (Regu) 0.1812 (0.1852)\tPrec@1 71.875 (69.005)\tPrec@5 87.500 (93.006)\n",
            "Epoch: [30][650/782]\tTime 0.023 (0.024)\tLoss (Class) 1.3300 (1.2291)\tLoss (Regu) 0.1905 (0.1851)\tPrec@1 70.312 (68.911)\tPrec@5 92.188 (92.924)\n",
            "Epoch: [30][700/782]\tTime 0.023 (0.024)\tLoss (Class) 1.0777 (1.2320)\tLoss (Regu) 0.1816 (0.1850)\tPrec@1 70.312 (68.835)\tPrec@5 95.312 (92.863)\n",
            "Epoch: [30][750/782]\tTime 0.023 (0.024)\tLoss (Class) 1.2094 (1.2325)\tLoss (Regu) 0.1805 (0.1848)\tPrec@1 68.750 (68.777)\tPrec@5 93.750 (92.830)\n",
            "Test: [0/157]\tTime 0.117 (0.117)\tLoss (Class) 1.5238 (1.5238)\tLoss (Regu) 0.2063 (0.2063)\tPrec@1 64.062 (64.062)\tPrec@5 87.500 (87.500)\n",
            "Test: [50/157]\tTime 0.008 (0.011)\tLoss (Class) 1.6651 (1.7969)\tLoss (Regu) 0.1985 (0.2038)\tPrec@1 60.938 (58.824)\tPrec@5 87.500 (85.784)\n",
            "Test: [100/157]\tTime 0.008 (0.010)\tLoss (Class) 1.9979 (1.7770)\tLoss (Regu) 0.2101 (0.2039)\tPrec@1 53.125 (58.756)\tPrec@5 81.250 (85.922)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 2.0770 (1.7560)\tLoss (Regu) 0.2112 (0.2039)\tPrec@1 51.562 (59.034)\tPrec@5 82.812 (86.248)\n",
            " * Train[68.800 %, 92.802 %, 1.232 loss] Val [58.950 %, 86.270%, 1.757 loss] Best: 59.290 %\n",
            "Time for 30 / 150 20.47874116897583\n",
            "Learning rate:  0.03\n",
            "Epoch: [31][0/782]\tTime 0.161 (0.161)\tLoss (Class) 1.4236 (1.4236)\tLoss (Regu) 0.1829 (0.1829)\tPrec@1 68.750 (68.750)\tPrec@5 89.062 (89.062)\n",
            "Epoch: [31][50/782]\tTime 0.022 (0.027)\tLoss (Class) 1.1020 (1.1666)\tLoss (Regu) 0.1883 (0.1880)\tPrec@1 65.625 (70.711)\tPrec@5 96.875 (93.934)\n",
            "Epoch: [31][100/782]\tTime 0.022 (0.026)\tLoss (Class) 1.2871 (1.1798)\tLoss (Regu) 0.1858 (0.1876)\tPrec@1 65.625 (70.699)\tPrec@5 92.188 (93.209)\n",
            "Epoch: [31][150/782]\tTime 0.022 (0.025)\tLoss (Class) 1.3791 (1.1754)\tLoss (Regu) 0.1882 (0.1878)\tPrec@1 67.188 (70.726)\tPrec@5 92.188 (93.222)\n",
            "Epoch: [31][200/782]\tTime 0.022 (0.025)\tLoss (Class) 1.0623 (1.1904)\tLoss (Regu) 0.1853 (0.1874)\tPrec@1 70.312 (70.204)\tPrec@5 100.000 (93.206)\n",
            "Epoch: [31][250/782]\tTime 0.022 (0.025)\tLoss (Class) 1.2997 (1.1872)\tLoss (Regu) 0.1903 (0.1875)\tPrec@1 65.625 (70.269)\tPrec@5 95.312 (93.302)\n",
            "Epoch: [31][300/782]\tTime 0.022 (0.025)\tLoss (Class) 1.3605 (1.1895)\tLoss (Regu) 0.1870 (0.1873)\tPrec@1 65.625 (70.287)\tPrec@5 93.750 (93.226)\n",
            "Epoch: [31][350/782]\tTime 0.023 (0.024)\tLoss (Class) 1.2167 (1.1921)\tLoss (Regu) 0.1842 (0.1868)\tPrec@1 64.062 (70.050)\tPrec@5 95.312 (93.243)\n",
            "Epoch: [31][400/782]\tTime 0.023 (0.025)\tLoss (Class) 1.3438 (1.1958)\tLoss (Regu) 0.1836 (0.1865)\tPrec@1 68.750 (69.970)\tPrec@5 87.500 (93.204)\n",
            "Epoch: [31][450/782]\tTime 0.023 (0.025)\tLoss (Class) 1.1489 (1.1965)\tLoss (Regu) 0.1840 (0.1861)\tPrec@1 62.500 (69.952)\tPrec@5 93.750 (93.282)\n",
            "Epoch: [31][500/782]\tTime 0.023 (0.024)\tLoss (Class) 0.9892 (1.2006)\tLoss (Regu) 0.1856 (0.1859)\tPrec@1 71.875 (69.838)\tPrec@5 95.312 (93.260)\n",
            "Epoch: [31][550/782]\tTime 0.023 (0.024)\tLoss (Class) 1.5495 (1.2054)\tLoss (Regu) 0.1840 (0.1857)\tPrec@1 62.500 (69.743)\tPrec@5 84.375 (93.276)\n",
            "Epoch: [31][600/782]\tTime 0.023 (0.024)\tLoss (Class) 1.1727 (1.2078)\tLoss (Regu) 0.1814 (0.1855)\tPrec@1 73.438 (69.738)\tPrec@5 92.188 (93.261)\n",
            "Epoch: [31][650/782]\tTime 0.023 (0.024)\tLoss (Class) 1.3071 (1.2070)\tLoss (Regu) 0.1841 (0.1855)\tPrec@1 68.750 (69.720)\tPrec@5 92.188 (93.287)\n",
            "Epoch: [31][700/782]\tTime 0.023 (0.024)\tLoss (Class) 1.2218 (1.2110)\tLoss (Regu) 0.1819 (0.1853)\tPrec@1 70.312 (69.597)\tPrec@5 95.312 (93.257)\n",
            "Epoch: [31][750/782]\tTime 0.023 (0.024)\tLoss (Class) 1.1223 (1.2133)\tLoss (Regu) 0.1796 (0.1852)\tPrec@1 71.875 (69.561)\tPrec@5 89.062 (93.188)\n",
            "Test: [0/157]\tTime 0.116 (0.116)\tLoss (Class) 2.6567 (2.6567)\tLoss (Regu) 0.2073 (0.2073)\tPrec@1 43.750 (43.750)\tPrec@5 78.125 (78.125)\n",
            "Test: [50/157]\tTime 0.008 (0.012)\tLoss (Class) 2.1935 (1.8573)\tLoss (Regu) 0.2067 (0.2020)\tPrec@1 54.688 (57.874)\tPrec@5 82.812 (84.222)\n",
            "Test: [100/157]\tTime 0.008 (0.010)\tLoss (Class) 1.8466 (1.8588)\tLoss (Regu) 0.2009 (0.2018)\tPrec@1 53.125 (57.287)\tPrec@5 85.938 (84.607)\n",
            "Test: [150/157]\tTime 0.007 (0.010)\tLoss (Class) 1.6217 (1.8310)\tLoss (Regu) 0.1999 (0.2018)\tPrec@1 56.250 (57.781)\tPrec@5 89.062 (85.203)\n",
            " * Train[69.470 %, 93.108 %, 1.217 loss] Val [57.950 %, 85.240%, 1.828 loss] Best: 59.290 %\n",
            "Time for 31 / 150 20.27817177772522\n",
            "Learning rate:  0.03\n",
            "Epoch: [32][0/782]\tTime 0.161 (0.161)\tLoss (Class) 0.9609 (0.9609)\tLoss (Regu) 0.1800 (0.1800)\tPrec@1 70.312 (70.312)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [32][50/782]\tTime 0.022 (0.027)\tLoss (Class) 1.4600 (1.1637)\tLoss (Regu) 0.1854 (0.1867)\tPrec@1 60.938 (70.159)\tPrec@5 92.188 (94.485)\n",
            "Epoch: [32][100/782]\tTime 0.023 (0.026)\tLoss (Class) 1.1995 (1.1817)\tLoss (Regu) 0.1841 (0.1862)\tPrec@1 67.188 (69.725)\tPrec@5 92.188 (93.998)\n",
            "Epoch: [32][150/782]\tTime 0.023 (0.025)\tLoss (Class) 1.2918 (1.1766)\tLoss (Regu) 0.1857 (0.1861)\tPrec@1 68.750 (69.961)\tPrec@5 93.750 (94.154)\n",
            "Epoch: [32][200/782]\tTime 0.024 (0.024)\tLoss (Class) 1.0403 (1.1850)\tLoss (Regu) 0.1852 (0.1862)\tPrec@1 75.000 (69.831)\tPrec@5 93.750 (93.952)\n",
            "Epoch: [32][250/782]\tTime 0.023 (0.024)\tLoss (Class) 1.0560 (1.1801)\tLoss (Regu) 0.1836 (0.1861)\tPrec@1 75.000 (70.207)\tPrec@5 98.438 (93.837)\n",
            "Epoch: [32][300/782]\tTime 0.030 (0.024)\tLoss (Class) 1.2796 (1.1856)\tLoss (Regu) 0.1830 (0.1860)\tPrec@1 71.875 (70.079)\tPrec@5 89.062 (93.719)\n",
            "Epoch: [32][350/782]\tTime 0.023 (0.024)\tLoss (Class) 1.0591 (1.1861)\tLoss (Regu) 0.1850 (0.1856)\tPrec@1 76.562 (70.197)\tPrec@5 96.875 (93.674)\n",
            "Epoch: [32][400/782]\tTime 0.023 (0.024)\tLoss (Class) 1.3513 (1.1852)\tLoss (Regu) 0.1873 (0.1858)\tPrec@1 64.062 (70.270)\tPrec@5 90.625 (93.695)\n",
            "Epoch: [32][450/782]\tTime 0.023 (0.024)\tLoss (Class) 1.2864 (1.1875)\tLoss (Regu) 0.1860 (0.1857)\tPrec@1 67.188 (70.215)\tPrec@5 93.750 (93.695)\n",
            "Epoch: [32][500/782]\tTime 0.022 (0.024)\tLoss (Class) 1.2292 (1.1877)\tLoss (Regu) 0.1865 (0.1857)\tPrec@1 64.062 (70.138)\tPrec@5 92.188 (93.722)\n",
            "Epoch: [32][550/782]\tTime 0.022 (0.024)\tLoss (Class) 1.2204 (1.1938)\tLoss (Regu) 0.1864 (0.1856)\tPrec@1 68.750 (70.035)\tPrec@5 96.875 (93.605)\n",
            "Epoch: [32][600/782]\tTime 0.023 (0.024)\tLoss (Class) 1.5295 (1.1933)\tLoss (Regu) 0.1826 (0.1856)\tPrec@1 65.625 (70.076)\tPrec@5 92.188 (93.586)\n",
            "Epoch: [32][650/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7941 (1.1947)\tLoss (Regu) 0.1820 (0.1855)\tPrec@1 84.375 (70.051)\tPrec@5 98.438 (93.551)\n",
            "Epoch: [32][700/782]\tTime 0.023 (0.024)\tLoss (Class) 1.0252 (1.1952)\tLoss (Regu) 0.1841 (0.1854)\tPrec@1 81.250 (70.029)\tPrec@5 90.625 (93.503)\n",
            "Epoch: [32][750/782]\tTime 0.023 (0.024)\tLoss (Class) 1.1660 (1.1985)\tLoss (Regu) 0.1856 (0.1853)\tPrec@1 75.000 (69.923)\tPrec@5 92.188 (93.430)\n",
            "Test: [0/157]\tTime 0.112 (0.112)\tLoss (Class) 1.6054 (1.6054)\tLoss (Regu) 0.2123 (0.2123)\tPrec@1 60.938 (60.938)\tPrec@5 87.500 (87.500)\n",
            "Test: [50/157]\tTime 0.007 (0.011)\tLoss (Class) 2.0255 (1.7426)\tLoss (Regu) 0.2013 (0.2080)\tPrec@1 56.250 (58.732)\tPrec@5 84.375 (86.581)\n",
            "Test: [100/157]\tTime 0.008 (0.010)\tLoss (Class) 1.7299 (1.7673)\tLoss (Regu) 0.2102 (0.2078)\tPrec@1 59.375 (58.679)\tPrec@5 89.062 (85.860)\n",
            "Test: [150/157]\tTime 0.008 (0.009)\tLoss (Class) 1.6789 (1.7608)\tLoss (Regu) 0.2018 (0.2076)\tPrec@1 67.188 (58.878)\tPrec@5 85.938 (85.855)\n",
            " * Train[69.800 %, 93.398 %, 1.201 loss] Val [58.850 %, 85.890%, 1.760 loss] Best: 59.290 %\n",
            "Time for 32 / 150 20.025513887405396\n",
            "Learning rate:  0.03\n",
            "Epoch: [33][0/782]\tTime 0.155 (0.155)\tLoss (Class) 1.1268 (1.1268)\tLoss (Regu) 0.1831 (0.1831)\tPrec@1 81.250 (81.250)\tPrec@5 95.312 (95.312)\n",
            "Epoch: [33][50/782]\tTime 0.022 (0.028)\tLoss (Class) 1.2138 (1.1573)\tLoss (Regu) 0.1859 (0.1847)\tPrec@1 67.188 (71.477)\tPrec@5 96.875 (94.087)\n",
            "Epoch: [33][100/782]\tTime 0.030 (0.027)\tLoss (Class) 1.3722 (1.1620)\tLoss (Regu) 0.1868 (0.1856)\tPrec@1 65.625 (71.334)\tPrec@5 95.312 (93.951)\n",
            "Epoch: [33][150/782]\tTime 0.023 (0.026)\tLoss (Class) 1.2823 (1.1590)\tLoss (Regu) 0.1856 (0.1857)\tPrec@1 67.188 (71.182)\tPrec@5 95.312 (93.791)\n",
            "Epoch: [33][200/782]\tTime 0.023 (0.025)\tLoss (Class) 1.3448 (1.1659)\tLoss (Regu) 0.1875 (0.1849)\tPrec@1 53.125 (70.779)\tPrec@5 93.750 (93.727)\n",
            "Epoch: [33][250/782]\tTime 0.023 (0.025)\tLoss (Class) 1.0626 (1.1646)\tLoss (Regu) 0.1834 (0.1849)\tPrec@1 78.125 (70.649)\tPrec@5 95.312 (93.812)\n",
            "Epoch: [33][300/782]\tTime 0.023 (0.024)\tLoss (Class) 0.9064 (1.1664)\tLoss (Regu) 0.1809 (0.1850)\tPrec@1 84.375 (70.655)\tPrec@5 96.875 (93.802)\n",
            "Epoch: [33][350/782]\tTime 0.023 (0.024)\tLoss (Class) 1.0231 (1.1703)\tLoss (Regu) 0.1875 (0.1850)\tPrec@1 76.562 (70.544)\tPrec@5 96.875 (93.710)\n",
            "Epoch: [33][400/782]\tTime 0.031 (0.024)\tLoss (Class) 1.0039 (1.1680)\tLoss (Regu) 0.1837 (0.1849)\tPrec@1 75.000 (70.648)\tPrec@5 93.750 (93.758)\n",
            "Epoch: [33][450/782]\tTime 0.023 (0.024)\tLoss (Class) 1.2090 (1.1704)\tLoss (Regu) 0.1866 (0.1849)\tPrec@1 70.312 (70.503)\tPrec@5 93.750 (93.733)\n",
            "Epoch: [33][500/782]\tTime 0.022 (0.024)\tLoss (Class) 1.0885 (1.1744)\tLoss (Regu) 0.1857 (0.1850)\tPrec@1 73.438 (70.468)\tPrec@5 95.312 (93.706)\n",
            "Epoch: [33][550/782]\tTime 0.023 (0.024)\tLoss (Class) 1.1815 (1.1776)\tLoss (Regu) 0.1827 (0.1849)\tPrec@1 70.312 (70.389)\tPrec@5 95.312 (93.648)\n",
            "Epoch: [33][600/782]\tTime 0.024 (0.024)\tLoss (Class) 1.1234 (1.1832)\tLoss (Regu) 0.1827 (0.1848)\tPrec@1 71.875 (70.325)\tPrec@5 93.750 (93.565)\n",
            "Epoch: [33][650/782]\tTime 0.022 (0.024)\tLoss (Class) 1.2043 (1.1870)\tLoss (Regu) 0.1856 (0.1848)\tPrec@1 70.312 (70.209)\tPrec@5 95.312 (93.520)\n",
            "Epoch: [33][700/782]\tTime 0.023 (0.024)\tLoss (Class) 1.4217 (1.1891)\tLoss (Regu) 0.1806 (0.1848)\tPrec@1 65.625 (70.139)\tPrec@5 85.938 (93.478)\n",
            "Epoch: [33][750/782]\tTime 0.023 (0.024)\tLoss (Class) 1.3060 (1.1928)\tLoss (Regu) 0.1838 (0.1848)\tPrec@1 64.062 (69.990)\tPrec@5 90.625 (93.448)\n",
            "Test: [0/157]\tTime 0.108 (0.108)\tLoss (Class) 1.3844 (1.3844)\tLoss (Regu) 0.2165 (0.2165)\tPrec@1 59.375 (59.375)\tPrec@5 93.750 (93.750)\n",
            "Test: [50/157]\tTime 0.008 (0.010)\tLoss (Class) 2.4790 (1.8215)\tLoss (Regu) 0.2141 (0.2110)\tPrec@1 57.812 (58.578)\tPrec@5 78.125 (86.336)\n",
            "Test: [100/157]\tTime 0.008 (0.009)\tLoss (Class) 1.8837 (1.8530)\tLoss (Regu) 0.2076 (0.2108)\tPrec@1 60.938 (57.550)\tPrec@5 84.375 (85.504)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.5833 (1.8204)\tLoss (Regu) 0.2142 (0.2110)\tPrec@1 65.625 (58.268)\tPrec@5 85.938 (85.896)\n",
            " * Train[70.046 %, 93.456 %, 1.192 loss] Val [58.220 %, 85.900%, 1.824 loss] Best: 59.290 %\n",
            "Time for 33 / 150 20.260823488235474\n",
            "Learning rate:  0.03\n",
            "Epoch: [34][0/782]\tTime 0.146 (0.146)\tLoss (Class) 1.1645 (1.1645)\tLoss (Regu) 0.1838 (0.1838)\tPrec@1 70.312 (70.312)\tPrec@5 93.750 (93.750)\n",
            "Epoch: [34][50/782]\tTime 0.021 (0.027)\tLoss (Class) 1.4712 (1.1235)\tLoss (Regu) 0.1851 (0.1882)\tPrec@1 59.375 (71.844)\tPrec@5 93.750 (93.964)\n",
            "Epoch: [34][100/782]\tTime 0.023 (0.026)\tLoss (Class) 1.3384 (1.1431)\tLoss (Regu) 0.1846 (0.1869)\tPrec@1 60.938 (71.535)\tPrec@5 93.750 (93.843)\n",
            "Epoch: [34][150/782]\tTime 0.022 (0.025)\tLoss (Class) 1.1354 (1.1479)\tLoss (Regu) 0.1876 (0.1868)\tPrec@1 73.438 (71.244)\tPrec@5 93.750 (93.812)\n",
            "Epoch: [34][200/782]\tTime 0.022 (0.025)\tLoss (Class) 0.9534 (1.1425)\tLoss (Regu) 0.1884 (0.1868)\tPrec@1 76.562 (71.354)\tPrec@5 95.312 (93.968)\n",
            "Epoch: [34][250/782]\tTime 0.022 (0.024)\tLoss (Class) 1.4976 (1.1423)\tLoss (Regu) 0.1858 (0.1867)\tPrec@1 54.688 (71.277)\tPrec@5 90.625 (93.993)\n",
            "Epoch: [34][300/782]\tTime 0.022 (0.024)\tLoss (Class) 1.2466 (1.1577)\tLoss (Regu) 0.1841 (0.1865)\tPrec@1 67.188 (70.769)\tPrec@5 95.312 (93.807)\n",
            "Epoch: [34][350/782]\tTime 0.022 (0.024)\tLoss (Class) 1.3131 (1.1612)\tLoss (Regu) 0.1882 (0.1865)\tPrec@1 65.625 (70.602)\tPrec@5 92.188 (93.746)\n",
            "Epoch: [34][400/782]\tTime 0.022 (0.024)\tLoss (Class) 1.2498 (1.1611)\tLoss (Regu) 0.1846 (0.1866)\tPrec@1 64.062 (70.554)\tPrec@5 95.312 (93.789)\n",
            "Epoch: [34][450/782]\tTime 0.024 (0.024)\tLoss (Class) 1.1523 (1.1609)\tLoss (Regu) 0.1824 (0.1865)\tPrec@1 70.312 (70.652)\tPrec@5 92.188 (93.802)\n",
            "Epoch: [34][500/782]\tTime 0.030 (0.024)\tLoss (Class) 1.4702 (1.1641)\tLoss (Regu) 0.1829 (0.1863)\tPrec@1 62.500 (70.574)\tPrec@5 92.188 (93.784)\n",
            "Epoch: [34][550/782]\tTime 0.023 (0.024)\tLoss (Class) 1.3183 (1.1639)\tLoss (Regu) 0.1867 (0.1862)\tPrec@1 67.188 (70.582)\tPrec@5 90.625 (93.798)\n",
            "Epoch: [34][600/782]\tTime 0.024 (0.024)\tLoss (Class) 1.2068 (1.1649)\tLoss (Regu) 0.1873 (0.1861)\tPrec@1 70.312 (70.526)\tPrec@5 90.625 (93.747)\n",
            "Epoch: [34][650/782]\tTime 0.023 (0.024)\tLoss (Class) 1.3767 (1.1692)\tLoss (Regu) 0.1849 (0.1858)\tPrec@1 70.312 (70.440)\tPrec@5 89.062 (93.690)\n",
            "Epoch: [34][700/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8677 (1.1704)\tLoss (Regu) 0.1869 (0.1856)\tPrec@1 84.375 (70.424)\tPrec@5 96.875 (93.659)\n",
            "Epoch: [34][750/782]\tTime 0.022 (0.024)\tLoss (Class) 0.9877 (1.1720)\tLoss (Regu) 0.1852 (0.1857)\tPrec@1 81.250 (70.352)\tPrec@5 87.500 (93.650)\n",
            "Test: [0/157]\tTime 0.117 (0.117)\tLoss (Class) 1.4453 (1.4453)\tLoss (Regu) 0.2124 (0.2124)\tPrec@1 62.500 (62.500)\tPrec@5 89.062 (89.062)\n",
            "Test: [50/157]\tTime 0.009 (0.011)\tLoss (Class) 1.8724 (1.7710)\tLoss (Regu) 0.2173 (0.2129)\tPrec@1 60.938 (59.130)\tPrec@5 82.812 (86.060)\n",
            "Test: [100/157]\tTime 0.008 (0.010)\tLoss (Class) 1.8096 (1.7605)\tLoss (Regu) 0.2163 (0.2135)\tPrec@1 64.062 (59.978)\tPrec@5 84.375 (86.061)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.5389 (1.7530)\tLoss (Regu) 0.2107 (0.2138)\tPrec@1 62.500 (60.182)\tPrec@5 85.938 (86.300)\n",
            " * Train[70.242 %, 93.606 %, 1.176 loss] Val [60.280 %, 86.340%, 1.752 loss] Best: 60.280 %\n",
            "Time for 34 / 150 20.602827072143555\n",
            "Learning rate:  0.03\n",
            "Epoch: [35][0/782]\tTime 0.154 (0.154)\tLoss (Class) 1.1072 (1.1072)\tLoss (Regu) 0.1857 (0.1857)\tPrec@1 70.312 (70.312)\tPrec@5 95.312 (95.312)\n",
            "Epoch: [35][50/782]\tTime 0.024 (0.028)\tLoss (Class) 1.1857 (1.0906)\tLoss (Regu) 0.1895 (0.1900)\tPrec@1 67.188 (72.580)\tPrec@5 93.750 (94.700)\n",
            "Epoch: [35][100/782]\tTime 0.024 (0.026)\tLoss (Class) 1.0325 (1.0873)\tLoss (Regu) 0.1892 (0.1903)\tPrec@1 70.312 (72.927)\tPrec@5 96.875 (94.632)\n",
            "Epoch: [35][150/782]\tTime 0.023 (0.026)\tLoss (Class) 1.2271 (1.0997)\tLoss (Regu) 0.1899 (0.1892)\tPrec@1 70.312 (72.796)\tPrec@5 96.875 (94.464)\n",
            "Epoch: [35][200/782]\tTime 0.023 (0.026)\tLoss (Class) 1.2627 (1.1110)\tLoss (Regu) 0.1874 (0.1888)\tPrec@1 68.750 (72.373)\tPrec@5 92.188 (94.333)\n",
            "Epoch: [35][250/782]\tTime 0.030 (0.025)\tLoss (Class) 1.1565 (1.1201)\tLoss (Regu) 0.1851 (0.1888)\tPrec@1 59.375 (72.024)\tPrec@5 95.312 (94.198)\n",
            "Epoch: [35][300/782]\tTime 0.023 (0.025)\tLoss (Class) 1.2549 (1.1288)\tLoss (Regu) 0.1849 (0.1884)\tPrec@1 70.312 (71.797)\tPrec@5 90.625 (94.015)\n",
            "Epoch: [35][350/782]\tTime 0.022 (0.025)\tLoss (Class) 1.3126 (1.1333)\tLoss (Regu) 0.1843 (0.1880)\tPrec@1 67.188 (71.666)\tPrec@5 93.750 (93.906)\n",
            "Epoch: [35][400/782]\tTime 0.024 (0.025)\tLoss (Class) 1.0246 (1.1347)\tLoss (Regu) 0.1877 (0.1878)\tPrec@1 68.750 (71.641)\tPrec@5 98.438 (93.964)\n",
            "Epoch: [35][450/782]\tTime 0.031 (0.025)\tLoss (Class) 1.4549 (1.1366)\tLoss (Regu) 0.1894 (0.1876)\tPrec@1 65.625 (71.574)\tPrec@5 87.500 (93.947)\n",
            "Epoch: [35][500/782]\tTime 0.022 (0.025)\tLoss (Class) 1.5380 (1.1386)\tLoss (Regu) 0.1830 (0.1872)\tPrec@1 57.812 (71.622)\tPrec@5 87.500 (93.831)\n",
            "Epoch: [35][550/782]\tTime 0.023 (0.024)\tLoss (Class) 1.2997 (1.1437)\tLoss (Regu) 0.1821 (0.1869)\tPrec@1 62.500 (71.461)\tPrec@5 92.188 (93.759)\n",
            "Epoch: [35][600/782]\tTime 0.022 (0.024)\tLoss (Class) 1.1709 (1.1502)\tLoss (Regu) 0.1853 (0.1868)\tPrec@1 71.875 (71.202)\tPrec@5 92.188 (93.706)\n",
            "Epoch: [35][650/782]\tTime 0.023 (0.024)\tLoss (Class) 1.1857 (1.1555)\tLoss (Regu) 0.1848 (0.1866)\tPrec@1 73.438 (71.069)\tPrec@5 92.188 (93.608)\n",
            "Epoch: [35][700/782]\tTime 0.027 (0.024)\tLoss (Class) 1.1832 (1.1584)\tLoss (Regu) 0.1824 (0.1865)\tPrec@1 67.188 (70.961)\tPrec@5 95.312 (93.590)\n",
            "Epoch: [35][750/782]\tTime 0.034 (0.024)\tLoss (Class) 1.4467 (1.1606)\tLoss (Regu) 0.1813 (0.1862)\tPrec@1 60.938 (70.883)\tPrec@5 89.062 (93.584)\n",
            "Test: [0/157]\tTime 0.117 (0.117)\tLoss (Class) 1.6029 (1.6029)\tLoss (Regu) 0.2088 (0.2088)\tPrec@1 60.938 (60.938)\tPrec@5 92.188 (92.188)\n",
            "Test: [50/157]\tTime 0.007 (0.011)\tLoss (Class) 1.5477 (1.6237)\tLoss (Regu) 0.2031 (0.2028)\tPrec@1 65.625 (61.489)\tPrec@5 84.375 (88.113)\n",
            "Test: [100/157]\tTime 0.008 (0.010)\tLoss (Class) 1.6722 (1.6326)\tLoss (Regu) 0.2018 (0.2026)\tPrec@1 57.812 (61.340)\tPrec@5 90.625 (87.902)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.8614 (1.6510)\tLoss (Regu) 0.2030 (0.2022)\tPrec@1 53.125 (61.031)\tPrec@5 84.375 (87.490)\n",
            " * Train[70.808 %, 93.590 %, 1.162 loss] Val [60.920 %, 87.530%, 1.650 loss] Best: 60.920 %\n",
            "Time for 35 / 150 20.27641487121582\n",
            "Learning rate:  0.03\n",
            "Epoch: [36][0/782]\tTime 0.145 (0.145)\tLoss (Class) 0.9989 (0.9989)\tLoss (Regu) 0.1792 (0.1792)\tPrec@1 73.438 (73.438)\tPrec@5 95.312 (95.312)\n",
            "Epoch: [36][50/782]\tTime 0.021 (0.026)\tLoss (Class) 0.7744 (1.1060)\tLoss (Regu) 0.1849 (0.1850)\tPrec@1 78.125 (72.181)\tPrec@5 98.438 (94.148)\n",
            "Epoch: [36][100/782]\tTime 0.024 (0.026)\tLoss (Class) 0.9516 (1.1115)\tLoss (Regu) 0.1883 (0.1848)\tPrec@1 76.562 (71.968)\tPrec@5 96.875 (94.261)\n",
            "Epoch: [36][150/782]\tTime 0.022 (0.025)\tLoss (Class) 1.1173 (1.1195)\tLoss (Regu) 0.1849 (0.1855)\tPrec@1 73.438 (71.844)\tPrec@5 89.062 (94.226)\n",
            "Epoch: [36][200/782]\tTime 0.023 (0.025)\tLoss (Class) 1.0857 (1.1152)\tLoss (Regu) 0.1851 (0.1855)\tPrec@1 76.562 (72.077)\tPrec@5 93.750 (94.333)\n",
            "Epoch: [36][250/782]\tTime 0.022 (0.025)\tLoss (Class) 1.2446 (1.1173)\tLoss (Regu) 0.1876 (0.1852)\tPrec@1 64.062 (71.987)\tPrec@5 93.750 (94.254)\n",
            "Epoch: [36][300/782]\tTime 0.023 (0.025)\tLoss (Class) 1.2624 (1.1289)\tLoss (Regu) 0.1873 (0.1853)\tPrec@1 71.875 (71.730)\tPrec@5 92.188 (94.108)\n",
            "Epoch: [36][350/782]\tTime 0.022 (0.025)\tLoss (Class) 1.2879 (1.1313)\tLoss (Regu) 0.1842 (0.1851)\tPrec@1 75.000 (71.612)\tPrec@5 92.188 (94.017)\n",
            "Epoch: [36][400/782]\tTime 0.022 (0.025)\tLoss (Class) 1.2446 (1.1382)\tLoss (Regu) 0.1847 (0.1851)\tPrec@1 65.625 (71.474)\tPrec@5 93.750 (93.992)\n",
            "Epoch: [36][450/782]\tTime 0.033 (0.025)\tLoss (Class) 0.8908 (1.1363)\tLoss (Regu) 0.1882 (0.1853)\tPrec@1 78.125 (71.567)\tPrec@5 95.312 (94.051)\n",
            "Epoch: [36][500/782]\tTime 0.023 (0.025)\tLoss (Class) 1.0334 (1.1373)\tLoss (Regu) 0.1877 (0.1855)\tPrec@1 71.875 (71.516)\tPrec@5 96.875 (93.981)\n",
            "Epoch: [36][550/782]\tTime 0.032 (0.025)\tLoss (Class) 1.3360 (1.1377)\tLoss (Regu) 0.1852 (0.1856)\tPrec@1 65.625 (71.438)\tPrec@5 95.312 (93.983)\n",
            "Epoch: [36][600/782]\tTime 0.023 (0.025)\tLoss (Class) 1.0078 (1.1413)\tLoss (Regu) 0.1890 (0.1856)\tPrec@1 78.125 (71.339)\tPrec@5 96.875 (93.929)\n",
            "Epoch: [36][650/782]\tTime 0.032 (0.025)\tLoss (Class) 1.3084 (1.1429)\tLoss (Regu) 0.1876 (0.1855)\tPrec@1 76.562 (71.333)\tPrec@5 90.625 (93.935)\n",
            "Epoch: [36][700/782]\tTime 0.022 (0.025)\tLoss (Class) 0.9095 (1.1433)\tLoss (Regu) 0.1850 (0.1855)\tPrec@1 78.125 (71.329)\tPrec@5 96.875 (93.926)\n",
            "Epoch: [36][750/782]\tTime 0.032 (0.025)\tLoss (Class) 1.2748 (1.1452)\tLoss (Regu) 0.1859 (0.1854)\tPrec@1 62.500 (71.238)\tPrec@5 98.438 (93.898)\n",
            "Test: [0/157]\tTime 0.108 (0.108)\tLoss (Class) 1.7588 (1.7588)\tLoss (Regu) 0.2150 (0.2150)\tPrec@1 60.938 (60.938)\tPrec@5 85.938 (85.938)\n",
            "Test: [50/157]\tTime 0.008 (0.011)\tLoss (Class) 1.9830 (1.8058)\tLoss (Regu) 0.2133 (0.2139)\tPrec@1 54.688 (58.915)\tPrec@5 90.625 (86.397)\n",
            "Test: [100/157]\tTime 0.007 (0.009)\tLoss (Class) 1.4193 (1.8013)\tLoss (Regu) 0.2102 (0.2134)\tPrec@1 64.062 (59.189)\tPrec@5 85.938 (86.293)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.5785 (1.7785)\tLoss (Regu) 0.2177 (0.2137)\tPrec@1 60.938 (59.634)\tPrec@5 87.500 (86.300)\n",
            " * Train[71.242 %, 93.906 %, 1.145 loss] Val [59.620 %, 86.350%, 1.778 loss] Best: 60.920 %\n",
            "Time for 36 / 150 20.873692512512207\n",
            "Learning rate:  0.03\n",
            "Epoch: [37][0/782]\tTime 0.151 (0.151)\tLoss (Class) 0.9857 (0.9857)\tLoss (Regu) 0.1870 (0.1870)\tPrec@1 68.750 (68.750)\tPrec@5 95.312 (95.312)\n",
            "Epoch: [37][50/782]\tTime 0.022 (0.026)\tLoss (Class) 0.9642 (1.1164)\tLoss (Regu) 0.1845 (0.1860)\tPrec@1 79.688 (72.610)\tPrec@5 96.875 (93.781)\n",
            "Epoch: [37][100/782]\tTime 0.024 (0.024)\tLoss (Class) 0.8871 (1.1085)\tLoss (Regu) 0.1846 (0.1862)\tPrec@1 81.250 (72.772)\tPrec@5 95.312 (94.353)\n",
            "Epoch: [37][150/782]\tTime 0.023 (0.024)\tLoss (Class) 1.1048 (1.1022)\tLoss (Regu) 0.1886 (0.1860)\tPrec@1 70.312 (72.724)\tPrec@5 95.312 (94.423)\n",
            "Epoch: [37][200/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7902 (1.1018)\tLoss (Regu) 0.1826 (0.1857)\tPrec@1 82.812 (72.598)\tPrec@5 98.438 (94.380)\n",
            "Epoch: [37][250/782]\tTime 0.025 (0.023)\tLoss (Class) 0.9737 (1.1051)\tLoss (Regu) 0.1849 (0.1859)\tPrec@1 78.125 (72.373)\tPrec@5 100.000 (94.354)\n",
            "Epoch: [37][300/782]\tTime 0.022 (0.023)\tLoss (Class) 1.2199 (1.1120)\tLoss (Regu) 0.1817 (0.1858)\tPrec@1 67.188 (72.083)\tPrec@5 90.625 (94.228)\n",
            "Epoch: [37][350/782]\tTime 0.023 (0.023)\tLoss (Class) 1.2371 (1.1112)\tLoss (Regu) 0.1868 (0.1855)\tPrec@1 65.625 (72.173)\tPrec@5 93.750 (94.222)\n",
            "Epoch: [37][400/782]\tTime 0.023 (0.023)\tLoss (Class) 0.9572 (1.1165)\tLoss (Regu) 0.1825 (0.1852)\tPrec@1 75.000 (72.089)\tPrec@5 96.875 (94.147)\n",
            "Epoch: [37][450/782]\tTime 0.023 (0.023)\tLoss (Class) 1.4220 (1.1214)\tLoss (Regu) 0.1885 (0.1852)\tPrec@1 67.188 (71.910)\tPrec@5 85.938 (94.062)\n",
            "Epoch: [37][500/782]\tTime 0.023 (0.023)\tLoss (Class) 1.0686 (1.1247)\tLoss (Regu) 0.1871 (0.1852)\tPrec@1 71.875 (71.844)\tPrec@5 96.875 (94.053)\n",
            "Epoch: [37][550/782]\tTime 0.023 (0.023)\tLoss (Class) 1.5873 (1.1253)\tLoss (Regu) 0.1818 (0.1851)\tPrec@1 57.812 (71.855)\tPrec@5 89.062 (94.090)\n",
            "Epoch: [37][600/782]\tTime 0.022 (0.023)\tLoss (Class) 1.1295 (1.1277)\tLoss (Regu) 0.1832 (0.1849)\tPrec@1 67.188 (71.727)\tPrec@5 96.875 (94.111)\n",
            "Epoch: [37][650/782]\tTime 0.022 (0.023)\tLoss (Class) 1.4225 (1.1324)\tLoss (Regu) 0.1840 (0.1848)\tPrec@1 59.375 (71.676)\tPrec@5 96.875 (94.021)\n",
            "Epoch: [37][700/782]\tTime 0.023 (0.023)\tLoss (Class) 1.2257 (1.1345)\tLoss (Regu) 0.1830 (0.1849)\tPrec@1 57.812 (71.585)\tPrec@5 95.312 (94.020)\n",
            "Epoch: [37][750/782]\tTime 0.022 (0.023)\tLoss (Class) 1.1597 (1.1348)\tLoss (Regu) 0.1812 (0.1848)\tPrec@1 73.438 (71.565)\tPrec@5 95.312 (94.039)\n",
            "Test: [0/157]\tTime 0.119 (0.119)\tLoss (Class) 1.6697 (1.6697)\tLoss (Regu) 0.1971 (0.1971)\tPrec@1 59.375 (59.375)\tPrec@5 89.062 (89.062)\n",
            "Test: [50/157]\tTime 0.009 (0.013)\tLoss (Class) 1.5935 (1.6845)\tLoss (Regu) 0.2087 (0.2057)\tPrec@1 60.938 (60.938)\tPrec@5 87.500 (87.439)\n",
            "Test: [100/157]\tTime 0.009 (0.011)\tLoss (Class) 1.6592 (1.7100)\tLoss (Regu) 0.2049 (0.2058)\tPrec@1 62.500 (60.040)\tPrec@5 85.938 (86.943)\n",
            "Test: [150/157]\tTime 0.007 (0.010)\tLoss (Class) 2.0322 (1.7214)\tLoss (Regu) 0.2063 (0.2063)\tPrec@1 60.938 (60.141)\tPrec@5 82.812 (86.910)\n",
            " * Train[71.502 %, 94.004 %, 1.137 loss] Val [60.060 %, 86.720%, 1.732 loss] Best: 60.920 %\n",
            "Time for 37 / 150 19.604114294052124\n",
            "Learning rate:  0.03\n",
            "Epoch: [38][0/782]\tTime 0.152 (0.152)\tLoss (Class) 1.1979 (1.1979)\tLoss (Regu) 0.1843 (0.1843)\tPrec@1 71.875 (71.875)\tPrec@5 90.625 (90.625)\n",
            "Epoch: [38][50/782]\tTime 0.021 (0.027)\tLoss (Class) 1.2458 (1.0546)\tLoss (Regu) 0.1851 (0.1877)\tPrec@1 62.500 (74.479)\tPrec@5 90.625 (95.741)\n",
            "Epoch: [38][100/782]\tTime 0.025 (0.026)\tLoss (Class) 0.8869 (1.0775)\tLoss (Regu) 0.1865 (0.1866)\tPrec@1 71.875 (73.438)\tPrec@5 96.875 (95.452)\n",
            "Epoch: [38][150/782]\tTime 0.023 (0.026)\tLoss (Class) 1.0615 (1.0803)\tLoss (Regu) 0.1865 (0.1869)\tPrec@1 68.750 (73.189)\tPrec@5 95.312 (95.230)\n",
            "Epoch: [38][200/782]\tTime 0.032 (0.025)\tLoss (Class) 0.9800 (1.0741)\tLoss (Regu) 0.1884 (0.1868)\tPrec@1 76.562 (73.329)\tPrec@5 93.750 (95.079)\n",
            "Epoch: [38][250/782]\tTime 0.022 (0.025)\tLoss (Class) 1.1482 (1.0741)\tLoss (Regu) 0.1880 (0.1867)\tPrec@1 73.438 (73.431)\tPrec@5 95.312 (94.995)\n",
            "Epoch: [38][300/782]\tTime 0.031 (0.025)\tLoss (Class) 1.0907 (1.0852)\tLoss (Regu) 0.1840 (0.1863)\tPrec@1 71.875 (73.178)\tPrec@5 95.312 (94.819)\n",
            "Epoch: [38][350/782]\tTime 0.023 (0.025)\tLoss (Class) 1.5859 (1.0894)\tLoss (Regu) 0.1840 (0.1861)\tPrec@1 59.375 (73.006)\tPrec@5 90.625 (94.720)\n",
            "Epoch: [38][400/782]\tTime 0.021 (0.025)\tLoss (Class) 1.2945 (1.0979)\tLoss (Regu) 0.1827 (0.1859)\tPrec@1 68.750 (72.752)\tPrec@5 90.625 (94.599)\n",
            "Epoch: [38][450/782]\tTime 0.022 (0.025)\tLoss (Class) 1.0467 (1.0984)\tLoss (Regu) 0.1849 (0.1859)\tPrec@1 68.750 (72.720)\tPrec@5 93.750 (94.554)\n",
            "Epoch: [38][500/782]\tTime 0.023 (0.025)\tLoss (Class) 1.0322 (1.1044)\tLoss (Regu) 0.1803 (0.1857)\tPrec@1 76.562 (72.480)\tPrec@5 96.875 (94.489)\n",
            "Epoch: [38][550/782]\tTime 0.023 (0.025)\tLoss (Class) 1.1718 (1.1076)\tLoss (Regu) 0.1876 (0.1858)\tPrec@1 70.312 (72.428)\tPrec@5 93.750 (94.482)\n",
            "Epoch: [38][600/782]\tTime 0.022 (0.025)\tLoss (Class) 1.0031 (1.1095)\tLoss (Regu) 0.1841 (0.1859)\tPrec@1 76.562 (72.377)\tPrec@5 96.875 (94.473)\n",
            "Epoch: [38][650/782]\tTime 0.023 (0.025)\tLoss (Class) 1.0987 (1.1130)\tLoss (Regu) 0.1902 (0.1859)\tPrec@1 73.438 (72.314)\tPrec@5 95.312 (94.436)\n",
            "Epoch: [38][700/782]\tTime 0.025 (0.025)\tLoss (Class) 1.2098 (1.1168)\tLoss (Regu) 0.1860 (0.1859)\tPrec@1 67.188 (72.176)\tPrec@5 90.625 (94.399)\n",
            "Epoch: [38][750/782]\tTime 0.022 (0.025)\tLoss (Class) 1.3902 (1.1205)\tLoss (Regu) 0.1814 (0.1858)\tPrec@1 64.062 (72.000)\tPrec@5 93.750 (94.366)\n",
            "Test: [0/157]\tTime 0.109 (0.109)\tLoss (Class) 1.5537 (1.5537)\tLoss (Regu) 0.2191 (0.2191)\tPrec@1 65.625 (65.625)\tPrec@5 89.062 (89.062)\n",
            "Test: [50/157]\tTime 0.007 (0.010)\tLoss (Class) 1.9110 (1.6968)\tLoss (Regu) 0.2139 (0.2141)\tPrec@1 53.125 (60.907)\tPrec@5 84.375 (87.745)\n",
            "Test: [100/157]\tTime 0.008 (0.009)\tLoss (Class) 2.3246 (1.7261)\tLoss (Regu) 0.2074 (0.2142)\tPrec@1 48.438 (60.845)\tPrec@5 78.125 (87.144)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.6634 (1.7228)\tLoss (Regu) 0.2132 (0.2141)\tPrec@1 67.188 (60.958)\tPrec@5 87.500 (87.076)\n",
            " * Train[72.004 %, 94.340 %, 1.121 loss] Val [60.850 %, 87.050%, 1.728 loss] Best: 60.920 %\n",
            "Time for 38 / 150 20.73236393928528\n",
            "Learning rate:  0.03\n",
            "Epoch: [39][0/782]\tTime 0.149 (0.149)\tLoss (Class) 1.2740 (1.2740)\tLoss (Regu) 0.1838 (0.1838)\tPrec@1 62.500 (62.500)\tPrec@5 96.875 (96.875)\n",
            "Epoch: [39][50/782]\tTime 0.030 (0.027)\tLoss (Class) 0.9401 (1.0441)\tLoss (Regu) 0.1904 (0.1874)\tPrec@1 76.562 (73.284)\tPrec@5 96.875 (95.466)\n",
            "Epoch: [39][100/782]\tTime 0.024 (0.026)\tLoss (Class) 1.0516 (1.0533)\tLoss (Regu) 0.1854 (0.1874)\tPrec@1 75.000 (73.267)\tPrec@5 95.312 (95.204)\n",
            "Epoch: [39][150/782]\tTime 0.030 (0.026)\tLoss (Class) 1.0540 (1.0595)\tLoss (Regu) 0.1886 (0.1867)\tPrec@1 71.875 (73.189)\tPrec@5 95.312 (95.023)\n",
            "Epoch: [39][200/782]\tTime 0.023 (0.025)\tLoss (Class) 1.1275 (1.0657)\tLoss (Regu) 0.1875 (0.1867)\tPrec@1 71.875 (73.290)\tPrec@5 96.875 (94.908)\n",
            "Epoch: [39][250/782]\tTime 0.031 (0.025)\tLoss (Class) 1.0591 (1.0712)\tLoss (Regu) 0.1854 (0.1866)\tPrec@1 75.000 (73.126)\tPrec@5 95.312 (94.864)\n",
            "Epoch: [39][300/782]\tTime 0.023 (0.025)\tLoss (Class) 0.8288 (1.0794)\tLoss (Regu) 0.1870 (0.1868)\tPrec@1 79.688 (72.981)\tPrec@5 98.438 (94.788)\n",
            "Epoch: [39][350/782]\tTime 0.023 (0.025)\tLoss (Class) 1.0176 (1.0804)\tLoss (Regu) 0.1840 (0.1868)\tPrec@1 73.438 (72.992)\tPrec@5 93.750 (94.756)\n",
            "Epoch: [39][400/782]\tTime 0.022 (0.025)\tLoss (Class) 1.1269 (1.0799)\tLoss (Regu) 0.1848 (0.1866)\tPrec@1 68.750 (73.060)\tPrec@5 96.875 (94.814)\n",
            "Epoch: [39][450/782]\tTime 0.023 (0.025)\tLoss (Class) 1.0905 (1.0837)\tLoss (Regu) 0.1836 (0.1865)\tPrec@1 70.312 (72.907)\tPrec@5 95.312 (94.834)\n",
            "Epoch: [39][500/782]\tTime 0.022 (0.025)\tLoss (Class) 0.7975 (1.0883)\tLoss (Regu) 0.1882 (0.1864)\tPrec@1 82.812 (72.767)\tPrec@5 98.438 (94.779)\n",
            "Epoch: [39][550/782]\tTime 0.022 (0.025)\tLoss (Class) 1.2119 (1.0918)\tLoss (Regu) 0.1876 (0.1863)\tPrec@1 65.625 (72.652)\tPrec@5 90.625 (94.666)\n",
            "Epoch: [39][600/782]\tTime 0.023 (0.025)\tLoss (Class) 0.9472 (1.0948)\tLoss (Regu) 0.1820 (0.1862)\tPrec@1 79.688 (72.580)\tPrec@5 98.438 (94.585)\n",
            "Epoch: [39][650/782]\tTime 0.022 (0.025)\tLoss (Class) 1.3852 (1.0969)\tLoss (Regu) 0.1828 (0.1862)\tPrec@1 65.625 (72.456)\tPrec@5 90.625 (94.549)\n",
            "Epoch: [39][700/782]\tTime 0.032 (0.025)\tLoss (Class) 1.1053 (1.1013)\tLoss (Regu) 0.1850 (0.1861)\tPrec@1 70.312 (72.316)\tPrec@5 92.188 (94.470)\n",
            "Epoch: [39][750/782]\tTime 0.023 (0.025)\tLoss (Class) 1.4330 (1.1023)\tLoss (Regu) 0.1870 (0.1859)\tPrec@1 62.500 (72.331)\tPrec@5 90.625 (94.449)\n",
            "Test: [0/157]\tTime 0.114 (0.114)\tLoss (Class) 2.1717 (2.1717)\tLoss (Regu) 0.2042 (0.2042)\tPrec@1 46.875 (46.875)\tPrec@5 82.812 (82.812)\n",
            "Test: [50/157]\tTime 0.007 (0.010)\tLoss (Class) 2.2296 (1.7569)\tLoss (Regu) 0.2085 (0.2082)\tPrec@1 46.875 (59.865)\tPrec@5 85.938 (86.857)\n",
            "Test: [100/157]\tTime 0.010 (0.009)\tLoss (Class) 1.7757 (1.7783)\tLoss (Regu) 0.2117 (0.2085)\tPrec@1 65.625 (59.452)\tPrec@5 87.500 (86.293)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.8899 (1.7771)\tLoss (Regu) 0.2058 (0.2088)\tPrec@1 62.500 (59.768)\tPrec@5 84.375 (86.279)\n",
            " * Train[72.294 %, 94.410 %, 1.104 loss] Val [59.820 %, 86.130%, 1.782 loss] Best: 60.920 %\n",
            "Time for 39 / 150 20.787176609039307\n",
            "Learning rate:  0.03\n",
            "Epoch: [40][0/782]\tTime 0.151 (0.151)\tLoss (Class) 1.0359 (1.0359)\tLoss (Regu) 0.1884 (0.1884)\tPrec@1 71.875 (71.875)\tPrec@5 98.438 (98.438)\n",
            "Epoch: [40][50/782]\tTime 0.022 (0.025)\tLoss (Class) 0.9643 (1.1233)\tLoss (Regu) 0.1870 (0.1868)\tPrec@1 70.312 (71.232)\tPrec@5 96.875 (94.240)\n",
            "Epoch: [40][100/782]\tTime 0.023 (0.024)\tLoss (Class) 1.1384 (1.1022)\tLoss (Regu) 0.1867 (0.1871)\tPrec@1 71.875 (72.169)\tPrec@5 95.312 (94.585)\n",
            "Epoch: [40][150/782]\tTime 0.023 (0.024)\tLoss (Class) 0.9294 (1.0854)\tLoss (Regu) 0.1909 (0.1871)\tPrec@1 76.562 (72.610)\tPrec@5 98.438 (94.826)\n",
            "Epoch: [40][200/782]\tTime 0.022 (0.024)\tLoss (Class) 0.9664 (1.0808)\tLoss (Regu) 0.1930 (0.1873)\tPrec@1 76.562 (72.738)\tPrec@5 96.875 (94.970)\n",
            "Epoch: [40][250/782]\tTime 0.021 (0.024)\tLoss (Class) 1.1223 (1.0818)\tLoss (Regu) 0.1848 (0.1871)\tPrec@1 68.750 (72.634)\tPrec@5 95.312 (94.871)\n",
            "Epoch: [40][300/782]\tTime 0.023 (0.024)\tLoss (Class) 1.0970 (1.0884)\tLoss (Regu) 0.1879 (0.1870)\tPrec@1 75.000 (72.519)\tPrec@5 95.312 (94.778)\n",
            "Epoch: [40][350/782]\tTime 0.023 (0.024)\tLoss (Class) 1.0248 (1.0951)\tLoss (Regu) 0.1843 (0.1868)\tPrec@1 75.000 (72.200)\tPrec@5 98.438 (94.712)\n",
            "Epoch: [40][400/782]\tTime 0.023 (0.024)\tLoss (Class) 1.0663 (1.0943)\tLoss (Regu) 0.1872 (0.1868)\tPrec@1 78.125 (72.230)\tPrec@5 90.625 (94.709)\n",
            "Epoch: [40][450/782]\tTime 0.025 (0.024)\tLoss (Class) 0.8462 (1.0941)\tLoss (Regu) 0.1896 (0.1870)\tPrec@1 81.250 (72.246)\tPrec@5 95.312 (94.658)\n",
            "Epoch: [40][500/782]\tTime 0.022 (0.024)\tLoss (Class) 0.7670 (1.0961)\tLoss (Regu) 0.1829 (0.1867)\tPrec@1 79.688 (72.202)\tPrec@5 100.000 (94.629)\n",
            "Epoch: [40][550/782]\tTime 0.022 (0.024)\tLoss (Class) 1.1723 (1.0963)\tLoss (Regu) 0.1856 (0.1868)\tPrec@1 67.188 (72.207)\tPrec@5 95.312 (94.652)\n",
            "Epoch: [40][600/782]\tTime 0.032 (0.024)\tLoss (Class) 1.1812 (1.1010)\tLoss (Regu) 0.1857 (0.1867)\tPrec@1 70.312 (72.091)\tPrec@5 95.312 (94.590)\n",
            "Epoch: [40][650/782]\tTime 0.023 (0.024)\tLoss (Class) 0.9451 (1.1047)\tLoss (Regu) 0.1806 (0.1864)\tPrec@1 76.562 (71.985)\tPrec@5 95.312 (94.561)\n",
            "Epoch: [40][700/782]\tTime 0.022 (0.024)\tLoss (Class) 1.2106 (1.1038)\tLoss (Regu) 0.1856 (0.1863)\tPrec@1 68.750 (71.995)\tPrec@5 92.188 (94.566)\n",
            "Epoch: [40][750/782]\tTime 0.025 (0.024)\tLoss (Class) 1.1207 (1.1073)\tLoss (Regu) 0.1867 (0.1863)\tPrec@1 71.875 (71.950)\tPrec@5 95.312 (94.497)\n",
            "Test: [0/157]\tTime 0.115 (0.115)\tLoss (Class) 1.2712 (1.2712)\tLoss (Regu) 0.2029 (0.2029)\tPrec@1 70.312 (70.312)\tPrec@5 89.062 (89.062)\n",
            "Test: [50/157]\tTime 0.009 (0.011)\tLoss (Class) 1.5940 (1.6707)\tLoss (Regu) 0.2120 (0.2105)\tPrec@1 54.688 (61.550)\tPrec@5 89.062 (87.868)\n",
            "Test: [100/157]\tTime 0.013 (0.010)\tLoss (Class) 1.6601 (1.6818)\tLoss (Regu) 0.2104 (0.2104)\tPrec@1 59.375 (61.556)\tPrec@5 89.062 (87.608)\n",
            "Test: [150/157]\tTime 0.008 (0.009)\tLoss (Class) 1.6129 (1.6625)\tLoss (Regu) 0.2101 (0.2107)\tPrec@1 60.938 (62.169)\tPrec@5 89.062 (87.790)\n",
            " * Train[72.042 %, 94.480 %, 1.106 loss] Val [62.070 %, 87.680%, 1.666 loss] Best: 62.070 %\n",
            "Time for 40 / 150 20.115914821624756\n",
            "Learning rate:  0.03\n",
            "Epoch: [41][0/782]\tTime 0.165 (0.165)\tLoss (Class) 1.1660 (1.1660)\tLoss (Regu) 0.1868 (0.1868)\tPrec@1 71.875 (71.875)\tPrec@5 92.188 (92.188)\n",
            "Epoch: [41][50/782]\tTime 0.023 (0.027)\tLoss (Class) 0.9774 (1.0238)\tLoss (Regu) 0.1868 (0.1880)\tPrec@1 76.562 (74.571)\tPrec@5 93.750 (95.466)\n",
            "Epoch: [41][100/782]\tTime 0.032 (0.026)\tLoss (Class) 1.0802 (1.0441)\tLoss (Regu) 0.1860 (0.1872)\tPrec@1 75.000 (73.747)\tPrec@5 93.750 (95.235)\n",
            "Epoch: [41][150/782]\tTime 0.023 (0.026)\tLoss (Class) 0.8785 (1.0563)\tLoss (Regu) 0.1848 (0.1870)\tPrec@1 71.875 (73.438)\tPrec@5 100.000 (95.054)\n",
            "Epoch: [41][200/782]\tTime 0.023 (0.025)\tLoss (Class) 1.0134 (1.0580)\tLoss (Regu) 0.1858 (0.1864)\tPrec@1 76.562 (73.368)\tPrec@5 95.312 (95.048)\n",
            "Epoch: [41][250/782]\tTime 0.023 (0.025)\tLoss (Class) 0.9376 (1.0569)\tLoss (Regu) 0.1879 (0.1866)\tPrec@1 73.438 (73.469)\tPrec@5 100.000 (95.039)\n",
            "Epoch: [41][300/782]\tTime 0.023 (0.025)\tLoss (Class) 1.1990 (1.0593)\tLoss (Regu) 0.1838 (0.1868)\tPrec@1 70.312 (73.432)\tPrec@5 92.188 (95.079)\n",
            "Epoch: [41][350/782]\tTime 0.023 (0.024)\tLoss (Class) 1.1496 (1.0595)\tLoss (Regu) 0.1839 (0.1869)\tPrec@1 79.688 (73.424)\tPrec@5 92.188 (95.103)\n",
            "Epoch: [41][400/782]\tTime 0.023 (0.024)\tLoss (Class) 1.3101 (1.0630)\tLoss (Regu) 0.1822 (0.1867)\tPrec@1 67.188 (73.293)\tPrec@5 92.188 (95.055)\n",
            "Epoch: [41][450/782]\tTime 0.024 (0.024)\tLoss (Class) 1.2227 (1.0657)\tLoss (Regu) 0.1874 (0.1867)\tPrec@1 62.500 (73.230)\tPrec@5 92.188 (95.032)\n",
            "Epoch: [41][500/782]\tTime 0.023 (0.024)\tLoss (Class) 1.1318 (1.0697)\tLoss (Regu) 0.1824 (0.1867)\tPrec@1 71.875 (73.119)\tPrec@5 93.750 (94.969)\n",
            "Epoch: [41][550/782]\tTime 0.022 (0.024)\tLoss (Class) 1.1531 (1.0744)\tLoss (Regu) 0.1853 (0.1865)\tPrec@1 71.875 (73.032)\tPrec@5 93.750 (94.876)\n",
            "Epoch: [41][600/782]\tTime 0.023 (0.024)\tLoss (Class) 1.2242 (1.0761)\tLoss (Regu) 0.1897 (0.1865)\tPrec@1 70.312 (73.016)\tPrec@5 93.750 (94.834)\n",
            "Epoch: [41][650/782]\tTime 0.023 (0.024)\tLoss (Class) 1.0239 (1.0796)\tLoss (Regu) 0.1834 (0.1865)\tPrec@1 75.000 (73.005)\tPrec@5 96.875 (94.782)\n",
            "Epoch: [41][700/782]\tTime 0.023 (0.024)\tLoss (Class) 1.2096 (1.0844)\tLoss (Regu) 0.1831 (0.1863)\tPrec@1 64.062 (72.909)\tPrec@5 90.625 (94.697)\n",
            "Epoch: [41][750/782]\tTime 0.022 (0.024)\tLoss (Class) 0.9184 (1.0864)\tLoss (Regu) 0.1830 (0.1862)\tPrec@1 78.125 (72.857)\tPrec@5 96.875 (94.684)\n",
            "Test: [0/157]\tTime 0.113 (0.113)\tLoss (Class) 1.3835 (1.3835)\tLoss (Regu) 0.2096 (0.2096)\tPrec@1 64.062 (64.062)\tPrec@5 89.062 (89.062)\n",
            "Test: [50/157]\tTime 0.008 (0.010)\tLoss (Class) 1.2535 (1.6796)\tLoss (Regu) 0.2120 (0.2075)\tPrec@1 67.188 (61.765)\tPrec@5 89.062 (86.918)\n",
            "Test: [100/157]\tTime 0.008 (0.009)\tLoss (Class) 1.6117 (1.6880)\tLoss (Regu) 0.2071 (0.2070)\tPrec@1 65.625 (61.510)\tPrec@5 87.500 (87.144)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.6781 (1.7068)\tLoss (Regu) 0.2095 (0.2069)\tPrec@1 56.250 (61.413)\tPrec@5 90.625 (86.786)\n",
            " * Train[72.842 %, 94.674 %, 1.088 loss] Val [61.370 %, 86.800%, 1.708 loss] Best: 62.070 %\n",
            "Time for 41 / 150 20.210015535354614\n",
            "Learning rate:  0.03\n",
            "Epoch: [42][0/782]\tTime 0.153 (0.153)\tLoss (Class) 0.8115 (0.8115)\tLoss (Regu) 0.1803 (0.1803)\tPrec@1 82.812 (82.812)\tPrec@5 98.438 (98.438)\n",
            "Epoch: [42][50/782]\tTime 0.022 (0.026)\tLoss (Class) 1.0491 (1.0502)\tLoss (Regu) 0.1863 (0.1835)\tPrec@1 75.000 (74.173)\tPrec@5 92.188 (95.037)\n",
            "Epoch: [42][100/782]\tTime 0.023 (0.026)\tLoss (Class) 1.1817 (1.0378)\tLoss (Regu) 0.1840 (0.1854)\tPrec@1 67.188 (74.149)\tPrec@5 95.312 (95.111)\n",
            "Epoch: [42][150/782]\tTime 0.023 (0.025)\tLoss (Class) 0.7978 (1.0248)\tLoss (Regu) 0.1930 (0.1862)\tPrec@1 75.000 (74.483)\tPrec@5 98.438 (95.364)\n",
            "Epoch: [42][200/782]\tTime 0.032 (0.025)\tLoss (Class) 0.8461 (1.0349)\tLoss (Regu) 0.1863 (0.1865)\tPrec@1 79.688 (73.966)\tPrec@5 96.875 (95.382)\n",
            "Epoch: [42][250/782]\tTime 0.023 (0.025)\tLoss (Class) 1.0830 (1.0475)\tLoss (Regu) 0.1849 (0.1865)\tPrec@1 73.438 (73.767)\tPrec@5 93.750 (95.157)\n",
            "Epoch: [42][300/782]\tTime 0.022 (0.024)\tLoss (Class) 1.1220 (1.0505)\tLoss (Regu) 0.1869 (0.1866)\tPrec@1 65.625 (73.593)\tPrec@5 95.312 (95.089)\n",
            "Epoch: [42][350/782]\tTime 0.032 (0.024)\tLoss (Class) 1.0149 (1.0511)\tLoss (Regu) 0.1873 (0.1868)\tPrec@1 71.875 (73.660)\tPrec@5 100.000 (95.130)\n",
            "Epoch: [42][400/782]\tTime 0.022 (0.024)\tLoss (Class) 1.1371 (1.0543)\tLoss (Regu) 0.1887 (0.1869)\tPrec@1 73.438 (73.574)\tPrec@5 93.750 (95.059)\n",
            "Epoch: [42][450/782]\tTime 0.031 (0.024)\tLoss (Class) 1.2757 (1.0581)\tLoss (Regu) 0.1828 (0.1869)\tPrec@1 64.062 (73.458)\tPrec@5 93.750 (95.073)\n",
            "Epoch: [42][500/782]\tTime 0.023 (0.025)\tLoss (Class) 1.2548 (1.0610)\tLoss (Regu) 0.1832 (0.1868)\tPrec@1 68.750 (73.406)\tPrec@5 92.188 (95.072)\n",
            "Epoch: [42][550/782]\tTime 0.023 (0.024)\tLoss (Class) 0.9418 (1.0625)\tLoss (Regu) 0.1844 (0.1866)\tPrec@1 81.250 (73.392)\tPrec@5 96.875 (95.026)\n",
            "Epoch: [42][600/782]\tTime 0.024 (0.024)\tLoss (Class) 1.1703 (1.0630)\tLoss (Regu) 0.1852 (0.1867)\tPrec@1 73.438 (73.331)\tPrec@5 93.750 (95.032)\n",
            "Epoch: [42][650/782]\tTime 0.022 (0.024)\tLoss (Class) 1.3233 (1.0692)\tLoss (Regu) 0.1809 (0.1864)\tPrec@1 60.938 (73.161)\tPrec@5 87.500 (94.914)\n",
            "Epoch: [42][700/782]\tTime 0.023 (0.024)\tLoss (Class) 1.1634 (1.0714)\tLoss (Regu) 0.1838 (0.1863)\tPrec@1 70.312 (73.074)\tPrec@5 92.188 (94.867)\n",
            "Epoch: [42][750/782]\tTime 0.032 (0.024)\tLoss (Class) 1.0229 (1.0718)\tLoss (Regu) 0.1890 (0.1862)\tPrec@1 75.000 (72.999)\tPrec@5 96.875 (94.898)\n",
            "Test: [0/157]\tTime 0.120 (0.120)\tLoss (Class) 1.8841 (1.8841)\tLoss (Regu) 0.2079 (0.2079)\tPrec@1 64.062 (64.062)\tPrec@5 89.062 (89.062)\n",
            "Test: [50/157]\tTime 0.008 (0.011)\tLoss (Class) 2.2278 (1.6938)\tLoss (Regu) 0.2060 (0.2050)\tPrec@1 51.562 (61.244)\tPrec@5 82.812 (87.194)\n",
            "Test: [100/157]\tTime 0.008 (0.010)\tLoss (Class) 1.9081 (1.6956)\tLoss (Regu) 0.2120 (0.2052)\tPrec@1 56.250 (60.845)\tPrec@5 81.250 (87.222)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.3914 (1.6931)\tLoss (Regu) 0.2028 (0.2052)\tPrec@1 73.438 (61.103)\tPrec@5 89.062 (87.283)\n",
            " * Train[72.922 %, 94.842 %, 1.075 loss] Val [61.020 %, 87.250%, 1.697 loss] Best: 62.070 %\n",
            "Time for 42 / 150 20.402855396270752\n",
            "Learning rate:  0.03\n",
            "Epoch: [43][0/782]\tTime 0.149 (0.149)\tLoss (Class) 0.8854 (0.8854)\tLoss (Regu) 0.1830 (0.1830)\tPrec@1 76.562 (76.562)\tPrec@5 98.438 (98.438)\n",
            "Epoch: [43][50/782]\tTime 0.023 (0.025)\tLoss (Class) 1.0535 (1.0276)\tLoss (Regu) 0.1878 (0.1835)\tPrec@1 75.000 (74.939)\tPrec@5 92.188 (95.129)\n",
            "Epoch: [43][100/782]\tTime 0.030 (0.024)\tLoss (Class) 0.8478 (1.0409)\tLoss (Regu) 0.1851 (0.1845)\tPrec@1 78.125 (74.428)\tPrec@5 98.438 (94.802)\n",
            "Epoch: [43][150/782]\tTime 0.030 (0.024)\tLoss (Class) 1.6023 (1.0460)\tLoss (Regu) 0.1862 (0.1854)\tPrec@1 57.812 (74.007)\tPrec@5 89.062 (94.837)\n",
            "Epoch: [43][200/782]\tTime 0.023 (0.025)\tLoss (Class) 0.9702 (1.0473)\tLoss (Regu) 0.1880 (0.1857)\tPrec@1 76.562 (73.912)\tPrec@5 98.438 (94.924)\n",
            "Epoch: [43][250/782]\tTime 0.023 (0.024)\tLoss (Class) 1.0211 (1.0520)\tLoss (Regu) 0.1863 (0.1858)\tPrec@1 71.875 (73.687)\tPrec@5 93.750 (94.989)\n",
            "Epoch: [43][300/782]\tTime 0.022 (0.024)\tLoss (Class) 0.7253 (1.0495)\tLoss (Regu) 0.1853 (0.1863)\tPrec@1 82.812 (73.759)\tPrec@5 98.438 (95.105)\n",
            "Epoch: [43][350/782]\tTime 0.022 (0.024)\tLoss (Class) 1.4450 (1.0525)\tLoss (Regu) 0.1886 (0.1862)\tPrec@1 64.062 (73.713)\tPrec@5 93.750 (95.068)\n",
            "Epoch: [43][400/782]\tTime 0.022 (0.024)\tLoss (Class) 1.1820 (1.0573)\tLoss (Regu) 0.1828 (0.1862)\tPrec@1 73.438 (73.605)\tPrec@5 93.750 (95.063)\n",
            "Epoch: [43][450/782]\tTime 0.023 (0.024)\tLoss (Class) 0.9028 (1.0552)\tLoss (Regu) 0.1898 (0.1862)\tPrec@1 79.688 (73.742)\tPrec@5 95.312 (95.063)\n",
            "Epoch: [43][500/782]\tTime 0.022 (0.024)\tLoss (Class) 0.9898 (1.0578)\tLoss (Regu) 0.1844 (0.1861)\tPrec@1 73.438 (73.631)\tPrec@5 96.875 (95.051)\n",
            "Epoch: [43][550/782]\tTime 0.022 (0.024)\tLoss (Class) 0.8852 (1.0593)\tLoss (Regu) 0.1838 (0.1859)\tPrec@1 76.562 (73.525)\tPrec@5 98.438 (95.032)\n",
            "Epoch: [43][600/782]\tTime 0.023 (0.024)\tLoss (Class) 1.1174 (1.0619)\tLoss (Regu) 0.1865 (0.1857)\tPrec@1 73.438 (73.502)\tPrec@5 95.312 (94.980)\n",
            "Epoch: [43][650/782]\tTime 0.022 (0.024)\tLoss (Class) 0.9795 (1.0613)\tLoss (Regu) 0.1863 (0.1857)\tPrec@1 73.438 (73.517)\tPrec@5 95.312 (95.008)\n",
            "Epoch: [43][700/782]\tTime 0.023 (0.023)\tLoss (Class) 1.1889 (1.0630)\tLoss (Regu) 0.1850 (0.1858)\tPrec@1 70.312 (73.469)\tPrec@5 92.188 (95.023)\n",
            "Epoch: [43][750/782]\tTime 0.023 (0.023)\tLoss (Class) 0.8772 (1.0654)\tLoss (Regu) 0.1833 (0.1857)\tPrec@1 79.688 (73.383)\tPrec@5 96.875 (94.992)\n",
            "Test: [0/157]\tTime 0.114 (0.114)\tLoss (Class) 1.9116 (1.9116)\tLoss (Regu) 0.2166 (0.2166)\tPrec@1 59.375 (59.375)\tPrec@5 89.062 (89.062)\n",
            "Test: [50/157]\tTime 0.015 (0.010)\tLoss (Class) 1.8148 (1.7928)\tLoss (Regu) 0.2190 (0.2129)\tPrec@1 53.125 (59.712)\tPrec@5 85.938 (86.857)\n",
            "Test: [100/157]\tTime 0.008 (0.010)\tLoss (Class) 1.3588 (1.7458)\tLoss (Regu) 0.2153 (0.2133)\tPrec@1 65.625 (60.675)\tPrec@5 90.625 (87.252)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.5761 (1.7451)\tLoss (Regu) 0.2114 (0.2133)\tPrec@1 70.312 (60.658)\tPrec@5 85.938 (86.983)\n",
            " * Train[73.266 %, 94.994 %, 1.067 loss] Val [60.780 %, 87.120%, 1.736 loss] Best: 62.070 %\n",
            "Time for 43 / 150 19.95011019706726\n",
            "Learning rate:  0.03\n",
            "Epoch: [44][0/782]\tTime 0.155 (0.155)\tLoss (Class) 1.2106 (1.2106)\tLoss (Regu) 0.1837 (0.1837)\tPrec@1 65.625 (65.625)\tPrec@5 93.750 (93.750)\n",
            "Epoch: [44][50/782]\tTime 0.030 (0.028)\tLoss (Class) 0.8258 (0.9859)\tLoss (Regu) 0.1909 (0.1884)\tPrec@1 81.250 (75.858)\tPrec@5 98.438 (95.619)\n",
            "Epoch: [44][100/782]\tTime 0.031 (0.027)\tLoss (Class) 1.0640 (1.0031)\tLoss (Regu) 0.1854 (0.1895)\tPrec@1 75.000 (75.495)\tPrec@5 93.750 (95.637)\n",
            "Epoch: [44][150/782]\tTime 0.022 (0.027)\tLoss (Class) 0.8389 (1.0090)\tLoss (Regu) 0.1883 (0.1891)\tPrec@1 81.250 (75.207)\tPrec@5 93.750 (95.582)\n",
            "Epoch: [44][200/782]\tTime 0.032 (0.026)\tLoss (Class) 1.0270 (1.0190)\tLoss (Regu) 0.1878 (0.1889)\tPrec@1 71.875 (74.837)\tPrec@5 98.438 (95.484)\n",
            "Epoch: [44][250/782]\tTime 0.023 (0.026)\tLoss (Class) 1.0317 (1.0270)\tLoss (Regu) 0.1882 (0.1883)\tPrec@1 75.000 (74.527)\tPrec@5 95.312 (95.337)\n",
            "Epoch: [44][300/782]\tTime 0.023 (0.026)\tLoss (Class) 0.9318 (1.0340)\tLoss (Regu) 0.1857 (0.1880)\tPrec@1 78.125 (74.367)\tPrec@5 93.750 (95.224)\n",
            "Epoch: [44][350/782]\tTime 0.023 (0.026)\tLoss (Class) 1.0301 (1.0359)\tLoss (Regu) 0.1890 (0.1879)\tPrec@1 71.875 (74.297)\tPrec@5 95.312 (95.183)\n",
            "Epoch: [44][400/782]\tTime 0.023 (0.025)\tLoss (Class) 1.0126 (1.0387)\tLoss (Regu) 0.1900 (0.1881)\tPrec@1 71.875 (74.193)\tPrec@5 98.438 (95.203)\n",
            "Epoch: [44][450/782]\tTime 0.023 (0.025)\tLoss (Class) 1.1399 (1.0411)\tLoss (Regu) 0.1901 (0.1880)\tPrec@1 75.000 (74.078)\tPrec@5 96.875 (95.170)\n",
            "Epoch: [44][500/782]\tTime 0.024 (0.025)\tLoss (Class) 0.9220 (1.0417)\tLoss (Regu) 0.1834 (0.1879)\tPrec@1 79.688 (74.074)\tPrec@5 93.750 (95.153)\n",
            "Epoch: [44][550/782]\tTime 0.032 (0.025)\tLoss (Class) 1.0882 (1.0432)\tLoss (Regu) 0.1898 (0.1879)\tPrec@1 73.438 (74.016)\tPrec@5 100.000 (95.165)\n",
            "Epoch: [44][600/782]\tTime 0.023 (0.025)\tLoss (Class) 1.3171 (1.0454)\tLoss (Regu) 0.1915 (0.1878)\tPrec@1 64.062 (73.942)\tPrec@5 92.188 (95.151)\n",
            "Epoch: [44][650/782]\tTime 0.023 (0.025)\tLoss (Class) 1.1396 (1.0522)\tLoss (Regu) 0.1892 (0.1877)\tPrec@1 65.625 (73.726)\tPrec@5 96.875 (95.096)\n",
            "Epoch: [44][700/782]\tTime 0.021 (0.025)\tLoss (Class) 1.2109 (1.0550)\tLoss (Regu) 0.1842 (0.1876)\tPrec@1 70.312 (73.614)\tPrec@5 95.312 (95.061)\n",
            "Epoch: [44][750/782]\tTime 0.022 (0.025)\tLoss (Class) 0.9719 (1.0591)\tLoss (Regu) 0.1870 (0.1873)\tPrec@1 65.625 (73.552)\tPrec@5 98.438 (94.990)\n",
            "Test: [0/157]\tTime 0.113 (0.113)\tLoss (Class) 1.7363 (1.7363)\tLoss (Regu) 0.2098 (0.2098)\tPrec@1 54.688 (54.688)\tPrec@5 89.062 (89.062)\n",
            "Test: [50/157]\tTime 0.008 (0.011)\tLoss (Class) 2.3394 (1.6844)\tLoss (Regu) 0.2052 (0.2059)\tPrec@1 56.250 (61.244)\tPrec@5 75.000 (87.010)\n",
            "Test: [100/157]\tTime 0.008 (0.009)\tLoss (Class) 1.9841 (1.6498)\tLoss (Regu) 0.1930 (0.2057)\tPrec@1 54.688 (61.974)\tPrec@5 79.688 (87.376)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 2.0256 (1.6647)\tLoss (Regu) 0.2079 (0.2056)\tPrec@1 59.375 (61.889)\tPrec@5 84.375 (87.365)\n",
            " * Train[73.482 %, 95.010 %, 1.060 loss] Val [61.870 %, 87.360%, 1.668 loss] Best: 62.070 %\n",
            "Time for 44 / 150 20.961376667022705\n",
            "Learning rate:  0.03\n",
            "Epoch: [45][0/782]\tTime 0.163 (0.163)\tLoss (Class) 0.8962 (0.8962)\tLoss (Regu) 0.1872 (0.1872)\tPrec@1 75.000 (75.000)\tPrec@5 96.875 (96.875)\n",
            "Epoch: [45][50/782]\tTime 0.023 (0.027)\tLoss (Class) 0.8456 (0.9809)\tLoss (Regu) 0.1854 (0.1860)\tPrec@1 78.125 (75.950)\tPrec@5 100.000 (96.048)\n",
            "Epoch: [45][100/782]\tTime 0.023 (0.025)\tLoss (Class) 1.0745 (0.9857)\tLoss (Regu) 0.1893 (0.1870)\tPrec@1 70.312 (75.449)\tPrec@5 96.875 (95.730)\n",
            "Epoch: [45][150/782]\tTime 0.026 (0.024)\tLoss (Class) 1.1277 (0.9881)\tLoss (Regu) 0.1879 (0.1872)\tPrec@1 71.875 (75.435)\tPrec@5 89.062 (95.788)\n",
            "Epoch: [45][200/782]\tTime 0.023 (0.024)\tLoss (Class) 1.1687 (0.9890)\tLoss (Regu) 0.1877 (0.1873)\tPrec@1 68.750 (75.288)\tPrec@5 95.312 (95.771)\n",
            "Epoch: [45][250/782]\tTime 0.024 (0.024)\tLoss (Class) 1.0442 (0.9963)\tLoss (Regu) 0.1861 (0.1872)\tPrec@1 76.562 (75.137)\tPrec@5 95.312 (95.667)\n",
            "Epoch: [45][300/782]\tTime 0.022 (0.025)\tLoss (Class) 0.8776 (1.0069)\tLoss (Regu) 0.1867 (0.1872)\tPrec@1 76.562 (74.834)\tPrec@5 96.875 (95.629)\n",
            "Epoch: [45][350/782]\tTime 0.023 (0.025)\tLoss (Class) 0.9768 (1.0141)\tLoss (Regu) 0.1868 (0.1872)\tPrec@1 75.000 (74.559)\tPrec@5 98.438 (95.557)\n",
            "Epoch: [45][400/782]\tTime 0.022 (0.025)\tLoss (Class) 1.0241 (1.0203)\tLoss (Regu) 0.1901 (0.1872)\tPrec@1 70.312 (74.392)\tPrec@5 100.000 (95.480)\n",
            "Epoch: [45][450/782]\tTime 0.022 (0.025)\tLoss (Class) 1.1909 (1.0225)\tLoss (Regu) 0.1904 (0.1872)\tPrec@1 71.875 (74.356)\tPrec@5 90.625 (95.468)\n",
            "Epoch: [45][500/782]\tTime 0.030 (0.025)\tLoss (Class) 1.1189 (1.0264)\tLoss (Regu) 0.1879 (0.1871)\tPrec@1 67.188 (74.339)\tPrec@5 96.875 (95.406)\n",
            "Epoch: [45][550/782]\tTime 0.023 (0.025)\tLoss (Class) 1.0047 (1.0332)\tLoss (Regu) 0.1888 (0.1870)\tPrec@1 78.125 (74.158)\tPrec@5 93.750 (95.369)\n",
            "Epoch: [45][600/782]\tTime 0.022 (0.024)\tLoss (Class) 0.9738 (1.0355)\tLoss (Regu) 0.1879 (0.1871)\tPrec@1 70.312 (74.080)\tPrec@5 96.875 (95.331)\n",
            "Epoch: [45][650/782]\tTime 0.022 (0.024)\tLoss (Class) 0.8404 (1.0356)\tLoss (Regu) 0.1876 (0.1869)\tPrec@1 81.250 (74.069)\tPrec@5 95.312 (95.325)\n",
            "Epoch: [45][700/782]\tTime 0.021 (0.024)\tLoss (Class) 1.0996 (1.0394)\tLoss (Regu) 0.1846 (0.1868)\tPrec@1 70.312 (74.017)\tPrec@5 93.750 (95.239)\n",
            "Epoch: [45][750/782]\tTime 0.023 (0.024)\tLoss (Class) 0.9626 (1.0412)\tLoss (Regu) 0.1828 (0.1865)\tPrec@1 73.438 (73.964)\tPrec@5 98.438 (95.184)\n",
            "Test: [0/157]\tTime 0.119 (0.119)\tLoss (Class) 1.2026 (1.2026)\tLoss (Regu) 0.2070 (0.2070)\tPrec@1 70.312 (70.312)\tPrec@5 92.188 (92.188)\n",
            "Test: [50/157]\tTime 0.008 (0.010)\tLoss (Class) 1.5200 (1.6424)\tLoss (Regu) 0.2009 (0.2101)\tPrec@1 68.750 (62.286)\tPrec@5 90.625 (88.695)\n",
            "Test: [100/157]\tTime 0.007 (0.009)\tLoss (Class) 1.7078 (1.6637)\tLoss (Regu) 0.2132 (0.2102)\tPrec@1 57.812 (62.562)\tPrec@5 85.938 (87.933)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.5707 (1.6802)\tLoss (Regu) 0.2109 (0.2102)\tPrec@1 68.750 (61.931)\tPrec@5 85.938 (87.614)\n",
            " * Train[73.940 %, 95.124 %, 1.044 loss] Val [62.080 %, 87.670%, 1.674 loss] Best: 62.080 %\n",
            "Time for 45 / 150 20.475242137908936\n",
            "Learning rate:  0.03\n",
            "Epoch: [46][0/782]\tTime 0.144 (0.144)\tLoss (Class) 1.0652 (1.0652)\tLoss (Regu) 0.1831 (0.1831)\tPrec@1 78.125 (78.125)\tPrec@5 96.875 (96.875)\n",
            "Epoch: [46][50/782]\tTime 0.033 (0.027)\tLoss (Class) 0.8069 (1.0190)\tLoss (Regu) 0.1866 (0.1862)\tPrec@1 82.812 (75.184)\tPrec@5 98.438 (94.914)\n",
            "Epoch: [46][100/782]\tTime 0.024 (0.026)\tLoss (Class) 1.0919 (1.0029)\tLoss (Regu) 0.1857 (0.1866)\tPrec@1 71.875 (75.418)\tPrec@5 96.875 (95.220)\n",
            "Epoch: [46][150/782]\tTime 0.023 (0.025)\tLoss (Class) 1.0381 (0.9991)\tLoss (Regu) 0.1889 (0.1874)\tPrec@1 71.875 (75.414)\tPrec@5 92.188 (95.188)\n",
            "Epoch: [46][200/782]\tTime 0.023 (0.024)\tLoss (Class) 0.9520 (0.9983)\tLoss (Regu) 0.1912 (0.1877)\tPrec@1 73.438 (75.443)\tPrec@5 98.438 (95.390)\n",
            "Epoch: [46][250/782]\tTime 0.024 (0.024)\tLoss (Class) 0.9306 (1.0057)\tLoss (Regu) 0.1888 (0.1878)\tPrec@1 75.000 (75.143)\tPrec@5 96.875 (95.375)\n",
            "Epoch: [46][300/782]\tTime 0.022 (0.024)\tLoss (Class) 1.3892 (1.0039)\tLoss (Regu) 0.1860 (0.1875)\tPrec@1 62.500 (75.078)\tPrec@5 93.750 (95.510)\n",
            "Epoch: [46][350/782]\tTime 0.023 (0.024)\tLoss (Class) 0.9225 (1.0122)\tLoss (Regu) 0.1864 (0.1873)\tPrec@1 75.000 (74.791)\tPrec@5 98.438 (95.491)\n",
            "Epoch: [46][400/782]\tTime 0.023 (0.024)\tLoss (Class) 1.1249 (1.0199)\tLoss (Regu) 0.1864 (0.1872)\tPrec@1 67.188 (74.536)\tPrec@5 95.312 (95.398)\n",
            "Epoch: [46][450/782]\tTime 0.021 (0.024)\tLoss (Class) 0.8566 (1.0203)\tLoss (Regu) 0.1834 (0.1872)\tPrec@1 76.562 (74.501)\tPrec@5 96.875 (95.413)\n",
            "Epoch: [46][500/782]\tTime 0.022 (0.023)\tLoss (Class) 0.9031 (1.0231)\tLoss (Regu) 0.1865 (0.1871)\tPrec@1 79.688 (74.386)\tPrec@5 98.438 (95.359)\n",
            "Epoch: [46][550/782]\tTime 0.022 (0.023)\tLoss (Class) 0.6374 (1.0255)\tLoss (Regu) 0.1864 (0.1869)\tPrec@1 87.500 (74.294)\tPrec@5 98.438 (95.327)\n",
            "Epoch: [46][600/782]\tTime 0.024 (0.023)\tLoss (Class) 1.1177 (1.0296)\tLoss (Regu) 0.1853 (0.1867)\tPrec@1 75.000 (74.199)\tPrec@5 96.875 (95.279)\n",
            "Epoch: [46][650/782]\tTime 0.024 (0.023)\tLoss (Class) 1.1894 (1.0313)\tLoss (Regu) 0.1869 (0.1866)\tPrec@1 64.062 (74.138)\tPrec@5 95.312 (95.267)\n",
            "Epoch: [46][700/782]\tTime 0.023 (0.023)\tLoss (Class) 0.7806 (1.0326)\tLoss (Regu) 0.1880 (0.1865)\tPrec@1 78.125 (74.106)\tPrec@5 98.438 (95.259)\n",
            "Epoch: [46][750/782]\tTime 0.023 (0.023)\tLoss (Class) 1.0864 (1.0338)\tLoss (Regu) 0.1860 (0.1866)\tPrec@1 75.000 (74.126)\tPrec@5 95.312 (95.231)\n",
            "Test: [0/157]\tTime 0.110 (0.110)\tLoss (Class) 1.6637 (1.6637)\tLoss (Regu) 0.2123 (0.2123)\tPrec@1 65.625 (65.625)\tPrec@5 85.938 (85.938)\n",
            "Test: [50/157]\tTime 0.008 (0.010)\tLoss (Class) 1.9114 (1.6631)\tLoss (Regu) 0.2159 (0.2094)\tPrec@1 54.688 (63.235)\tPrec@5 87.500 (87.469)\n",
            "Test: [100/157]\tTime 0.008 (0.009)\tLoss (Class) 2.0042 (1.6728)\tLoss (Regu) 0.2106 (0.2093)\tPrec@1 57.812 (62.624)\tPrec@5 87.500 (87.438)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.9468 (1.6804)\tLoss (Regu) 0.2039 (0.2089)\tPrec@1 59.375 (62.417)\tPrec@5 87.500 (87.562)\n",
            " * Train[74.142 %, 95.202 %, 1.034 loss] Val [62.460 %, 87.590%, 1.677 loss] Best: 62.460 %\n",
            "Time for 46 / 150 19.83204984664917\n",
            "Learning rate:  0.03\n",
            "Epoch: [47][0/782]\tTime 0.148 (0.148)\tLoss (Class) 0.9221 (0.9221)\tLoss (Regu) 0.1832 (0.1832)\tPrec@1 73.438 (73.438)\tPrec@5 98.438 (98.438)\n",
            "Epoch: [47][50/782]\tTime 0.021 (0.027)\tLoss (Class) 0.7672 (0.9483)\tLoss (Regu) 0.1873 (0.1871)\tPrec@1 85.938 (77.083)\tPrec@5 98.438 (96.385)\n",
            "Epoch: [47][100/782]\tTime 0.023 (0.026)\tLoss (Class) 0.8865 (0.9517)\tLoss (Regu) 0.1828 (0.1873)\tPrec@1 73.438 (77.150)\tPrec@5 95.312 (96.225)\n",
            "Epoch: [47][150/782]\tTime 0.021 (0.026)\tLoss (Class) 1.0283 (0.9699)\tLoss (Regu) 0.1866 (0.1871)\tPrec@1 71.875 (76.345)\tPrec@5 95.312 (95.964)\n",
            "Epoch: [47][200/782]\tTime 0.023 (0.026)\tLoss (Class) 1.0030 (0.9692)\tLoss (Regu) 0.1867 (0.1873)\tPrec@1 71.875 (76.329)\tPrec@5 95.312 (95.958)\n",
            "Epoch: [47][250/782]\tTime 0.023 (0.025)\tLoss (Class) 0.9268 (0.9824)\tLoss (Regu) 0.1868 (0.1871)\tPrec@1 73.438 (75.710)\tPrec@5 95.312 (95.873)\n",
            "Epoch: [47][300/782]\tTime 0.023 (0.025)\tLoss (Class) 1.2345 (0.9941)\tLoss (Regu) 0.1906 (0.1872)\tPrec@1 70.312 (75.467)\tPrec@5 95.312 (95.795)\n",
            "Epoch: [47][350/782]\tTime 0.023 (0.024)\tLoss (Class) 1.0548 (1.0023)\tLoss (Regu) 0.1839 (0.1871)\tPrec@1 70.312 (75.142)\tPrec@5 98.438 (95.718)\n",
            "Epoch: [47][400/782]\tTime 0.023 (0.024)\tLoss (Class) 1.1304 (1.0049)\tLoss (Regu) 0.1853 (0.1869)\tPrec@1 67.188 (75.164)\tPrec@5 98.438 (95.675)\n",
            "Epoch: [47][450/782]\tTime 0.023 (0.024)\tLoss (Class) 1.2281 (1.0076)\tLoss (Regu) 0.1880 (0.1869)\tPrec@1 65.625 (75.062)\tPrec@5 93.750 (95.635)\n",
            "Epoch: [47][500/782]\tTime 0.023 (0.024)\tLoss (Class) 1.3008 (1.0110)\tLoss (Regu) 0.1841 (0.1869)\tPrec@1 67.188 (74.978)\tPrec@5 89.062 (95.581)\n",
            "Epoch: [47][550/782]\tTime 0.023 (0.024)\tLoss (Class) 1.3260 (1.0144)\tLoss (Regu) 0.1854 (0.1869)\tPrec@1 70.312 (74.895)\tPrec@5 90.625 (95.585)\n",
            "Epoch: [47][600/782]\tTime 0.030 (0.024)\tLoss (Class) 0.7953 (1.0149)\tLoss (Regu) 0.1879 (0.1868)\tPrec@1 81.250 (74.854)\tPrec@5 98.438 (95.580)\n",
            "Epoch: [47][650/782]\tTime 0.022 (0.024)\tLoss (Class) 0.9259 (1.0200)\tLoss (Regu) 0.1896 (0.1869)\tPrec@1 76.562 (74.712)\tPrec@5 93.750 (95.509)\n",
            "Epoch: [47][700/782]\tTime 0.024 (0.024)\tLoss (Class) 0.9464 (1.0226)\tLoss (Regu) 0.1854 (0.1869)\tPrec@1 76.562 (74.576)\tPrec@5 98.438 (95.524)\n",
            "Epoch: [47][750/782]\tTime 0.025 (0.024)\tLoss (Class) 1.0036 (1.0259)\tLoss (Regu) 0.1917 (0.1868)\tPrec@1 76.562 (74.528)\tPrec@5 93.750 (95.458)\n",
            "Test: [0/157]\tTime 0.112 (0.112)\tLoss (Class) 1.5362 (1.5362)\tLoss (Regu) 0.2140 (0.2140)\tPrec@1 68.750 (68.750)\tPrec@5 87.500 (87.500)\n",
            "Test: [50/157]\tTime 0.008 (0.010)\tLoss (Class) 1.3487 (1.6203)\tLoss (Regu) 0.2055 (0.2086)\tPrec@1 64.062 (62.714)\tPrec@5 93.750 (88.603)\n",
            "Test: [100/157]\tTime 0.008 (0.009)\tLoss (Class) 1.6080 (1.6205)\tLoss (Regu) 0.2124 (0.2089)\tPrec@1 64.062 (63.041)\tPrec@5 84.375 (88.506)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.2287 (1.6123)\tLoss (Regu) 0.2087 (0.2090)\tPrec@1 70.312 (63.028)\tPrec@5 89.062 (88.524)\n",
            " * Train[74.482 %, 95.416 %, 1.027 loss] Val [63.050 %, 88.610%, 1.611 loss] Best: 63.050 %\n",
            "Time for 47 / 150 20.300350427627563\n",
            "Learning rate:  0.03\n",
            "Epoch: [48][0/782]\tTime 0.149 (0.149)\tLoss (Class) 1.0842 (1.0842)\tLoss (Regu) 0.1874 (0.1874)\tPrec@1 76.562 (76.562)\tPrec@5 93.750 (93.750)\n",
            "Epoch: [48][50/782]\tTime 0.025 (0.025)\tLoss (Class) 0.7628 (1.0200)\tLoss (Regu) 0.1913 (0.1874)\tPrec@1 84.375 (75.000)\tPrec@5 98.438 (95.650)\n",
            "Epoch: [48][100/782]\tTime 0.022 (0.025)\tLoss (Class) 0.9337 (0.9826)\tLoss (Regu) 0.1868 (0.1882)\tPrec@1 84.375 (76.191)\tPrec@5 93.750 (96.055)\n",
            "Epoch: [48][150/782]\tTime 0.022 (0.024)\tLoss (Class) 1.0124 (0.9722)\tLoss (Regu) 0.1883 (0.1878)\tPrec@1 78.125 (76.325)\tPrec@5 95.312 (96.047)\n",
            "Epoch: [48][200/782]\tTime 0.023 (0.024)\tLoss (Class) 1.1328 (0.9770)\tLoss (Regu) 0.1869 (0.1874)\tPrec@1 73.438 (75.979)\tPrec@5 90.625 (95.826)\n",
            "Epoch: [48][250/782]\tTime 0.022 (0.024)\tLoss (Class) 1.0122 (0.9777)\tLoss (Regu) 0.1858 (0.1875)\tPrec@1 82.812 (75.890)\tPrec@5 95.312 (95.891)\n",
            "Epoch: [48][300/782]\tTime 0.030 (0.024)\tLoss (Class) 0.9282 (0.9871)\tLoss (Regu) 0.1871 (0.1878)\tPrec@1 79.688 (75.540)\tPrec@5 93.750 (95.821)\n",
            "Epoch: [48][350/782]\tTime 0.022 (0.024)\tLoss (Class) 1.1478 (0.9922)\tLoss (Regu) 0.1864 (0.1875)\tPrec@1 73.438 (75.303)\tPrec@5 89.062 (95.798)\n",
            "Epoch: [48][400/782]\tTime 0.022 (0.024)\tLoss (Class) 0.8406 (0.9980)\tLoss (Regu) 0.1872 (0.1870)\tPrec@1 81.250 (75.203)\tPrec@5 98.438 (95.749)\n",
            "Epoch: [48][450/782]\tTime 0.022 (0.024)\tLoss (Class) 0.9229 (1.0002)\tLoss (Regu) 0.1877 (0.1869)\tPrec@1 79.688 (75.159)\tPrec@5 95.312 (95.673)\n",
            "Epoch: [48][500/782]\tTime 0.023 (0.024)\tLoss (Class) 0.9416 (1.0052)\tLoss (Regu) 0.1887 (0.1868)\tPrec@1 81.250 (75.069)\tPrec@5 95.312 (95.596)\n",
            "Epoch: [48][550/782]\tTime 0.031 (0.024)\tLoss (Class) 0.9902 (1.0083)\tLoss (Regu) 0.1853 (0.1867)\tPrec@1 71.875 (74.932)\tPrec@5 98.438 (95.630)\n",
            "Epoch: [48][600/782]\tTime 0.022 (0.024)\tLoss (Class) 1.1138 (1.0121)\tLoss (Regu) 0.1854 (0.1866)\tPrec@1 73.438 (74.852)\tPrec@5 96.875 (95.611)\n",
            "Epoch: [48][650/782]\tTime 0.023 (0.024)\tLoss (Class) 1.1639 (1.0159)\tLoss (Regu) 0.1868 (0.1864)\tPrec@1 70.312 (74.760)\tPrec@5 93.750 (95.521)\n",
            "Epoch: [48][700/782]\tTime 0.023 (0.024)\tLoss (Class) 1.2851 (1.0203)\tLoss (Regu) 0.1828 (0.1863)\tPrec@1 70.312 (74.724)\tPrec@5 95.312 (95.486)\n",
            "Epoch: [48][750/782]\tTime 0.022 (0.024)\tLoss (Class) 0.7738 (1.0207)\tLoss (Regu) 0.1864 (0.1862)\tPrec@1 81.250 (74.740)\tPrec@5 96.875 (95.439)\n",
            "Test: [0/157]\tTime 0.106 (0.106)\tLoss (Class) 1.2470 (1.2470)\tLoss (Regu) 0.2114 (0.2114)\tPrec@1 75.000 (75.000)\tPrec@5 89.062 (89.062)\n",
            "Test: [50/157]\tTime 0.007 (0.010)\tLoss (Class) 2.0384 (1.7886)\tLoss (Regu) 0.2059 (0.2113)\tPrec@1 64.062 (60.263)\tPrec@5 87.500 (86.703)\n",
            "Test: [100/157]\tTime 0.008 (0.009)\tLoss (Class) 2.2759 (1.7913)\tLoss (Regu) 0.2110 (0.2115)\tPrec@1 56.250 (60.009)\tPrec@5 75.000 (86.696)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 2.0946 (1.7720)\tLoss (Regu) 0.2122 (0.2113)\tPrec@1 62.500 (60.524)\tPrec@5 75.000 (86.910)\n",
            " * Train[74.748 %, 95.428 %, 1.021 loss] Val [60.590 %, 86.980%, 1.768 loss] Best: 63.050 %\n",
            "Time for 48 / 150 20.112823247909546\n",
            "Learning rate:  0.03\n",
            "Epoch: [49][0/782]\tTime 0.163 (0.163)\tLoss (Class) 1.0159 (1.0159)\tLoss (Regu) 0.1821 (0.1821)\tPrec@1 76.562 (76.562)\tPrec@5 90.625 (90.625)\n",
            "Epoch: [49][50/782]\tTime 0.022 (0.027)\tLoss (Class) 0.8032 (0.9349)\tLoss (Regu) 0.1897 (0.1891)\tPrec@1 82.812 (77.022)\tPrec@5 98.438 (96.569)\n",
            "Epoch: [49][100/782]\tTime 0.025 (0.025)\tLoss (Class) 0.9266 (0.9477)\tLoss (Regu) 0.1848 (0.1887)\tPrec@1 81.250 (76.501)\tPrec@5 95.312 (96.426)\n",
            "Epoch: [49][150/782]\tTime 0.022 (0.024)\tLoss (Class) 1.0831 (0.9563)\tLoss (Regu) 0.1864 (0.1879)\tPrec@1 76.562 (76.283)\tPrec@5 92.188 (96.378)\n",
            "Epoch: [49][200/782]\tTime 0.022 (0.024)\tLoss (Class) 1.2351 (0.9748)\tLoss (Regu) 0.1886 (0.1878)\tPrec@1 67.188 (75.995)\tPrec@5 95.312 (96.144)\n",
            "Epoch: [49][250/782]\tTime 0.023 (0.023)\tLoss (Class) 0.9872 (0.9853)\tLoss (Regu) 0.1888 (0.1879)\tPrec@1 76.562 (75.710)\tPrec@5 95.312 (96.066)\n",
            "Epoch: [49][300/782]\tTime 0.022 (0.023)\tLoss (Class) 0.8872 (0.9849)\tLoss (Regu) 0.1904 (0.1878)\tPrec@1 75.000 (75.799)\tPrec@5 93.750 (95.998)\n",
            "Epoch: [49][350/782]\tTime 0.024 (0.023)\tLoss (Class) 1.0267 (0.9827)\tLoss (Regu) 0.1820 (0.1878)\tPrec@1 75.000 (75.828)\tPrec@5 95.312 (95.967)\n",
            "Epoch: [49][400/782]\tTime 0.023 (0.023)\tLoss (Class) 1.0466 (0.9895)\tLoss (Regu) 0.1827 (0.1875)\tPrec@1 70.312 (75.686)\tPrec@5 95.312 (95.901)\n",
            "Epoch: [49][450/782]\tTime 0.030 (0.023)\tLoss (Class) 0.9382 (0.9937)\tLoss (Regu) 0.1867 (0.1873)\tPrec@1 79.688 (75.568)\tPrec@5 93.750 (95.780)\n",
            "Epoch: [49][500/782]\tTime 0.022 (0.023)\tLoss (Class) 1.1067 (0.9946)\tLoss (Regu) 0.1853 (0.1873)\tPrec@1 65.625 (75.524)\tPrec@5 96.875 (95.749)\n",
            "Epoch: [49][550/782]\tTime 0.023 (0.023)\tLoss (Class) 0.8057 (1.0006)\tLoss (Regu) 0.1884 (0.1873)\tPrec@1 76.562 (75.286)\tPrec@5 95.312 (95.658)\n",
            "Epoch: [49][600/782]\tTime 0.023 (0.023)\tLoss (Class) 0.9135 (1.0028)\tLoss (Regu) 0.1835 (0.1871)\tPrec@1 78.125 (75.244)\tPrec@5 95.312 (95.609)\n",
            "Epoch: [49][650/782]\tTime 0.032 (0.024)\tLoss (Class) 1.2111 (1.0035)\tLoss (Regu) 0.1818 (0.1869)\tPrec@1 67.188 (75.187)\tPrec@5 93.750 (95.591)\n",
            "Epoch: [49][700/782]\tTime 0.023 (0.024)\tLoss (Class) 1.1632 (1.0038)\tLoss (Regu) 0.1861 (0.1869)\tPrec@1 67.188 (75.152)\tPrec@5 96.875 (95.607)\n",
            "Epoch: [49][750/782]\tTime 0.032 (0.024)\tLoss (Class) 1.4663 (1.0093)\tLoss (Regu) 0.1832 (0.1868)\tPrec@1 65.625 (74.971)\tPrec@5 89.062 (95.535)\n",
            "Test: [0/157]\tTime 0.109 (0.109)\tLoss (Class) 1.8502 (1.8502)\tLoss (Regu) 0.2079 (0.2079)\tPrec@1 62.500 (62.500)\tPrec@5 84.375 (84.375)\n",
            "Test: [50/157]\tTime 0.008 (0.012)\tLoss (Class) 2.5311 (1.8491)\tLoss (Regu) 0.2096 (0.2083)\tPrec@1 50.000 (59.988)\tPrec@5 79.688 (85.968)\n",
            "Test: [100/157]\tTime 0.007 (0.010)\tLoss (Class) 2.0852 (1.8228)\tLoss (Regu) 0.2034 (0.2082)\tPrec@1 53.125 (59.514)\tPrec@5 84.375 (86.479)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.7042 (1.8310)\tLoss (Regu) 0.2073 (0.2082)\tPrec@1 60.938 (59.447)\tPrec@5 90.625 (86.155)\n",
            " * Train[74.862 %, 95.498 %, 1.011 loss] Val [59.450 %, 86.240%, 1.829 loss] Best: 63.050 %\n",
            "Time for 49 / 150 20.082693815231323\n",
            "Learning rate:  0.03\n",
            "Epoch: [50][0/782]\tTime 0.146 (0.146)\tLoss (Class) 1.0423 (1.0423)\tLoss (Regu) 0.1869 (0.1869)\tPrec@1 73.438 (73.438)\tPrec@5 96.875 (96.875)\n",
            "Epoch: [50][50/782]\tTime 0.023 (0.025)\tLoss (Class) 0.8956 (0.9492)\tLoss (Regu) 0.1847 (0.1871)\tPrec@1 78.125 (76.746)\tPrec@5 98.438 (96.324)\n",
            "Epoch: [50][100/782]\tTime 0.023 (0.024)\tLoss (Class) 1.2065 (0.9543)\tLoss (Regu) 0.1863 (0.1874)\tPrec@1 71.875 (76.779)\tPrec@5 96.875 (96.117)\n",
            "Epoch: [50][150/782]\tTime 0.022 (0.024)\tLoss (Class) 0.9641 (0.9575)\tLoss (Regu) 0.1831 (0.1875)\tPrec@1 79.688 (76.925)\tPrec@5 98.438 (96.068)\n",
            "Epoch: [50][200/782]\tTime 0.023 (0.025)\tLoss (Class) 0.8893 (0.9488)\tLoss (Regu) 0.1866 (0.1872)\tPrec@1 79.688 (77.006)\tPrec@5 100.000 (96.121)\n",
            "Epoch: [50][250/782]\tTime 0.022 (0.025)\tLoss (Class) 0.7377 (0.9557)\tLoss (Regu) 0.1924 (0.1875)\tPrec@1 84.375 (76.762)\tPrec@5 98.438 (96.022)\n",
            "Epoch: [50][300/782]\tTime 0.022 (0.025)\tLoss (Class) 1.1727 (0.9616)\tLoss (Regu) 0.1856 (0.1877)\tPrec@1 73.438 (76.464)\tPrec@5 96.875 (96.024)\n",
            "Epoch: [50][350/782]\tTime 0.023 (0.025)\tLoss (Class) 0.8658 (0.9639)\tLoss (Regu) 0.1897 (0.1879)\tPrec@1 81.250 (76.487)\tPrec@5 98.438 (95.958)\n",
            "Epoch: [50][400/782]\tTime 0.022 (0.025)\tLoss (Class) 1.0067 (0.9721)\tLoss (Regu) 0.1859 (0.1880)\tPrec@1 73.438 (76.224)\tPrec@5 98.438 (95.924)\n",
            "Epoch: [50][450/782]\tTime 0.022 (0.025)\tLoss (Class) 1.1381 (0.9763)\tLoss (Regu) 0.1909 (0.1881)\tPrec@1 76.562 (76.098)\tPrec@5 90.625 (95.832)\n",
            "Epoch: [50][500/782]\tTime 0.022 (0.025)\tLoss (Class) 1.0312 (0.9766)\tLoss (Regu) 0.1877 (0.1880)\tPrec@1 75.000 (76.101)\tPrec@5 92.188 (95.877)\n",
            "Epoch: [50][550/782]\tTime 0.023 (0.025)\tLoss (Class) 0.8505 (0.9778)\tLoss (Regu) 0.1841 (0.1880)\tPrec@1 78.125 (76.089)\tPrec@5 100.000 (95.851)\n",
            "Epoch: [50][600/782]\tTime 0.023 (0.025)\tLoss (Class) 0.9609 (0.9833)\tLoss (Regu) 0.1881 (0.1878)\tPrec@1 78.125 (75.933)\tPrec@5 95.312 (95.809)\n",
            "Epoch: [50][650/782]\tTime 0.030 (0.025)\tLoss (Class) 1.0562 (0.9888)\tLoss (Regu) 0.1837 (0.1877)\tPrec@1 76.562 (75.708)\tPrec@5 95.312 (95.725)\n",
            "Epoch: [50][700/782]\tTime 0.031 (0.025)\tLoss (Class) 1.0576 (0.9925)\tLoss (Regu) 0.1821 (0.1875)\tPrec@1 68.750 (75.664)\tPrec@5 96.875 (95.691)\n",
            "Epoch: [50][750/782]\tTime 0.023 (0.025)\tLoss (Class) 0.9596 (0.9964)\tLoss (Regu) 0.1849 (0.1872)\tPrec@1 78.125 (75.539)\tPrec@5 92.188 (95.620)\n",
            "Test: [0/157]\tTime 0.112 (0.112)\tLoss (Class) 1.9795 (1.9795)\tLoss (Regu) 0.2029 (0.2029)\tPrec@1 60.938 (60.938)\tPrec@5 85.938 (85.938)\n",
            "Test: [50/157]\tTime 0.008 (0.011)\tLoss (Class) 1.2619 (1.6122)\tLoss (Regu) 0.2087 (0.2079)\tPrec@1 71.875 (62.929)\tPrec@5 90.625 (87.929)\n",
            "Test: [100/157]\tTime 0.009 (0.009)\tLoss (Class) 1.5083 (1.6706)\tLoss (Regu) 0.2082 (0.2086)\tPrec@1 60.938 (61.881)\tPrec@5 90.625 (87.577)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 2.0438 (1.6798)\tLoss (Regu) 0.2047 (0.2085)\tPrec@1 56.250 (62.086)\tPrec@5 84.375 (87.810)\n",
            " * Train[75.458 %, 95.590 %, 0.999 loss] Val [62.190 %, 87.880%, 1.678 loss] Best: 63.050 %\n",
            "Time for 50 / 150 20.720675468444824\n",
            "Learning rate:  0.03\n",
            "Epoch: [51][0/782]\tTime 0.153 (0.153)\tLoss (Class) 1.0270 (1.0270)\tLoss (Regu) 0.1854 (0.1854)\tPrec@1 70.312 (70.312)\tPrec@5 98.438 (98.438)\n",
            "Epoch: [51][50/782]\tTime 0.023 (0.029)\tLoss (Class) 0.7684 (0.9302)\tLoss (Regu) 0.1843 (0.1872)\tPrec@1 82.812 (77.328)\tPrec@5 98.438 (96.599)\n",
            "Epoch: [51][100/782]\tTime 0.023 (0.027)\tLoss (Class) 1.1789 (0.9244)\tLoss (Regu) 0.1874 (0.1878)\tPrec@1 71.875 (77.336)\tPrec@5 92.188 (96.411)\n",
            "Epoch: [51][150/782]\tTime 0.023 (0.027)\tLoss (Class) 0.9626 (0.9386)\tLoss (Regu) 0.1835 (0.1874)\tPrec@1 71.875 (76.997)\tPrec@5 95.312 (96.409)\n",
            "Epoch: [51][200/782]\tTime 0.022 (0.026)\tLoss (Class) 0.7923 (0.9609)\tLoss (Regu) 0.1840 (0.1867)\tPrec@1 82.812 (76.353)\tPrec@5 96.875 (96.105)\n",
            "Epoch: [51][250/782]\tTime 0.027 (0.026)\tLoss (Class) 0.8373 (0.9705)\tLoss (Regu) 0.1891 (0.1865)\tPrec@1 78.125 (75.822)\tPrec@5 98.438 (96.103)\n",
            "Epoch: [51][300/782]\tTime 0.023 (0.026)\tLoss (Class) 1.1262 (0.9755)\tLoss (Regu) 0.1848 (0.1865)\tPrec@1 71.875 (75.628)\tPrec@5 92.188 (96.003)\n",
            "Epoch: [51][350/782]\tTime 0.023 (0.025)\tLoss (Class) 1.1117 (0.9788)\tLoss (Regu) 0.1868 (0.1867)\tPrec@1 73.438 (75.565)\tPrec@5 96.875 (95.980)\n",
            "Epoch: [51][400/782]\tTime 0.023 (0.025)\tLoss (Class) 0.9254 (0.9802)\tLoss (Regu) 0.1888 (0.1867)\tPrec@1 78.125 (75.522)\tPrec@5 95.312 (95.924)\n",
            "Epoch: [51][450/782]\tTime 0.022 (0.025)\tLoss (Class) 0.9319 (0.9846)\tLoss (Regu) 0.1837 (0.1866)\tPrec@1 78.125 (75.340)\tPrec@5 95.312 (95.877)\n",
            "Epoch: [51][500/782]\tTime 0.022 (0.025)\tLoss (Class) 0.9385 (0.9877)\tLoss (Regu) 0.1889 (0.1867)\tPrec@1 76.562 (75.193)\tPrec@5 95.312 (95.877)\n",
            "Epoch: [51][550/782]\tTime 0.023 (0.025)\tLoss (Class) 1.1990 (0.9887)\tLoss (Regu) 0.1821 (0.1866)\tPrec@1 64.062 (75.153)\tPrec@5 98.438 (95.834)\n",
            "Epoch: [51][600/782]\tTime 0.022 (0.025)\tLoss (Class) 0.9481 (0.9916)\tLoss (Regu) 0.1807 (0.1866)\tPrec@1 73.438 (75.091)\tPrec@5 98.438 (95.806)\n",
            "Epoch: [51][650/782]\tTime 0.023 (0.025)\tLoss (Class) 1.2501 (0.9938)\tLoss (Regu) 0.1830 (0.1864)\tPrec@1 71.875 (75.062)\tPrec@5 90.625 (95.754)\n",
            "Epoch: [51][700/782]\tTime 0.023 (0.025)\tLoss (Class) 0.9861 (0.9966)\tLoss (Regu) 0.1840 (0.1863)\tPrec@1 75.000 (75.051)\tPrec@5 96.875 (95.740)\n",
            "Epoch: [51][750/782]\tTime 0.023 (0.025)\tLoss (Class) 1.0873 (0.9997)\tLoss (Regu) 0.1856 (0.1862)\tPrec@1 71.875 (74.994)\tPrec@5 93.750 (95.681)\n",
            "Test: [0/157]\tTime 0.114 (0.114)\tLoss (Class) 1.4683 (1.4683)\tLoss (Regu) 0.2017 (0.2017)\tPrec@1 71.875 (71.875)\tPrec@5 87.500 (87.500)\n",
            "Test: [50/157]\tTime 0.008 (0.013)\tLoss (Class) 1.5320 (1.5974)\tLoss (Regu) 0.2069 (0.2034)\tPrec@1 67.188 (63.817)\tPrec@5 89.062 (88.419)\n",
            "Test: [100/157]\tTime 0.007 (0.011)\tLoss (Class) 1.6933 (1.6099)\tLoss (Regu) 0.2060 (0.2039)\tPrec@1 60.938 (63.521)\tPrec@5 82.812 (88.475)\n",
            "Test: [150/157]\tTime 0.007 (0.010)\tLoss (Class) 1.6478 (1.6188)\tLoss (Regu) 0.2045 (0.2039)\tPrec@1 65.625 (63.483)\tPrec@5 89.062 (88.483)\n",
            " * Train[74.992 %, 95.678 %, 1.000 loss] Val [63.520 %, 88.410%, 1.619 loss] Best: 63.520 %\n",
            "Time for 51 / 150 20.845725774765015\n",
            "Learning rate:  0.03\n",
            "Epoch: [52][0/782]\tTime 0.147 (0.147)\tLoss (Class) 0.9390 (0.9390)\tLoss (Regu) 0.1822 (0.1822)\tPrec@1 76.562 (76.562)\tPrec@5 95.312 (95.312)\n",
            "Epoch: [52][50/782]\tTime 0.022 (0.025)\tLoss (Class) 0.8683 (0.8979)\tLoss (Regu) 0.1913 (0.1876)\tPrec@1 81.250 (78.768)\tPrec@5 93.750 (96.783)\n",
            "Epoch: [52][100/782]\tTime 0.023 (0.024)\tLoss (Class) 1.0817 (0.9069)\tLoss (Regu) 0.1814 (0.1883)\tPrec@1 67.188 (77.924)\tPrec@5 100.000 (96.736)\n",
            "Epoch: [52][150/782]\tTime 0.022 (0.024)\tLoss (Class) 0.8966 (0.9303)\tLoss (Regu) 0.1813 (0.1876)\tPrec@1 78.125 (77.142)\tPrec@5 95.312 (96.502)\n",
            "Epoch: [52][200/782]\tTime 0.024 (0.024)\tLoss (Class) 0.9921 (0.9325)\tLoss (Regu) 0.1882 (0.1872)\tPrec@1 73.438 (77.044)\tPrec@5 96.875 (96.447)\n",
            "Epoch: [52][250/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8455 (0.9334)\tLoss (Regu) 0.1892 (0.1875)\tPrec@1 76.562 (76.731)\tPrec@5 95.312 (96.445)\n",
            "Epoch: [52][300/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8599 (0.9404)\tLoss (Regu) 0.1855 (0.1874)\tPrec@1 81.250 (76.578)\tPrec@5 93.750 (96.397)\n",
            "Epoch: [52][350/782]\tTime 0.024 (0.024)\tLoss (Class) 0.8833 (0.9463)\tLoss (Regu) 0.1870 (0.1874)\tPrec@1 84.375 (76.585)\tPrec@5 98.438 (96.305)\n",
            "Epoch: [52][400/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7818 (0.9497)\tLoss (Regu) 0.1868 (0.1872)\tPrec@1 79.688 (76.531)\tPrec@5 100.000 (96.252)\n",
            "Epoch: [52][450/782]\tTime 0.023 (0.024)\tLoss (Class) 1.1084 (0.9567)\tLoss (Regu) 0.1832 (0.1871)\tPrec@1 67.188 (76.375)\tPrec@5 92.188 (96.158)\n",
            "Epoch: [52][500/782]\tTime 0.023 (0.024)\tLoss (Class) 1.0296 (0.9617)\tLoss (Regu) 0.1885 (0.1870)\tPrec@1 70.312 (76.216)\tPrec@5 96.875 (96.108)\n",
            "Epoch: [52][550/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8795 (0.9659)\tLoss (Regu) 0.1867 (0.1869)\tPrec@1 79.688 (76.097)\tPrec@5 100.000 (96.061)\n",
            "Epoch: [52][600/782]\tTime 0.032 (0.024)\tLoss (Class) 0.6211 (0.9742)\tLoss (Regu) 0.1931 (0.1869)\tPrec@1 85.938 (75.780)\tPrec@5 100.000 (95.981)\n",
            "Epoch: [52][650/782]\tTime 0.023 (0.024)\tLoss (Class) 1.0949 (0.9779)\tLoss (Regu) 0.1883 (0.1869)\tPrec@1 73.438 (75.679)\tPrec@5 93.750 (95.891)\n",
            "Epoch: [52][700/782]\tTime 0.023 (0.024)\tLoss (Class) 1.2365 (0.9831)\tLoss (Regu) 0.1905 (0.1868)\tPrec@1 67.188 (75.508)\tPrec@5 96.875 (95.825)\n",
            "Epoch: [52][750/782]\tTime 0.023 (0.024)\tLoss (Class) 1.0887 (0.9874)\tLoss (Regu) 0.1900 (0.1870)\tPrec@1 75.000 (75.381)\tPrec@5 93.750 (95.799)\n",
            "Test: [0/157]\tTime 0.114 (0.114)\tLoss (Class) 1.6235 (1.6235)\tLoss (Regu) 0.2163 (0.2163)\tPrec@1 59.375 (59.375)\tPrec@5 89.062 (89.062)\n",
            "Test: [50/157]\tTime 0.008 (0.010)\tLoss (Class) 1.3644 (1.6582)\tLoss (Regu) 0.2195 (0.2143)\tPrec@1 71.875 (62.040)\tPrec@5 93.750 (88.634)\n",
            "Test: [100/157]\tTime 0.008 (0.009)\tLoss (Class) 1.3996 (1.6556)\tLoss (Regu) 0.2150 (0.2141)\tPrec@1 71.875 (62.871)\tPrec@5 92.188 (88.397)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.7418 (1.6508)\tLoss (Regu) 0.2139 (0.2143)\tPrec@1 67.188 (63.411)\tPrec@5 84.375 (88.442)\n",
            " * Train[75.296 %, 95.760 %, 0.989 loss] Val [63.390 %, 88.480%, 1.650 loss] Best: 63.520 %\n",
            "Time for 52 / 150 20.17121458053589\n",
            "Learning rate:  0.03\n",
            "Epoch: [53][0/782]\tTime 0.160 (0.160)\tLoss (Class) 1.0754 (1.0754)\tLoss (Regu) 0.1870 (0.1870)\tPrec@1 71.875 (71.875)\tPrec@5 96.875 (96.875)\n",
            "Epoch: [53][50/782]\tTime 0.030 (0.028)\tLoss (Class) 0.7955 (0.9464)\tLoss (Regu) 0.1932 (0.1911)\tPrec@1 82.812 (77.053)\tPrec@5 100.000 (96.722)\n",
            "Epoch: [53][100/782]\tTime 0.022 (0.026)\tLoss (Class) 0.7705 (0.9421)\tLoss (Regu) 0.1920 (0.1909)\tPrec@1 78.125 (76.965)\tPrec@5 98.438 (96.566)\n",
            "Epoch: [53][150/782]\tTime 0.029 (0.026)\tLoss (Class) 1.0358 (0.9435)\tLoss (Regu) 0.1885 (0.1900)\tPrec@1 78.125 (76.883)\tPrec@5 93.750 (96.471)\n",
            "Epoch: [53][200/782]\tTime 0.022 (0.025)\tLoss (Class) 1.0758 (0.9450)\tLoss (Regu) 0.1871 (0.1892)\tPrec@1 76.562 (76.749)\tPrec@5 93.750 (96.494)\n",
            "Epoch: [53][250/782]\tTime 0.022 (0.025)\tLoss (Class) 0.9512 (0.9471)\tLoss (Regu) 0.1892 (0.1885)\tPrec@1 75.000 (76.687)\tPrec@5 93.750 (96.439)\n",
            "Epoch: [53][300/782]\tTime 0.022 (0.024)\tLoss (Class) 0.9828 (0.9541)\tLoss (Regu) 0.1868 (0.1883)\tPrec@1 76.562 (76.485)\tPrec@5 96.875 (96.299)\n",
            "Epoch: [53][350/782]\tTime 0.022 (0.024)\tLoss (Class) 1.0669 (0.9569)\tLoss (Regu) 0.1866 (0.1883)\tPrec@1 76.562 (76.473)\tPrec@5 95.312 (96.230)\n",
            "Epoch: [53][400/782]\tTime 0.026 (0.024)\tLoss (Class) 0.9036 (0.9637)\tLoss (Regu) 0.1866 (0.1883)\tPrec@1 81.250 (76.165)\tPrec@5 93.750 (96.170)\n",
            "Epoch: [53][450/782]\tTime 0.030 (0.024)\tLoss (Class) 0.9148 (0.9698)\tLoss (Regu) 0.1832 (0.1881)\tPrec@1 70.312 (76.012)\tPrec@5 98.438 (96.078)\n",
            "Epoch: [53][500/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6716 (0.9744)\tLoss (Regu) 0.1862 (0.1878)\tPrec@1 87.500 (75.867)\tPrec@5 100.000 (95.989)\n",
            "Epoch: [53][550/782]\tTime 0.023 (0.024)\tLoss (Class) 0.9903 (0.9747)\tLoss (Regu) 0.1890 (0.1877)\tPrec@1 76.562 (75.927)\tPrec@5 96.875 (95.990)\n",
            "Epoch: [53][600/782]\tTime 0.022 (0.024)\tLoss (Class) 1.1513 (0.9759)\tLoss (Regu) 0.1892 (0.1878)\tPrec@1 78.125 (75.824)\tPrec@5 90.625 (96.033)\n",
            "Epoch: [53][650/782]\tTime 0.022 (0.024)\tLoss (Class) 1.0201 (0.9789)\tLoss (Regu) 0.1879 (0.1878)\tPrec@1 78.125 (75.751)\tPrec@5 100.000 (95.994)\n",
            "Epoch: [53][700/782]\tTime 0.021 (0.024)\tLoss (Class) 1.0076 (0.9797)\tLoss (Regu) 0.1871 (0.1877)\tPrec@1 71.875 (75.673)\tPrec@5 96.875 (95.979)\n",
            "Epoch: [53][750/782]\tTime 0.023 (0.024)\tLoss (Class) 1.0853 (0.9817)\tLoss (Regu) 0.1887 (0.1876)\tPrec@1 70.312 (75.608)\tPrec@5 96.875 (95.920)\n",
            "Test: [0/157]\tTime 0.110 (0.110)\tLoss (Class) 1.8094 (1.8094)\tLoss (Regu) 0.2073 (0.2073)\tPrec@1 46.875 (46.875)\tPrec@5 93.750 (93.750)\n",
            "Test: [50/157]\tTime 0.007 (0.010)\tLoss (Class) 1.5933 (1.6847)\tLoss (Regu) 0.2112 (0.2084)\tPrec@1 65.625 (61.826)\tPrec@5 87.500 (87.806)\n",
            "Test: [100/157]\tTime 0.008 (0.009)\tLoss (Class) 1.4995 (1.6496)\tLoss (Regu) 0.2113 (0.2081)\tPrec@1 70.312 (61.989)\tPrec@5 87.500 (88.475)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.7745 (1.6655)\tLoss (Regu) 0.2083 (0.2079)\tPrec@1 59.375 (62.014)\tPrec@5 84.375 (88.028)\n",
            " * Train[75.484 %, 95.882 %, 0.986 loss] Val [62.060 %, 87.970%, 1.665 loss] Best: 63.520 %\n",
            "Time for 53 / 150 20.21481990814209\n",
            "Learning rate:  0.03\n",
            "Epoch: [54][0/782]\tTime 0.151 (0.151)\tLoss (Class) 0.9978 (0.9978)\tLoss (Regu) 0.1837 (0.1837)\tPrec@1 68.750 (68.750)\tPrec@5 96.875 (96.875)\n",
            "Epoch: [54][50/782]\tTime 0.023 (0.025)\tLoss (Class) 1.1979 (0.9564)\tLoss (Regu) 0.1865 (0.1874)\tPrec@1 65.625 (76.532)\tPrec@5 93.750 (96.078)\n",
            "Epoch: [54][100/782]\tTime 0.031 (0.024)\tLoss (Class) 1.0977 (0.9489)\tLoss (Regu) 0.1881 (0.1877)\tPrec@1 71.875 (76.655)\tPrec@5 96.875 (96.287)\n",
            "Epoch: [54][150/782]\tTime 0.021 (0.025)\tLoss (Class) 1.1557 (0.9568)\tLoss (Regu) 0.1862 (0.1880)\tPrec@1 70.312 (76.231)\tPrec@5 95.312 (96.296)\n",
            "Epoch: [54][200/782]\tTime 0.022 (0.025)\tLoss (Class) 0.8530 (0.9517)\tLoss (Regu) 0.1895 (0.1887)\tPrec@1 82.812 (76.415)\tPrec@5 98.438 (96.377)\n",
            "Epoch: [54][250/782]\tTime 0.022 (0.025)\tLoss (Class) 0.9226 (0.9528)\tLoss (Regu) 0.1892 (0.1889)\tPrec@1 79.688 (76.388)\tPrec@5 95.312 (96.290)\n",
            "Epoch: [54][300/782]\tTime 0.022 (0.025)\tLoss (Class) 0.9221 (0.9568)\tLoss (Regu) 0.1876 (0.1889)\tPrec@1 76.562 (76.282)\tPrec@5 96.875 (96.159)\n",
            "Epoch: [54][350/782]\tTime 0.023 (0.024)\tLoss (Class) 0.9899 (0.9573)\tLoss (Regu) 0.1885 (0.1887)\tPrec@1 67.188 (76.331)\tPrec@5 96.875 (96.203)\n",
            "Epoch: [54][400/782]\tTime 0.023 (0.024)\tLoss (Class) 0.9231 (0.9601)\tLoss (Regu) 0.1895 (0.1886)\tPrec@1 70.312 (76.290)\tPrec@5 98.438 (96.162)\n",
            "Epoch: [54][450/782]\tTime 0.022 (0.024)\tLoss (Class) 0.9434 (0.9651)\tLoss (Regu) 0.1877 (0.1883)\tPrec@1 78.125 (76.084)\tPrec@5 96.875 (96.144)\n",
            "Epoch: [54][500/782]\tTime 0.022 (0.024)\tLoss (Class) 1.0417 (0.9659)\tLoss (Regu) 0.1861 (0.1882)\tPrec@1 70.312 (76.120)\tPrec@5 95.312 (96.123)\n",
            "Epoch: [54][550/782]\tTime 0.023 (0.024)\tLoss (Class) 0.9092 (0.9655)\tLoss (Regu) 0.1831 (0.1882)\tPrec@1 79.688 (76.148)\tPrec@5 93.750 (96.095)\n",
            "Epoch: [54][600/782]\tTime 0.024 (0.024)\tLoss (Class) 1.1303 (0.9655)\tLoss (Regu) 0.1879 (0.1881)\tPrec@1 70.312 (76.178)\tPrec@5 93.750 (96.051)\n",
            "Epoch: [54][650/782]\tTime 0.022 (0.024)\tLoss (Class) 0.8943 (0.9695)\tLoss (Regu) 0.1858 (0.1879)\tPrec@1 75.000 (76.008)\tPrec@5 98.438 (96.004)\n",
            "Epoch: [54][700/782]\tTime 0.021 (0.024)\tLoss (Class) 1.0684 (0.9716)\tLoss (Regu) 0.1852 (0.1877)\tPrec@1 75.000 (75.979)\tPrec@5 96.875 (95.966)\n",
            "Epoch: [54][750/782]\tTime 0.022 (0.024)\tLoss (Class) 0.9784 (0.9744)\tLoss (Regu) 0.1875 (0.1876)\tPrec@1 78.125 (75.843)\tPrec@5 96.875 (95.905)\n",
            "Test: [0/157]\tTime 0.119 (0.119)\tLoss (Class) 1.2970 (1.2970)\tLoss (Regu) 0.2158 (0.2158)\tPrec@1 68.750 (68.750)\tPrec@5 93.750 (93.750)\n",
            "Test: [50/157]\tTime 0.009 (0.012)\tLoss (Class) 1.6617 (1.7928)\tLoss (Regu) 0.2095 (0.2103)\tPrec@1 64.062 (60.141)\tPrec@5 89.062 (87.102)\n",
            "Test: [100/157]\tTime 0.009 (0.010)\tLoss (Class) 2.1535 (1.7853)\tLoss (Regu) 0.2117 (0.2103)\tPrec@1 54.688 (60.257)\tPrec@5 87.500 (87.191)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.6879 (1.7753)\tLoss (Regu) 0.2113 (0.2105)\tPrec@1 62.500 (60.213)\tPrec@5 85.938 (87.159)\n",
            " * Train[75.758 %, 95.924 %, 0.976 loss] Val [60.190 %, 87.150%, 1.776 loss] Best: 63.520 %\n",
            "Time for 54 / 150 20.00441837310791\n",
            "Learning rate:  0.03\n",
            "Epoch: [55][0/782]\tTime 0.147 (0.147)\tLoss (Class) 0.9210 (0.9210)\tLoss (Regu) 0.1865 (0.1865)\tPrec@1 76.562 (76.562)\tPrec@5 98.438 (98.438)\n",
            "Epoch: [55][50/782]\tTime 0.022 (0.025)\tLoss (Class) 0.8742 (0.9210)\tLoss (Regu) 0.1886 (0.1877)\tPrec@1 73.438 (77.482)\tPrec@5 98.438 (96.538)\n",
            "Epoch: [55][100/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8824 (0.9137)\tLoss (Regu) 0.1867 (0.1876)\tPrec@1 81.250 (78.032)\tPrec@5 96.875 (96.612)\n",
            "Epoch: [55][150/782]\tTime 0.023 (0.023)\tLoss (Class) 0.7252 (0.9118)\tLoss (Regu) 0.1850 (0.1877)\tPrec@1 85.938 (78.166)\tPrec@5 98.438 (96.523)\n",
            "Epoch: [55][200/782]\tTime 0.022 (0.023)\tLoss (Class) 0.9693 (0.9196)\tLoss (Regu) 0.1874 (0.1877)\tPrec@1 75.000 (77.799)\tPrec@5 96.875 (96.471)\n",
            "Epoch: [55][250/782]\tTime 0.023 (0.023)\tLoss (Class) 1.0473 (0.9261)\tLoss (Regu) 0.1873 (0.1875)\tPrec@1 76.562 (77.552)\tPrec@5 93.750 (96.421)\n",
            "Epoch: [55][300/782]\tTime 0.022 (0.023)\tLoss (Class) 0.9369 (0.9314)\tLoss (Regu) 0.1853 (0.1872)\tPrec@1 81.250 (77.305)\tPrec@5 96.875 (96.434)\n",
            "Epoch: [55][350/782]\tTime 0.024 (0.023)\tLoss (Class) 1.0452 (0.9332)\tLoss (Regu) 0.1881 (0.1872)\tPrec@1 76.562 (77.239)\tPrec@5 95.312 (96.399)\n",
            "Epoch: [55][400/782]\tTime 0.023 (0.023)\tLoss (Class) 1.2699 (0.9353)\tLoss (Regu) 0.1880 (0.1873)\tPrec@1 68.750 (77.225)\tPrec@5 92.188 (96.329)\n",
            "Epoch: [55][450/782]\tTime 0.023 (0.023)\tLoss (Class) 0.8331 (0.9365)\tLoss (Regu) 0.1849 (0.1872)\tPrec@1 78.125 (77.131)\tPrec@5 98.438 (96.279)\n",
            "Epoch: [55][500/782]\tTime 0.023 (0.023)\tLoss (Class) 1.1789 (0.9397)\tLoss (Regu) 0.1876 (0.1871)\tPrec@1 71.875 (77.062)\tPrec@5 92.188 (96.248)\n",
            "Epoch: [55][550/782]\tTime 0.029 (0.023)\tLoss (Class) 0.7369 (0.9427)\tLoss (Regu) 0.1893 (0.1870)\tPrec@1 84.375 (76.914)\tPrec@5 100.000 (96.237)\n",
            "Epoch: [55][600/782]\tTime 0.023 (0.023)\tLoss (Class) 1.0634 (0.9450)\tLoss (Regu) 0.1867 (0.1870)\tPrec@1 75.000 (76.864)\tPrec@5 90.625 (96.215)\n",
            "Epoch: [55][650/782]\tTime 0.024 (0.024)\tLoss (Class) 0.8244 (0.9466)\tLoss (Regu) 0.1878 (0.1869)\tPrec@1 79.688 (76.743)\tPrec@5 96.875 (96.169)\n",
            "Epoch: [55][700/782]\tTime 0.025 (0.024)\tLoss (Class) 0.9668 (0.9506)\tLoss (Regu) 0.1869 (0.1868)\tPrec@1 70.312 (76.692)\tPrec@5 95.312 (96.084)\n",
            "Epoch: [55][750/782]\tTime 0.030 (0.024)\tLoss (Class) 1.4417 (0.9539)\tLoss (Regu) 0.1831 (0.1867)\tPrec@1 67.188 (76.579)\tPrec@5 92.188 (96.037)\n",
            "Test: [0/157]\tTime 0.115 (0.115)\tLoss (Class) 1.4947 (1.4947)\tLoss (Regu) 0.2115 (0.2115)\tPrec@1 65.625 (65.625)\tPrec@5 92.188 (92.188)\n",
            "Test: [50/157]\tTime 0.008 (0.011)\tLoss (Class) 1.4387 (1.7683)\tLoss (Regu) 0.2066 (0.2100)\tPrec@1 62.500 (60.968)\tPrec@5 93.750 (87.806)\n",
            "Test: [100/157]\tTime 0.009 (0.010)\tLoss (Class) 1.6394 (1.6995)\tLoss (Regu) 0.2054 (0.2097)\tPrec@1 62.500 (62.608)\tPrec@5 90.625 (88.274)\n",
            "Test: [150/157]\tTime 0.007 (0.010)\tLoss (Class) 1.8772 (1.6703)\tLoss (Regu) 0.2128 (0.2097)\tPrec@1 60.938 (63.017)\tPrec@5 85.938 (88.380)\n",
            " * Train[76.532 %, 96.016 %, 0.956 loss] Val [62.970 %, 88.330%, 1.673 loss] Best: 63.520 %\n",
            "Time for 55 / 150 20.177104711532593\n",
            "Learning rate:  0.03\n",
            "Epoch: [56][0/782]\tTime 0.154 (0.154)\tLoss (Class) 1.0776 (1.0776)\tLoss (Regu) 0.1827 (0.1827)\tPrec@1 73.438 (73.438)\tPrec@5 93.750 (93.750)\n",
            "Epoch: [56][50/782]\tTime 0.030 (0.027)\tLoss (Class) 1.0522 (0.9396)\tLoss (Regu) 0.1854 (0.1861)\tPrec@1 70.312 (76.808)\tPrec@5 93.750 (95.956)\n",
            "Epoch: [56][100/782]\tTime 0.022 (0.026)\tLoss (Class) 0.7235 (0.9065)\tLoss (Regu) 0.1888 (0.1868)\tPrec@1 82.812 (77.661)\tPrec@5 98.438 (96.457)\n",
            "Epoch: [56][150/782]\tTime 0.033 (0.025)\tLoss (Class) 1.1017 (0.9165)\tLoss (Regu) 0.1849 (0.1872)\tPrec@1 75.000 (77.452)\tPrec@5 95.312 (96.337)\n",
            "Epoch: [56][200/782]\tTime 0.024 (0.025)\tLoss (Class) 1.2018 (0.9191)\tLoss (Regu) 0.1883 (0.1869)\tPrec@1 67.188 (77.363)\tPrec@5 93.750 (96.479)\n",
            "Epoch: [56][250/782]\tTime 0.030 (0.025)\tLoss (Class) 1.0467 (0.9242)\tLoss (Regu) 0.1862 (0.1868)\tPrec@1 76.562 (77.067)\tPrec@5 92.188 (96.433)\n",
            "Epoch: [56][300/782]\tTime 0.021 (0.025)\tLoss (Class) 1.1786 (0.9253)\tLoss (Regu) 0.1839 (0.1868)\tPrec@1 73.438 (77.004)\tPrec@5 93.750 (96.418)\n",
            "Epoch: [56][350/782]\tTime 0.022 (0.025)\tLoss (Class) 0.7943 (0.9269)\tLoss (Regu) 0.1841 (0.1866)\tPrec@1 84.375 (76.976)\tPrec@5 96.875 (96.385)\n",
            "Epoch: [56][400/782]\tTime 0.023 (0.025)\tLoss (Class) 0.9163 (0.9309)\tLoss (Regu) 0.1894 (0.1863)\tPrec@1 81.250 (76.976)\tPrec@5 96.875 (96.337)\n",
            "Epoch: [56][450/782]\tTime 0.022 (0.025)\tLoss (Class) 1.0766 (0.9331)\tLoss (Regu) 0.1814 (0.1864)\tPrec@1 70.312 (76.937)\tPrec@5 93.750 (96.310)\n",
            "Epoch: [56][500/782]\tTime 0.023 (0.025)\tLoss (Class) 1.1246 (0.9378)\tLoss (Regu) 0.1884 (0.1864)\tPrec@1 75.000 (76.803)\tPrec@5 95.312 (96.267)\n",
            "Epoch: [56][550/782]\tTime 0.023 (0.025)\tLoss (Class) 0.7357 (0.9419)\tLoss (Regu) 0.1869 (0.1862)\tPrec@1 89.062 (76.560)\tPrec@5 98.438 (96.277)\n",
            "Epoch: [56][600/782]\tTime 0.024 (0.025)\tLoss (Class) 0.8040 (0.9430)\tLoss (Regu) 0.1876 (0.1862)\tPrec@1 79.688 (76.479)\tPrec@5 96.875 (96.313)\n",
            "Epoch: [56][650/782]\tTime 0.022 (0.024)\tLoss (Class) 1.1391 (0.9472)\tLoss (Regu) 0.1822 (0.1862)\tPrec@1 71.875 (76.380)\tPrec@5 98.438 (96.241)\n",
            "Epoch: [56][700/782]\tTime 0.023 (0.024)\tLoss (Class) 1.0374 (0.9510)\tLoss (Regu) 0.1832 (0.1860)\tPrec@1 68.750 (76.246)\tPrec@5 95.312 (96.200)\n",
            "Epoch: [56][750/782]\tTime 0.022 (0.024)\tLoss (Class) 0.7565 (0.9543)\tLoss (Regu) 0.1861 (0.1860)\tPrec@1 87.500 (76.128)\tPrec@5 95.312 (96.151)\n",
            "Test: [0/157]\tTime 0.112 (0.112)\tLoss (Class) 1.8204 (1.8204)\tLoss (Regu) 0.2126 (0.2126)\tPrec@1 56.250 (56.250)\tPrec@5 84.375 (84.375)\n",
            "Test: [50/157]\tTime 0.008 (0.013)\tLoss (Class) 1.5669 (1.6846)\tLoss (Regu) 0.2144 (0.2141)\tPrec@1 64.062 (63.174)\tPrec@5 87.500 (87.714)\n",
            "Test: [100/157]\tTime 0.008 (0.011)\tLoss (Class) 1.8121 (1.6832)\tLoss (Regu) 0.2123 (0.2143)\tPrec@1 59.375 (63.444)\tPrec@5 87.500 (87.593)\n",
            "Test: [150/157]\tTime 0.007 (0.010)\tLoss (Class) 1.6327 (1.6626)\tLoss (Regu) 0.2161 (0.2147)\tPrec@1 60.938 (63.390)\tPrec@5 89.062 (88.131)\n",
            " * Train[76.092 %, 96.136 %, 0.955 loss] Val [63.460 %, 88.200%, 1.658 loss] Best: 63.520 %\n",
            "Time for 56 / 150 20.569838047027588\n",
            "Learning rate:  0.03\n",
            "Epoch: [57][0/782]\tTime 0.151 (0.151)\tLoss (Class) 0.9389 (0.9389)\tLoss (Regu) 0.1848 (0.1848)\tPrec@1 79.688 (79.688)\tPrec@5 96.875 (96.875)\n",
            "Epoch: [57][50/782]\tTime 0.022 (0.027)\tLoss (Class) 0.8711 (0.9032)\tLoss (Regu) 0.1861 (0.1876)\tPrec@1 75.000 (78.554)\tPrec@5 98.438 (96.385)\n",
            "Epoch: [57][100/782]\tTime 0.022 (0.026)\tLoss (Class) 0.8198 (0.9004)\tLoss (Regu) 0.1915 (0.1877)\tPrec@1 82.812 (78.280)\tPrec@5 93.750 (96.643)\n",
            "Epoch: [57][150/782]\tTime 0.023 (0.026)\tLoss (Class) 0.8789 (0.9043)\tLoss (Regu) 0.1870 (0.1879)\tPrec@1 78.125 (78.301)\tPrec@5 95.312 (96.585)\n",
            "Epoch: [57][200/782]\tTime 0.024 (0.025)\tLoss (Class) 0.8644 (0.9105)\tLoss (Regu) 0.1885 (0.1886)\tPrec@1 79.688 (78.063)\tPrec@5 100.000 (96.502)\n",
            "Epoch: [57][250/782]\tTime 0.023 (0.025)\tLoss (Class) 0.7949 (0.9107)\tLoss (Regu) 0.1905 (0.1891)\tPrec@1 78.125 (77.988)\tPrec@5 98.438 (96.545)\n",
            "Epoch: [57][300/782]\tTime 0.023 (0.025)\tLoss (Class) 1.2146 (0.9199)\tLoss (Regu) 0.1899 (0.1894)\tPrec@1 73.438 (77.814)\tPrec@5 93.750 (96.460)\n",
            "Epoch: [57][350/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8938 (0.9220)\tLoss (Regu) 0.1872 (0.1892)\tPrec@1 82.812 (77.684)\tPrec@5 96.875 (96.416)\n",
            "Epoch: [57][400/782]\tTime 0.021 (0.024)\tLoss (Class) 0.9197 (0.9255)\tLoss (Regu) 0.1886 (0.1890)\tPrec@1 84.375 (77.595)\tPrec@5 95.312 (96.388)\n",
            "Epoch: [57][450/782]\tTime 0.032 (0.024)\tLoss (Class) 0.9475 (0.9290)\tLoss (Regu) 0.1895 (0.1889)\tPrec@1 76.562 (77.446)\tPrec@5 96.875 (96.324)\n",
            "Epoch: [57][500/782]\tTime 0.025 (0.024)\tLoss (Class) 0.8055 (0.9373)\tLoss (Regu) 0.1884 (0.1887)\tPrec@1 81.250 (77.196)\tPrec@5 96.875 (96.208)\n",
            "Epoch: [57][550/782]\tTime 0.023 (0.024)\tLoss (Class) 0.9810 (0.9410)\tLoss (Regu) 0.1817 (0.1885)\tPrec@1 73.438 (77.045)\tPrec@5 96.875 (96.129)\n",
            "Epoch: [57][600/782]\tTime 0.024 (0.024)\tLoss (Class) 1.0621 (0.9451)\tLoss (Regu) 0.1850 (0.1884)\tPrec@1 76.562 (76.942)\tPrec@5 93.750 (96.077)\n",
            "Epoch: [57][650/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8991 (0.9470)\tLoss (Regu) 0.1869 (0.1882)\tPrec@1 75.000 (76.875)\tPrec@5 100.000 (96.073)\n",
            "Epoch: [57][700/782]\tTime 0.023 (0.024)\tLoss (Class) 0.9960 (0.9544)\tLoss (Regu) 0.1852 (0.1879)\tPrec@1 75.000 (76.605)\tPrec@5 93.750 (96.037)\n",
            "Epoch: [57][750/782]\tTime 0.023 (0.024)\tLoss (Class) 0.9635 (0.9570)\tLoss (Regu) 0.1892 (0.1879)\tPrec@1 78.125 (76.515)\tPrec@5 93.750 (96.020)\n",
            "Test: [0/157]\tTime 0.113 (0.113)\tLoss (Class) 1.7073 (1.7073)\tLoss (Regu) 0.2139 (0.2139)\tPrec@1 64.062 (64.062)\tPrec@5 92.188 (92.188)\n",
            "Test: [50/157]\tTime 0.008 (0.010)\tLoss (Class) 1.7910 (1.7000)\tLoss (Regu) 0.2107 (0.2114)\tPrec@1 56.250 (61.520)\tPrec@5 89.062 (88.817)\n",
            "Test: [100/157]\tTime 0.008 (0.009)\tLoss (Class) 2.2023 (1.7312)\tLoss (Regu) 0.2070 (0.2108)\tPrec@1 57.812 (61.665)\tPrec@5 79.688 (88.041)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 2.3262 (1.7396)\tLoss (Regu) 0.2113 (0.2108)\tPrec@1 53.125 (61.651)\tPrec@5 85.938 (87.914)\n",
            " * Train[76.508 %, 96.026 %, 0.957 loss] Val [61.580 %, 87.850%, 1.743 loss] Best: 63.520 %\n",
            "Time for 57 / 150 20.063522577285767\n",
            "Learning rate:  0.03\n",
            "Epoch: [58][0/782]\tTime 0.147 (0.147)\tLoss (Class) 0.9469 (0.9469)\tLoss (Regu) 0.1850 (0.1850)\tPrec@1 78.125 (78.125)\tPrec@5 98.438 (98.438)\n",
            "Epoch: [58][50/782]\tTime 0.022 (0.026)\tLoss (Class) 0.9149 (0.8896)\tLoss (Regu) 0.1887 (0.1883)\tPrec@1 84.375 (78.738)\tPrec@5 95.312 (96.814)\n",
            "Epoch: [58][100/782]\tTime 0.023 (0.025)\tLoss (Class) 0.9315 (0.8997)\tLoss (Regu) 0.1882 (0.1874)\tPrec@1 79.688 (78.125)\tPrec@5 95.312 (96.674)\n",
            "Epoch: [58][150/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7724 (0.9030)\tLoss (Regu) 0.1890 (0.1878)\tPrec@1 79.688 (77.928)\tPrec@5 100.000 (96.616)\n",
            "Epoch: [58][200/782]\tTime 0.022 (0.024)\tLoss (Class) 0.8610 (0.9143)\tLoss (Regu) 0.1880 (0.1877)\tPrec@1 73.438 (77.589)\tPrec@5 98.438 (96.463)\n",
            "Epoch: [58][250/782]\tTime 0.022 (0.024)\tLoss (Class) 1.0264 (0.9169)\tLoss (Regu) 0.1851 (0.1876)\tPrec@1 76.562 (77.546)\tPrec@5 93.750 (96.489)\n",
            "Epoch: [58][300/782]\tTime 0.022 (0.024)\tLoss (Class) 0.9140 (0.9227)\tLoss (Regu) 0.1865 (0.1875)\tPrec@1 68.750 (77.320)\tPrec@5 98.438 (96.434)\n",
            "Epoch: [58][350/782]\tTime 0.023 (0.024)\tLoss (Class) 1.1682 (0.9274)\tLoss (Regu) 0.1820 (0.1874)\tPrec@1 73.438 (77.186)\tPrec@5 93.750 (96.385)\n",
            "Epoch: [58][400/782]\tTime 0.023 (0.023)\tLoss (Class) 0.8165 (0.9284)\tLoss (Regu) 0.1882 (0.1874)\tPrec@1 82.812 (77.159)\tPrec@5 98.438 (96.400)\n",
            "Epoch: [58][450/782]\tTime 0.023 (0.023)\tLoss (Class) 0.9134 (0.9323)\tLoss (Regu) 0.1799 (0.1873)\tPrec@1 84.375 (77.054)\tPrec@5 95.312 (96.338)\n",
            "Epoch: [58][500/782]\tTime 0.023 (0.023)\tLoss (Class) 0.8050 (0.9321)\tLoss (Regu) 0.1882 (0.1874)\tPrec@1 82.812 (77.068)\tPrec@5 96.875 (96.339)\n",
            "Epoch: [58][550/782]\tTime 0.023 (0.023)\tLoss (Class) 0.7465 (0.9343)\tLoss (Regu) 0.1867 (0.1875)\tPrec@1 81.250 (76.968)\tPrec@5 96.875 (96.336)\n",
            "Epoch: [58][600/782]\tTime 0.032 (0.023)\tLoss (Class) 0.9010 (0.9347)\tLoss (Regu) 0.1857 (0.1874)\tPrec@1 71.875 (76.963)\tPrec@5 98.438 (96.337)\n",
            "Epoch: [58][650/782]\tTime 0.023 (0.023)\tLoss (Class) 0.9242 (0.9390)\tLoss (Regu) 0.1887 (0.1874)\tPrec@1 75.000 (76.884)\tPrec@5 95.312 (96.294)\n",
            "Epoch: [58][700/782]\tTime 0.023 (0.023)\tLoss (Class) 1.2781 (0.9438)\tLoss (Regu) 0.1844 (0.1872)\tPrec@1 71.875 (76.756)\tPrec@5 92.188 (96.238)\n",
            "Epoch: [58][750/782]\tTime 0.022 (0.023)\tLoss (Class) 0.8982 (0.9473)\tLoss (Regu) 0.1894 (0.1873)\tPrec@1 76.562 (76.596)\tPrec@5 96.875 (96.226)\n",
            "Test: [0/157]\tTime 0.104 (0.104)\tLoss (Class) 2.1085 (2.1085)\tLoss (Regu) 0.2126 (0.2126)\tPrec@1 56.250 (56.250)\tPrec@5 89.062 (89.062)\n",
            "Test: [50/157]\tTime 0.008 (0.010)\tLoss (Class) 2.1525 (1.7994)\tLoss (Regu) 0.2193 (0.2132)\tPrec@1 56.250 (60.233)\tPrec@5 79.688 (87.040)\n",
            "Test: [100/157]\tTime 0.008 (0.009)\tLoss (Class) 1.5590 (1.7535)\tLoss (Regu) 0.2139 (0.2132)\tPrec@1 67.188 (61.665)\tPrec@5 82.812 (87.515)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.7353 (1.7485)\tLoss (Regu) 0.2173 (0.2131)\tPrec@1 68.750 (62.200)\tPrec@5 82.812 (87.490)\n",
            " * Train[76.496 %, 96.218 %, 0.949 loss] Val [62.160 %, 87.570%, 1.740 loss] Best: 63.520 %\n",
            "Time for 58 / 150 19.760313749313354\n",
            "Learning rate:  0.03\n",
            "Epoch: [59][0/782]\tTime 0.142 (0.142)\tLoss (Class) 0.7608 (0.7608)\tLoss (Regu) 0.1863 (0.1863)\tPrec@1 82.812 (82.812)\tPrec@5 96.875 (96.875)\n",
            "Epoch: [59][50/782]\tTime 0.022 (0.025)\tLoss (Class) 0.8451 (0.9078)\tLoss (Regu) 0.1876 (0.1895)\tPrec@1 78.125 (76.930)\tPrec@5 96.875 (96.752)\n",
            "Epoch: [59][100/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7864 (0.9245)\tLoss (Regu) 0.1896 (0.1890)\tPrec@1 81.250 (77.135)\tPrec@5 98.438 (96.473)\n",
            "Epoch: [59][150/782]\tTime 0.022 (0.024)\tLoss (Class) 0.7593 (0.9094)\tLoss (Regu) 0.1866 (0.1885)\tPrec@1 79.688 (77.877)\tPrec@5 95.312 (96.658)\n",
            "Epoch: [59][200/782]\tTime 0.033 (0.024)\tLoss (Class) 0.9988 (0.9082)\tLoss (Regu) 0.1873 (0.1885)\tPrec@1 71.875 (77.729)\tPrec@5 96.875 (96.727)\n",
            "Epoch: [59][250/782]\tTime 0.022 (0.024)\tLoss (Class) 1.0886 (0.9101)\tLoss (Regu) 0.1891 (0.1883)\tPrec@1 68.750 (77.677)\tPrec@5 100.000 (96.663)\n",
            "Epoch: [59][300/782]\tTime 0.024 (0.024)\tLoss (Class) 0.8797 (0.9108)\tLoss (Regu) 0.1840 (0.1880)\tPrec@1 73.438 (77.606)\tPrec@5 98.438 (96.704)\n",
            "Epoch: [59][350/782]\tTime 0.023 (0.025)\tLoss (Class) 0.8556 (0.9116)\tLoss (Regu) 0.1857 (0.1880)\tPrec@1 81.250 (77.546)\tPrec@5 96.875 (96.697)\n",
            "Epoch: [59][400/782]\tTime 0.023 (0.025)\tLoss (Class) 0.9470 (0.9134)\tLoss (Regu) 0.1870 (0.1881)\tPrec@1 79.688 (77.568)\tPrec@5 95.312 (96.618)\n",
            "Epoch: [59][450/782]\tTime 0.023 (0.025)\tLoss (Class) 1.0219 (0.9171)\tLoss (Regu) 0.1864 (0.1880)\tPrec@1 75.000 (77.477)\tPrec@5 93.750 (96.549)\n",
            "Epoch: [59][500/782]\tTime 0.023 (0.025)\tLoss (Class) 1.0016 (0.9202)\tLoss (Regu) 0.1904 (0.1882)\tPrec@1 75.000 (77.420)\tPrec@5 95.312 (96.491)\n",
            "Epoch: [59][550/782]\tTime 0.025 (0.025)\tLoss (Class) 1.3025 (0.9219)\tLoss (Regu) 0.1832 (0.1882)\tPrec@1 62.500 (77.379)\tPrec@5 95.312 (96.461)\n",
            "Epoch: [59][600/782]\tTime 0.024 (0.025)\tLoss (Class) 1.1950 (0.9260)\tLoss (Regu) 0.1855 (0.1882)\tPrec@1 62.500 (77.215)\tPrec@5 93.750 (96.436)\n",
            "Epoch: [59][650/782]\tTime 0.022 (0.025)\tLoss (Class) 1.1860 (0.9331)\tLoss (Regu) 0.1879 (0.1881)\tPrec@1 67.188 (76.966)\tPrec@5 90.625 (96.345)\n",
            "Epoch: [59][700/782]\tTime 0.023 (0.025)\tLoss (Class) 1.2517 (0.9352)\tLoss (Regu) 0.1885 (0.1881)\tPrec@1 70.312 (76.966)\tPrec@5 93.750 (96.316)\n",
            "Epoch: [59][750/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8681 (0.9393)\tLoss (Regu) 0.1861 (0.1880)\tPrec@1 81.250 (76.837)\tPrec@5 96.875 (96.286)\n",
            "Test: [0/157]\tTime 0.113 (0.113)\tLoss (Class) 2.4020 (2.4020)\tLoss (Regu) 0.2052 (0.2052)\tPrec@1 51.562 (51.562)\tPrec@5 78.125 (78.125)\n",
            "Test: [50/157]\tTime 0.008 (0.010)\tLoss (Class) 2.0784 (1.8992)\tLoss (Regu) 0.2030 (0.2093)\tPrec@1 64.062 (58.456)\tPrec@5 79.688 (85.509)\n",
            "Test: [100/157]\tTime 0.008 (0.009)\tLoss (Class) 1.7387 (1.8631)\tLoss (Regu) 0.2077 (0.2101)\tPrec@1 60.938 (59.189)\tPrec@5 87.500 (86.200)\n",
            "Test: [150/157]\tTime 0.007 (0.008)\tLoss (Class) 1.6054 (1.8317)\tLoss (Regu) 0.2128 (0.2099)\tPrec@1 67.188 (59.623)\tPrec@5 84.375 (86.310)\n",
            " * Train[76.748 %, 96.280 %, 0.941 loss] Val [59.600 %, 86.390%, 1.829 loss] Best: 63.520 %\n",
            "Time for 59 / 150 20.489771127700806\n",
            "Learning rate:  0.03\n",
            "Epoch: [60][0/782]\tTime 0.142 (0.142)\tLoss (Class) 0.8334 (0.8334)\tLoss (Regu) 0.1849 (0.1849)\tPrec@1 76.562 (76.562)\tPrec@5 98.438 (98.438)\n",
            "Epoch: [60][50/782]\tTime 0.023 (0.027)\tLoss (Class) 0.9264 (0.9063)\tLoss (Regu) 0.1856 (0.1884)\tPrec@1 73.438 (78.064)\tPrec@5 100.000 (96.569)\n",
            "Epoch: [60][100/782]\tTime 0.023 (0.025)\tLoss (Class) 1.0984 (0.9114)\tLoss (Regu) 0.1889 (0.1884)\tPrec@1 78.125 (78.125)\tPrec@5 93.750 (96.272)\n",
            "Epoch: [60][150/782]\tTime 0.022 (0.025)\tLoss (Class) 0.9024 (0.9047)\tLoss (Regu) 0.1883 (0.1885)\tPrec@1 82.812 (78.363)\tPrec@5 98.438 (96.420)\n",
            "Epoch: [60][200/782]\tTime 0.032 (0.025)\tLoss (Class) 0.8650 (0.8974)\tLoss (Regu) 0.1903 (0.1884)\tPrec@1 79.688 (78.389)\tPrec@5 98.438 (96.611)\n",
            "Epoch: [60][250/782]\tTime 0.022 (0.025)\tLoss (Class) 0.8186 (0.9025)\tLoss (Regu) 0.1861 (0.1884)\tPrec@1 82.812 (78.032)\tPrec@5 96.875 (96.589)\n",
            "Epoch: [60][300/782]\tTime 0.037 (0.025)\tLoss (Class) 0.9028 (0.9050)\tLoss (Regu) 0.1851 (0.1883)\tPrec@1 81.250 (77.845)\tPrec@5 95.312 (96.553)\n",
            "Epoch: [60][350/782]\tTime 0.022 (0.025)\tLoss (Class) 0.9210 (0.9038)\tLoss (Regu) 0.1855 (0.1884)\tPrec@1 78.125 (77.813)\tPrec@5 95.312 (96.572)\n",
            "Epoch: [60][400/782]\tTime 0.023 (0.025)\tLoss (Class) 1.0240 (0.9050)\tLoss (Regu) 0.1843 (0.1882)\tPrec@1 71.875 (77.681)\tPrec@5 100.000 (96.602)\n",
            "Epoch: [60][450/782]\tTime 0.023 (0.024)\tLoss (Class) 1.0749 (0.9089)\tLoss (Regu) 0.1902 (0.1881)\tPrec@1 76.562 (77.543)\tPrec@5 92.188 (96.549)\n",
            "Epoch: [60][500/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7804 (0.9109)\tLoss (Regu) 0.1874 (0.1880)\tPrec@1 82.812 (77.545)\tPrec@5 96.875 (96.513)\n",
            "Epoch: [60][550/782]\tTime 0.022 (0.024)\tLoss (Class) 1.1135 (0.9153)\tLoss (Regu) 0.1870 (0.1879)\tPrec@1 70.312 (77.416)\tPrec@5 92.188 (96.498)\n",
            "Epoch: [60][600/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8949 (0.9204)\tLoss (Regu) 0.1842 (0.1879)\tPrec@1 82.812 (77.322)\tPrec@5 96.875 (96.443)\n",
            "Epoch: [60][650/782]\tTime 0.022 (0.024)\tLoss (Class) 0.9268 (0.9260)\tLoss (Regu) 0.1847 (0.1878)\tPrec@1 76.562 (77.191)\tPrec@5 98.438 (96.393)\n",
            "Epoch: [60][700/782]\tTime 0.022 (0.024)\tLoss (Class) 0.8310 (0.9302)\tLoss (Regu) 0.1916 (0.1877)\tPrec@1 79.688 (77.073)\tPrec@5 96.875 (96.347)\n",
            "Epoch: [60][750/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8817 (0.9332)\tLoss (Regu) 0.1859 (0.1878)\tPrec@1 78.125 (76.952)\tPrec@5 98.438 (96.330)\n",
            "Test: [0/157]\tTime 0.110 (0.110)\tLoss (Class) 1.6043 (1.6043)\tLoss (Regu) 0.2145 (0.2145)\tPrec@1 64.062 (64.062)\tPrec@5 89.062 (89.062)\n",
            "Test: [50/157]\tTime 0.008 (0.010)\tLoss (Class) 2.3087 (1.6549)\tLoss (Regu) 0.2094 (0.2102)\tPrec@1 57.812 (63.787)\tPrec@5 79.688 (88.664)\n",
            "Test: [100/157]\tTime 0.009 (0.009)\tLoss (Class) 1.7853 (1.6680)\tLoss (Regu) 0.2132 (0.2108)\tPrec@1 60.938 (63.134)\tPrec@5 90.625 (88.738)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 2.0680 (1.6594)\tLoss (Regu) 0.2086 (0.2108)\tPrec@1 53.125 (63.121)\tPrec@5 87.500 (88.721)\n",
            " * Train[76.844 %, 96.322 %, 0.936 loss] Val [62.980 %, 88.670%, 1.667 loss] Best: 63.520 %\n",
            "Time for 60 / 150 20.011338233947754\n",
            "Learning rate:  0.03\n",
            "Epoch: [61][0/782]\tTime 0.151 (0.151)\tLoss (Class) 0.9779 (0.9779)\tLoss (Regu) 0.1883 (0.1883)\tPrec@1 75.000 (75.000)\tPrec@5 98.438 (98.438)\n",
            "Epoch: [61][50/782]\tTime 0.024 (0.029)\tLoss (Class) 1.0479 (0.9168)\tLoss (Regu) 0.1883 (0.1869)\tPrec@1 78.125 (77.757)\tPrec@5 93.750 (96.201)\n",
            "Epoch: [61][100/782]\tTime 0.022 (0.028)\tLoss (Class) 0.5395 (0.8831)\tLoss (Regu) 0.1923 (0.1879)\tPrec@1 87.500 (78.697)\tPrec@5 100.000 (96.720)\n",
            "Epoch: [61][150/782]\tTime 0.026 (0.027)\tLoss (Class) 0.9231 (0.8817)\tLoss (Regu) 0.1881 (0.1891)\tPrec@1 87.500 (78.787)\tPrec@5 96.875 (96.865)\n",
            "Epoch: [61][200/782]\tTime 0.030 (0.026)\tLoss (Class) 1.1588 (0.8910)\tLoss (Regu) 0.1873 (0.1884)\tPrec@1 64.062 (78.350)\tPrec@5 96.875 (96.821)\n",
            "Epoch: [61][250/782]\tTime 0.024 (0.026)\tLoss (Class) 0.7650 (0.8978)\tLoss (Regu) 0.1883 (0.1888)\tPrec@1 81.250 (78.094)\tPrec@5 96.875 (96.807)\n",
            "Epoch: [61][300/782]\tTime 0.022 (0.026)\tLoss (Class) 1.1243 (0.8966)\tLoss (Regu) 0.1844 (0.1883)\tPrec@1 73.438 (78.141)\tPrec@5 93.750 (96.813)\n",
            "Epoch: [61][350/782]\tTime 0.025 (0.026)\tLoss (Class) 0.9357 (0.9057)\tLoss (Regu) 0.1869 (0.1882)\tPrec@1 75.000 (77.836)\tPrec@5 95.312 (96.706)\n",
            "Epoch: [61][400/782]\tTime 0.022 (0.025)\tLoss (Class) 0.9237 (0.9131)\tLoss (Regu) 0.1842 (0.1879)\tPrec@1 76.562 (77.665)\tPrec@5 98.438 (96.614)\n",
            "Epoch: [61][450/782]\tTime 0.023 (0.025)\tLoss (Class) 0.9800 (0.9094)\tLoss (Regu) 0.1889 (0.1879)\tPrec@1 75.000 (77.751)\tPrec@5 92.188 (96.650)\n",
            "Epoch: [61][500/782]\tTime 0.022 (0.025)\tLoss (Class) 1.0374 (0.9128)\tLoss (Regu) 0.1865 (0.1878)\tPrec@1 76.562 (77.601)\tPrec@5 93.750 (96.597)\n",
            "Epoch: [61][550/782]\tTime 0.030 (0.025)\tLoss (Class) 1.0815 (0.9143)\tLoss (Regu) 0.1898 (0.1878)\tPrec@1 68.750 (77.498)\tPrec@5 96.875 (96.580)\n",
            "Epoch: [61][600/782]\tTime 0.022 (0.025)\tLoss (Class) 0.9217 (0.9170)\tLoss (Regu) 0.1845 (0.1876)\tPrec@1 79.688 (77.470)\tPrec@5 96.875 (96.506)\n",
            "Epoch: [61][650/782]\tTime 0.022 (0.025)\tLoss (Class) 0.9494 (0.9196)\tLoss (Regu) 0.1844 (0.1875)\tPrec@1 76.562 (77.398)\tPrec@5 98.438 (96.431)\n",
            "Epoch: [61][700/782]\tTime 0.023 (0.025)\tLoss (Class) 1.2689 (0.9224)\tLoss (Regu) 0.1857 (0.1873)\tPrec@1 68.750 (77.327)\tPrec@5 95.312 (96.423)\n",
            "Epoch: [61][750/782]\tTime 0.023 (0.025)\tLoss (Class) 0.9539 (0.9276)\tLoss (Regu) 0.1818 (0.1871)\tPrec@1 68.750 (77.114)\tPrec@5 95.312 (96.363)\n",
            "Test: [0/157]\tTime 0.113 (0.113)\tLoss (Class) 1.5433 (1.5433)\tLoss (Regu) 0.2088 (0.2088)\tPrec@1 57.812 (57.812)\tPrec@5 89.062 (89.062)\n",
            "Test: [50/157]\tTime 0.014 (0.013)\tLoss (Class) 1.5643 (1.7106)\tLoss (Regu) 0.2113 (0.2080)\tPrec@1 71.875 (62.071)\tPrec@5 87.500 (87.684)\n",
            "Test: [100/157]\tTime 0.009 (0.012)\tLoss (Class) 1.8351 (1.6798)\tLoss (Regu) 0.2030 (0.2075)\tPrec@1 62.500 (62.469)\tPrec@5 92.188 (88.181)\n",
            "Test: [150/157]\tTime 0.007 (0.011)\tLoss (Class) 1.7988 (1.6829)\tLoss (Regu) 0.2032 (0.2076)\tPrec@1 60.938 (62.459)\tPrec@5 90.625 (88.028)\n",
            " * Train[77.060 %, 96.326 %, 0.929 loss] Val [62.390 %, 88.070%, 1.682 loss] Best: 63.520 %\n",
            "Time for 61 / 150 20.832889556884766\n",
            "Learning rate:  0.03\n",
            "Epoch: [62][0/782]\tTime 0.146 (0.146)\tLoss (Class) 0.8371 (0.8371)\tLoss (Regu) 0.1850 (0.1850)\tPrec@1 82.812 (82.812)\tPrec@5 95.312 (95.312)\n",
            "Epoch: [62][50/782]\tTime 0.026 (0.027)\tLoss (Class) 0.9832 (0.8859)\tLoss (Regu) 0.1918 (0.1870)\tPrec@1 71.875 (77.788)\tPrec@5 95.312 (96.783)\n",
            "Epoch: [62][100/782]\tTime 0.023 (0.026)\tLoss (Class) 0.9603 (0.8742)\tLoss (Regu) 0.1828 (0.1874)\tPrec@1 79.688 (78.605)\tPrec@5 96.875 (96.875)\n",
            "Epoch: [62][150/782]\tTime 0.023 (0.025)\tLoss (Class) 0.8967 (0.8826)\tLoss (Regu) 0.1892 (0.1870)\tPrec@1 82.812 (78.177)\tPrec@5 96.875 (96.834)\n",
            "Epoch: [62][200/782]\tTime 0.022 (0.025)\tLoss (Class) 0.7539 (0.8870)\tLoss (Regu) 0.1893 (0.1871)\tPrec@1 81.250 (78.141)\tPrec@5 100.000 (96.828)\n",
            "Epoch: [62][250/782]\tTime 0.023 (0.025)\tLoss (Class) 0.8474 (0.8904)\tLoss (Regu) 0.1915 (0.1868)\tPrec@1 79.688 (78.181)\tPrec@5 98.438 (96.757)\n",
            "Epoch: [62][300/782]\tTime 0.022 (0.024)\tLoss (Class) 0.8715 (0.8922)\tLoss (Regu) 0.1908 (0.1870)\tPrec@1 78.125 (78.130)\tPrec@5 98.438 (96.688)\n",
            "Epoch: [62][350/782]\tTime 0.023 (0.024)\tLoss (Class) 1.0549 (0.8975)\tLoss (Regu) 0.1806 (0.1868)\tPrec@1 67.188 (78.005)\tPrec@5 96.875 (96.679)\n",
            "Epoch: [62][400/782]\tTime 0.022 (0.024)\tLoss (Class) 0.8831 (0.9019)\tLoss (Regu) 0.1885 (0.1867)\tPrec@1 78.125 (77.837)\tPrec@5 98.438 (96.661)\n",
            "Epoch: [62][450/782]\tTime 0.022 (0.024)\tLoss (Class) 0.7343 (0.9085)\tLoss (Regu) 0.1862 (0.1866)\tPrec@1 85.938 (77.727)\tPrec@5 95.312 (96.622)\n",
            "Epoch: [62][500/782]\tTime 0.023 (0.024)\tLoss (Class) 0.9629 (0.9097)\tLoss (Regu) 0.1884 (0.1867)\tPrec@1 73.438 (77.691)\tPrec@5 98.438 (96.654)\n",
            "Epoch: [62][550/782]\tTime 0.023 (0.024)\tLoss (Class) 0.9009 (0.9101)\tLoss (Regu) 0.1867 (0.1867)\tPrec@1 76.562 (77.680)\tPrec@5 100.000 (96.662)\n",
            "Epoch: [62][600/782]\tTime 0.026 (0.024)\tLoss (Class) 0.9397 (0.9131)\tLoss (Regu) 0.1854 (0.1866)\tPrec@1 76.562 (77.597)\tPrec@5 98.438 (96.633)\n",
            "Epoch: [62][650/782]\tTime 0.023 (0.024)\tLoss (Class) 1.0716 (0.9179)\tLoss (Regu) 0.1820 (0.1867)\tPrec@1 68.750 (77.448)\tPrec@5 95.312 (96.558)\n",
            "Epoch: [62][700/782]\tTime 0.023 (0.024)\tLoss (Class) 1.0168 (0.9187)\tLoss (Regu) 0.1950 (0.1869)\tPrec@1 81.250 (77.492)\tPrec@5 93.750 (96.536)\n",
            "Epoch: [62][750/782]\tTime 0.022 (0.024)\tLoss (Class) 0.9253 (0.9191)\tLoss (Regu) 0.1863 (0.1870)\tPrec@1 81.250 (77.459)\tPrec@5 95.312 (96.557)\n",
            "Test: [0/157]\tTime 0.120 (0.120)\tLoss (Class) 1.6716 (1.6716)\tLoss (Regu) 0.2051 (0.2051)\tPrec@1 67.188 (67.188)\tPrec@5 85.938 (85.938)\n",
            "Test: [50/157]\tTime 0.007 (0.011)\tLoss (Class) 1.6454 (1.6786)\tLoss (Regu) 0.1988 (0.2052)\tPrec@1 75.000 (63.174)\tPrec@5 87.500 (87.990)\n",
            "Test: [100/157]\tTime 0.008 (0.011)\tLoss (Class) 1.7374 (1.6603)\tLoss (Regu) 0.2100 (0.2051)\tPrec@1 62.500 (62.670)\tPrec@5 87.500 (88.212)\n",
            "Test: [150/157]\tTime 0.007 (0.010)\tLoss (Class) 1.8553 (1.6508)\tLoss (Regu) 0.2056 (0.2053)\tPrec@1 54.688 (62.810)\tPrec@5 90.625 (88.545)\n",
            " * Train[77.402 %, 96.520 %, 0.921 loss] Val [62.810 %, 88.590%, 1.648 loss] Best: 63.520 %\n",
            "Time for 62 / 150 20.34177803993225\n",
            "Learning rate:  0.03\n",
            "Epoch: [63][0/782]\tTime 0.153 (0.153)\tLoss (Class) 0.7167 (0.7167)\tLoss (Regu) 0.1826 (0.1826)\tPrec@1 82.812 (82.812)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [63][50/782]\tTime 0.023 (0.026)\tLoss (Class) 0.9546 (0.8905)\tLoss (Regu) 0.1888 (0.1870)\tPrec@1 73.438 (78.799)\tPrec@5 100.000 (97.059)\n",
            "Epoch: [63][100/782]\tTime 0.023 (0.025)\tLoss (Class) 0.7777 (0.8711)\tLoss (Regu) 0.1893 (0.1880)\tPrec@1 79.688 (79.301)\tPrec@5 98.438 (97.231)\n",
            "Epoch: [63][150/782]\tTime 0.022 (0.024)\tLoss (Class) 0.7342 (0.8723)\tLoss (Regu) 0.1912 (0.1876)\tPrec@1 84.375 (79.056)\tPrec@5 100.000 (97.227)\n",
            "Epoch: [63][200/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7862 (0.8712)\tLoss (Regu) 0.1896 (0.1876)\tPrec@1 85.938 (79.112)\tPrec@5 98.438 (97.132)\n",
            "Epoch: [63][250/782]\tTime 0.023 (0.024)\tLoss (Class) 0.9094 (0.8730)\tLoss (Regu) 0.1909 (0.1881)\tPrec@1 75.000 (79.015)\tPrec@5 96.875 (97.049)\n",
            "Epoch: [63][300/782]\tTime 0.023 (0.023)\tLoss (Class) 0.9871 (0.8808)\tLoss (Regu) 0.1882 (0.1883)\tPrec@1 73.438 (78.831)\tPrec@5 95.312 (96.989)\n",
            "Epoch: [63][350/782]\tTime 0.023 (0.024)\tLoss (Class) 1.0570 (0.8846)\tLoss (Regu) 0.1911 (0.1881)\tPrec@1 76.562 (78.739)\tPrec@5 95.312 (96.946)\n",
            "Epoch: [63][400/782]\tTime 0.024 (0.024)\tLoss (Class) 0.8990 (0.8908)\tLoss (Regu) 0.1907 (0.1880)\tPrec@1 78.125 (78.522)\tPrec@5 98.438 (96.863)\n",
            "Epoch: [63][450/782]\tTime 0.023 (0.024)\tLoss (Class) 1.0127 (0.8953)\tLoss (Regu) 0.1885 (0.1879)\tPrec@1 78.125 (78.347)\tPrec@5 96.875 (96.799)\n",
            "Epoch: [63][500/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8900 (0.8993)\tLoss (Regu) 0.1900 (0.1879)\tPrec@1 81.250 (78.212)\tPrec@5 93.750 (96.747)\n",
            "Epoch: [63][550/782]\tTime 0.024 (0.024)\tLoss (Class) 0.9792 (0.9033)\tLoss (Regu) 0.1841 (0.1879)\tPrec@1 73.438 (78.051)\tPrec@5 96.875 (96.711)\n",
            "Epoch: [63][600/782]\tTime 0.030 (0.024)\tLoss (Class) 0.9591 (0.9067)\tLoss (Regu) 0.1868 (0.1879)\tPrec@1 78.125 (77.961)\tPrec@5 98.438 (96.688)\n",
            "Epoch: [63][650/782]\tTime 0.032 (0.024)\tLoss (Class) 1.0382 (0.9101)\tLoss (Regu) 0.1870 (0.1879)\tPrec@1 71.875 (77.779)\tPrec@5 95.312 (96.659)\n",
            "Epoch: [63][700/782]\tTime 0.024 (0.024)\tLoss (Class) 0.8260 (0.9150)\tLoss (Regu) 0.1859 (0.1879)\tPrec@1 81.250 (77.639)\tPrec@5 96.875 (96.634)\n",
            "Epoch: [63][750/782]\tTime 0.024 (0.024)\tLoss (Class) 0.9371 (0.9169)\tLoss (Regu) 0.1871 (0.1877)\tPrec@1 81.250 (77.592)\tPrec@5 98.438 (96.609)\n",
            "Test: [0/157]\tTime 0.119 (0.119)\tLoss (Class) 1.4720 (1.4720)\tLoss (Regu) 0.2068 (0.2068)\tPrec@1 70.312 (70.312)\tPrec@5 89.062 (89.062)\n",
            "Test: [50/157]\tTime 0.008 (0.013)\tLoss (Class) 1.6496 (1.6778)\tLoss (Regu) 0.2056 (0.2084)\tPrec@1 56.250 (62.960)\tPrec@5 89.062 (88.205)\n",
            "Test: [100/157]\tTime 0.008 (0.012)\tLoss (Class) 1.4924 (1.6795)\tLoss (Regu) 0.2109 (0.2085)\tPrec@1 67.188 (62.392)\tPrec@5 89.062 (88.366)\n",
            "Test: [150/157]\tTime 0.007 (0.011)\tLoss (Class) 1.4478 (1.6636)\tLoss (Regu) 0.2087 (0.2084)\tPrec@1 67.188 (62.883)\tPrec@5 85.938 (88.535)\n",
            " * Train[77.588 %, 96.600 %, 0.917 loss] Val [62.890 %, 88.550%, 1.664 loss] Best: 63.520 %\n",
            "Time for 63 / 150 20.71269202232361\n",
            "Learning rate:  0.03\n",
            "Epoch: [64][0/782]\tTime 0.159 (0.159)\tLoss (Class) 0.9014 (0.9014)\tLoss (Regu) 0.1883 (0.1883)\tPrec@1 79.688 (79.688)\tPrec@5 96.875 (96.875)\n",
            "Epoch: [64][50/782]\tTime 0.024 (0.028)\tLoss (Class) 0.7486 (0.8574)\tLoss (Regu) 0.1894 (0.1893)\tPrec@1 76.562 (80.086)\tPrec@5 98.438 (96.844)\n",
            "Epoch: [64][100/782]\tTime 0.023 (0.027)\tLoss (Class) 1.0204 (0.8624)\tLoss (Regu) 0.1886 (0.1897)\tPrec@1 81.250 (79.842)\tPrec@5 95.312 (96.875)\n",
            "Epoch: [64][150/782]\tTime 0.023 (0.026)\tLoss (Class) 1.0512 (0.8674)\tLoss (Regu) 0.1914 (0.1901)\tPrec@1 71.875 (79.305)\tPrec@5 96.875 (97.030)\n",
            "Epoch: [64][200/782]\tTime 0.022 (0.025)\tLoss (Class) 0.6140 (0.8750)\tLoss (Regu) 0.1867 (0.1894)\tPrec@1 90.625 (79.112)\tPrec@5 98.438 (96.883)\n",
            "Epoch: [64][250/782]\tTime 0.023 (0.025)\tLoss (Class) 0.7414 (0.8802)\tLoss (Regu) 0.1897 (0.1890)\tPrec@1 84.375 (78.922)\tPrec@5 100.000 (96.850)\n",
            "Epoch: [64][300/782]\tTime 0.025 (0.025)\tLoss (Class) 0.7704 (0.8875)\tLoss (Regu) 0.1882 (0.1888)\tPrec@1 85.938 (78.696)\tPrec@5 98.438 (96.787)\n",
            "Epoch: [64][350/782]\tTime 0.022 (0.025)\tLoss (Class) 1.1391 (0.8901)\tLoss (Regu) 0.1895 (0.1888)\tPrec@1 71.875 (78.641)\tPrec@5 95.312 (96.822)\n",
            "Epoch: [64][400/782]\tTime 0.022 (0.025)\tLoss (Class) 0.8060 (0.8895)\tLoss (Regu) 0.1874 (0.1889)\tPrec@1 84.375 (78.612)\tPrec@5 96.875 (96.840)\n",
            "Epoch: [64][450/782]\tTime 0.030 (0.025)\tLoss (Class) 0.8385 (0.8931)\tLoss (Regu) 0.1870 (0.1886)\tPrec@1 71.875 (78.485)\tPrec@5 96.875 (96.764)\n",
            "Epoch: [64][500/782]\tTime 0.031 (0.025)\tLoss (Class) 0.9151 (0.8949)\tLoss (Regu) 0.1850 (0.1884)\tPrec@1 76.562 (78.396)\tPrec@5 95.312 (96.725)\n",
            "Epoch: [64][550/782]\tTime 0.023 (0.025)\tLoss (Class) 0.9416 (0.8978)\tLoss (Regu) 0.1867 (0.1885)\tPrec@1 82.812 (78.264)\tPrec@5 93.750 (96.696)\n",
            "Epoch: [64][600/782]\tTime 0.032 (0.025)\tLoss (Class) 0.8227 (0.9049)\tLoss (Regu) 0.1907 (0.1883)\tPrec@1 78.125 (77.985)\tPrec@5 100.000 (96.636)\n",
            "Epoch: [64][650/782]\tTime 0.023 (0.025)\tLoss (Class) 1.1489 (0.9072)\tLoss (Regu) 0.1832 (0.1880)\tPrec@1 71.875 (77.871)\tPrec@5 92.188 (96.623)\n",
            "Epoch: [64][700/782]\tTime 0.023 (0.025)\tLoss (Class) 0.7847 (0.9077)\tLoss (Regu) 0.1871 (0.1879)\tPrec@1 79.688 (77.842)\tPrec@5 96.875 (96.585)\n",
            "Epoch: [64][750/782]\tTime 0.024 (0.025)\tLoss (Class) 0.7742 (0.9127)\tLoss (Regu) 0.1872 (0.1878)\tPrec@1 82.812 (77.692)\tPrec@5 98.438 (96.536)\n",
            "Test: [0/157]\tTime 0.119 (0.119)\tLoss (Class) 1.9679 (1.9679)\tLoss (Regu) 0.2145 (0.2145)\tPrec@1 62.500 (62.500)\tPrec@5 81.250 (81.250)\n",
            "Test: [50/157]\tTime 0.008 (0.011)\tLoss (Class) 1.9900 (1.7819)\tLoss (Regu) 0.2202 (0.2160)\tPrec@1 53.125 (62.194)\tPrec@5 85.938 (87.163)\n",
            "Test: [100/157]\tTime 0.007 (0.009)\tLoss (Class) 1.3094 (1.7483)\tLoss (Regu) 0.2162 (0.2158)\tPrec@1 67.188 (62.314)\tPrec@5 90.625 (87.794)\n",
            "Test: [150/157]\tTime 0.008 (0.009)\tLoss (Class) 1.8026 (1.7367)\tLoss (Regu) 0.2193 (0.2160)\tPrec@1 59.375 (62.676)\tPrec@5 87.500 (87.779)\n",
            " * Train[77.674 %, 96.556 %, 0.913 loss] Val [62.570 %, 87.710%, 1.750 loss] Best: 63.520 %\n",
            "Time for 64 / 150 20.914827585220337\n",
            "Learning rate:  0.03\n",
            "Epoch: [65][0/782]\tTime 0.153 (0.153)\tLoss (Class) 0.9169 (0.9169)\tLoss (Regu) 0.1898 (0.1898)\tPrec@1 78.125 (78.125)\tPrec@5 96.875 (96.875)\n",
            "Epoch: [65][50/782]\tTime 0.022 (0.025)\tLoss (Class) 0.7055 (0.8700)\tLoss (Regu) 0.1863 (0.1899)\tPrec@1 84.375 (78.860)\tPrec@5 98.438 (97.273)\n",
            "Epoch: [65][100/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8892 (0.8635)\tLoss (Regu) 0.1887 (0.1894)\tPrec@1 79.688 (79.100)\tPrec@5 95.312 (97.231)\n",
            "Epoch: [65][150/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8248 (0.8670)\tLoss (Regu) 0.1870 (0.1893)\tPrec@1 78.125 (78.808)\tPrec@5 96.875 (97.279)\n",
            "Epoch: [65][200/782]\tTime 0.023 (0.023)\tLoss (Class) 0.8996 (0.8646)\tLoss (Regu) 0.1905 (0.1890)\tPrec@1 75.000 (79.081)\tPrec@5 96.875 (97.170)\n",
            "Epoch: [65][250/782]\tTime 0.023 (0.023)\tLoss (Class) 0.9219 (0.8633)\tLoss (Regu) 0.1905 (0.1888)\tPrec@1 79.688 (79.021)\tPrec@5 96.875 (97.224)\n",
            "Epoch: [65][300/782]\tTime 0.023 (0.023)\tLoss (Class) 0.9436 (0.8702)\tLoss (Regu) 0.1814 (0.1883)\tPrec@1 73.438 (78.774)\tPrec@5 98.438 (97.135)\n",
            "Epoch: [65][350/782]\tTime 0.023 (0.023)\tLoss (Class) 1.0052 (0.8759)\tLoss (Regu) 0.1850 (0.1881)\tPrec@1 68.750 (78.588)\tPrec@5 95.312 (97.071)\n",
            "Epoch: [65][400/782]\tTime 0.023 (0.023)\tLoss (Class) 0.7325 (0.8835)\tLoss (Regu) 0.1878 (0.1877)\tPrec@1 81.250 (78.273)\tPrec@5 100.000 (97.043)\n",
            "Epoch: [65][450/782]\tTime 0.023 (0.023)\tLoss (Class) 0.8101 (0.8848)\tLoss (Regu) 0.1867 (0.1877)\tPrec@1 81.250 (78.274)\tPrec@5 96.875 (97.003)\n",
            "Epoch: [65][500/782]\tTime 0.023 (0.023)\tLoss (Class) 0.8255 (0.8928)\tLoss (Regu) 0.1878 (0.1877)\tPrec@1 73.438 (78.038)\tPrec@5 100.000 (96.953)\n",
            "Epoch: [65][550/782]\tTime 0.022 (0.023)\tLoss (Class) 0.9387 (0.8949)\tLoss (Regu) 0.1839 (0.1877)\tPrec@1 73.438 (78.017)\tPrec@5 95.312 (96.903)\n",
            "Epoch: [65][600/782]\tTime 0.023 (0.023)\tLoss (Class) 0.7706 (0.8983)\tLoss (Regu) 0.1851 (0.1876)\tPrec@1 79.688 (77.865)\tPrec@5 100.000 (96.875)\n",
            "Epoch: [65][650/782]\tTime 0.023 (0.023)\tLoss (Class) 0.8918 (0.8994)\tLoss (Regu) 0.1912 (0.1875)\tPrec@1 76.562 (77.847)\tPrec@5 96.875 (96.829)\n",
            "Epoch: [65][700/782]\tTime 0.032 (0.023)\tLoss (Class) 1.0850 (0.9017)\tLoss (Regu) 0.1836 (0.1875)\tPrec@1 73.438 (77.777)\tPrec@5 95.312 (96.772)\n",
            "Epoch: [65][750/782]\tTime 0.023 (0.023)\tLoss (Class) 1.0160 (0.9051)\tLoss (Regu) 0.1848 (0.1874)\tPrec@1 71.875 (77.642)\tPrec@5 96.875 (96.744)\n",
            "Test: [0/157]\tTime 0.108 (0.108)\tLoss (Class) 1.8638 (1.8638)\tLoss (Regu) 0.2142 (0.2142)\tPrec@1 60.938 (60.938)\tPrec@5 84.375 (84.375)\n",
            "Test: [50/157]\tTime 0.008 (0.010)\tLoss (Class) 1.5677 (1.7184)\tLoss (Regu) 0.2100 (0.2139)\tPrec@1 70.312 (62.224)\tPrec@5 89.062 (88.450)\n",
            "Test: [100/157]\tTime 0.008 (0.009)\tLoss (Class) 1.4797 (1.6683)\tLoss (Regu) 0.2138 (0.2137)\tPrec@1 67.188 (63.243)\tPrec@5 87.500 (89.047)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.8447 (1.6677)\tLoss (Regu) 0.2125 (0.2139)\tPrec@1 53.125 (63.483)\tPrec@5 87.500 (88.742)\n",
            " * Train[77.636 %, 96.748 %, 0.906 loss] Val [63.610 %, 88.790%, 1.661 loss] Best: 63.610 %\n",
            "Time for 65 / 150 19.79341721534729\n",
            "Learning rate:  0.03\n",
            "Epoch: [66][0/782]\tTime 0.147 (0.147)\tLoss (Class) 0.9400 (0.9400)\tLoss (Regu) 0.1891 (0.1891)\tPrec@1 68.750 (68.750)\tPrec@5 98.438 (98.438)\n",
            "Epoch: [66][50/782]\tTime 0.022 (0.025)\tLoss (Class) 0.8907 (0.8622)\tLoss (Regu) 0.1889 (0.1902)\tPrec@1 81.250 (79.075)\tPrec@5 98.438 (97.304)\n",
            "Epoch: [66][100/782]\tTime 0.030 (0.024)\tLoss (Class) 0.7207 (0.8367)\tLoss (Regu) 0.1952 (0.1904)\tPrec@1 84.375 (79.672)\tPrec@5 95.312 (97.602)\n",
            "Epoch: [66][150/782]\tTime 0.023 (0.025)\tLoss (Class) 0.7419 (0.8485)\tLoss (Regu) 0.1917 (0.1905)\tPrec@1 79.688 (79.294)\tPrec@5 95.312 (97.351)\n",
            "Epoch: [66][200/782]\tTime 0.032 (0.025)\tLoss (Class) 0.8107 (0.8548)\tLoss (Regu) 0.1891 (0.1901)\tPrec@1 79.688 (79.299)\tPrec@5 98.438 (97.225)\n",
            "Epoch: [66][250/782]\tTime 0.022 (0.025)\tLoss (Class) 1.0925 (0.8604)\tLoss (Regu) 0.1854 (0.1895)\tPrec@1 70.312 (79.221)\tPrec@5 95.312 (97.093)\n",
            "Epoch: [66][300/782]\tTime 0.033 (0.025)\tLoss (Class) 0.7214 (0.8652)\tLoss (Regu) 0.1882 (0.1889)\tPrec@1 85.938 (79.116)\tPrec@5 100.000 (96.989)\n",
            "Epoch: [66][350/782]\tTime 0.022 (0.025)\tLoss (Class) 0.8634 (0.8709)\tLoss (Regu) 0.1850 (0.1886)\tPrec@1 79.688 (78.926)\tPrec@5 95.312 (96.902)\n",
            "Epoch: [66][400/782]\tTime 0.022 (0.025)\tLoss (Class) 0.7632 (0.8757)\tLoss (Regu) 0.1865 (0.1885)\tPrec@1 87.500 (78.834)\tPrec@5 95.312 (96.824)\n",
            "Epoch: [66][450/782]\tTime 0.031 (0.024)\tLoss (Class) 0.9327 (0.8800)\tLoss (Regu) 0.1907 (0.1882)\tPrec@1 71.875 (78.721)\tPrec@5 96.875 (96.778)\n",
            "Epoch: [66][500/782]\tTime 0.023 (0.024)\tLoss (Class) 0.9630 (0.8839)\tLoss (Regu) 0.1870 (0.1882)\tPrec@1 78.125 (78.640)\tPrec@5 95.312 (96.753)\n",
            "Epoch: [66][550/782]\tTime 0.031 (0.024)\tLoss (Class) 0.8895 (0.8888)\tLoss (Regu) 0.1944 (0.1880)\tPrec@1 75.000 (78.468)\tPrec@5 100.000 (96.696)\n",
            "Epoch: [66][600/782]\tTime 0.023 (0.024)\tLoss (Class) 1.0434 (0.8915)\tLoss (Regu) 0.1874 (0.1882)\tPrec@1 75.000 (78.359)\tPrec@5 93.750 (96.683)\n",
            "Epoch: [66][650/782]\tTime 0.022 (0.024)\tLoss (Class) 1.0592 (0.8934)\tLoss (Regu) 0.1895 (0.1882)\tPrec@1 71.875 (78.305)\tPrec@5 92.188 (96.690)\n",
            "Epoch: [66][700/782]\tTime 0.025 (0.024)\tLoss (Class) 0.8202 (0.8966)\tLoss (Regu) 0.1849 (0.1882)\tPrec@1 75.000 (78.203)\tPrec@5 100.000 (96.665)\n",
            "Epoch: [66][750/782]\tTime 0.022 (0.024)\tLoss (Class) 1.1389 (0.9001)\tLoss (Regu) 0.1930 (0.1881)\tPrec@1 75.000 (78.100)\tPrec@5 96.875 (96.636)\n",
            "Test: [0/157]\tTime 0.108 (0.108)\tLoss (Class) 1.6481 (1.6481)\tLoss (Regu) 0.2118 (0.2118)\tPrec@1 59.375 (59.375)\tPrec@5 85.938 (85.938)\n",
            "Test: [50/157]\tTime 0.009 (0.010)\tLoss (Class) 1.5970 (1.7523)\tLoss (Regu) 0.2064 (0.2102)\tPrec@1 60.938 (61.673)\tPrec@5 90.625 (87.623)\n",
            "Test: [100/157]\tTime 0.008 (0.010)\tLoss (Class) 1.4454 (1.7344)\tLoss (Regu) 0.2117 (0.2102)\tPrec@1 71.875 (62.160)\tPrec@5 89.062 (87.918)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.3762 (1.7171)\tLoss (Regu) 0.2103 (0.2103)\tPrec@1 73.438 (62.769)\tPrec@5 89.062 (88.173)\n",
            " * Train[78.034 %, 96.604 %, 0.902 loss] Val [62.910 %, 88.180%, 1.715 loss] Best: 63.610 %\n",
            "Time for 66 / 150 20.35853147506714\n",
            "Learning rate:  0.03\n",
            "Epoch: [67][0/782]\tTime 0.143 (0.143)\tLoss (Class) 0.8932 (0.8932)\tLoss (Regu) 0.1865 (0.1865)\tPrec@1 73.438 (73.438)\tPrec@5 98.438 (98.438)\n",
            "Epoch: [67][50/782]\tTime 0.032 (0.027)\tLoss (Class) 0.8570 (0.8579)\tLoss (Regu) 0.1907 (0.1881)\tPrec@1 79.688 (79.075)\tPrec@5 95.312 (97.181)\n",
            "Epoch: [67][100/782]\tTime 0.022 (0.026)\tLoss (Class) 0.7002 (0.8455)\tLoss (Regu) 0.1879 (0.1887)\tPrec@1 85.938 (79.548)\tPrec@5 98.438 (97.355)\n",
            "Epoch: [67][150/782]\tTime 0.032 (0.026)\tLoss (Class) 0.7720 (0.8462)\tLoss (Regu) 0.1885 (0.1892)\tPrec@1 81.250 (79.439)\tPrec@5 100.000 (97.299)\n",
            "Epoch: [67][200/782]\tTime 0.023 (0.025)\tLoss (Class) 0.8320 (0.8432)\tLoss (Regu) 0.1875 (0.1890)\tPrec@1 81.250 (79.625)\tPrec@5 98.438 (97.303)\n",
            "Epoch: [67][250/782]\tTime 0.023 (0.026)\tLoss (Class) 0.6656 (0.8407)\tLoss (Regu) 0.1897 (0.1888)\tPrec@1 84.375 (79.744)\tPrec@5 98.438 (97.329)\n",
            "Epoch: [67][300/782]\tTime 0.023 (0.025)\tLoss (Class) 1.0474 (0.8502)\tLoss (Regu) 0.1808 (0.1885)\tPrec@1 75.000 (79.521)\tPrec@5 95.312 (97.254)\n",
            "Epoch: [67][350/782]\tTime 0.025 (0.025)\tLoss (Class) 0.8219 (0.8563)\tLoss (Regu) 0.1837 (0.1879)\tPrec@1 79.688 (79.398)\tPrec@5 93.750 (97.151)\n",
            "Epoch: [67][400/782]\tTime 0.023 (0.025)\tLoss (Class) 0.9719 (0.8649)\tLoss (Regu) 0.1868 (0.1879)\tPrec@1 68.750 (79.087)\tPrec@5 100.000 (97.070)\n",
            "Epoch: [67][450/782]\tTime 0.021 (0.025)\tLoss (Class) 0.8464 (0.8710)\tLoss (Regu) 0.1877 (0.1878)\tPrec@1 75.000 (78.936)\tPrec@5 98.438 (96.975)\n",
            "Epoch: [67][500/782]\tTime 0.025 (0.025)\tLoss (Class) 0.8293 (0.8710)\tLoss (Regu) 0.1928 (0.1879)\tPrec@1 75.000 (78.955)\tPrec@5 98.438 (96.987)\n",
            "Epoch: [67][550/782]\tTime 0.021 (0.025)\tLoss (Class) 0.9549 (0.8735)\tLoss (Regu) 0.1868 (0.1878)\tPrec@1 75.000 (78.882)\tPrec@5 98.438 (96.957)\n",
            "Epoch: [67][600/782]\tTime 0.022 (0.025)\tLoss (Class) 0.9092 (0.8757)\tLoss (Regu) 0.1857 (0.1878)\tPrec@1 71.875 (78.806)\tPrec@5 98.438 (96.958)\n",
            "Epoch: [67][650/782]\tTime 0.022 (0.025)\tLoss (Class) 1.0355 (0.8777)\tLoss (Regu) 0.1875 (0.1877)\tPrec@1 73.438 (78.713)\tPrec@5 92.188 (96.916)\n",
            "Epoch: [67][700/782]\tTime 0.024 (0.025)\tLoss (Class) 1.0043 (0.8855)\tLoss (Regu) 0.1819 (0.1875)\tPrec@1 73.438 (78.488)\tPrec@5 100.000 (96.813)\n",
            "Epoch: [67][750/782]\tTime 0.030 (0.025)\tLoss (Class) 1.0132 (0.8894)\tLoss (Regu) 0.1853 (0.1876)\tPrec@1 75.000 (78.325)\tPrec@5 93.750 (96.796)\n",
            "Test: [0/157]\tTime 0.111 (0.111)\tLoss (Class) 2.0835 (2.0835)\tLoss (Regu) 0.2049 (0.2049)\tPrec@1 56.250 (56.250)\tPrec@5 78.125 (78.125)\n",
            "Test: [50/157]\tTime 0.014 (0.011)\tLoss (Class) 1.9380 (1.8963)\tLoss (Regu) 0.2091 (0.2086)\tPrec@1 53.125 (59.252)\tPrec@5 90.625 (85.784)\n",
            "Test: [100/157]\tTime 0.008 (0.010)\tLoss (Class) 2.0091 (1.8884)\tLoss (Regu) 0.2099 (0.2090)\tPrec@1 51.562 (59.174)\tPrec@5 82.812 (85.582)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.7906 (1.8566)\tLoss (Regu) 0.2079 (0.2091)\tPrec@1 60.938 (59.427)\tPrec@5 85.938 (85.917)\n",
            " * Train[78.310 %, 96.764 %, 0.891 loss] Val [59.480 %, 85.810%, 1.858 loss] Best: 63.610 %\n",
            "Time for 67 / 150 21.05907893180847\n",
            "Learning rate:  0.03\n",
            "Epoch: [68][0/782]\tTime 0.143 (0.143)\tLoss (Class) 0.7807 (0.7807)\tLoss (Regu) 0.1876 (0.1876)\tPrec@1 84.375 (84.375)\tPrec@5 95.312 (95.312)\n",
            "Epoch: [68][50/782]\tTime 0.022 (0.025)\tLoss (Class) 0.8067 (0.8857)\tLoss (Regu) 0.1887 (0.1883)\tPrec@1 84.375 (78.768)\tPrec@5 96.875 (96.415)\n",
            "Epoch: [68][100/782]\tTime 0.023 (0.024)\tLoss (Class) 0.9063 (0.8753)\tLoss (Regu) 0.1912 (0.1884)\tPrec@1 75.000 (78.543)\tPrec@5 95.312 (97.061)\n",
            "Epoch: [68][150/782]\tTime 0.022 (0.024)\tLoss (Class) 0.6813 (0.8685)\tLoss (Regu) 0.1903 (0.1889)\tPrec@1 84.375 (78.787)\tPrec@5 100.000 (97.072)\n",
            "Epoch: [68][200/782]\tTime 0.023 (0.023)\tLoss (Class) 0.9388 (0.8725)\tLoss (Regu) 0.1864 (0.1886)\tPrec@1 73.438 (78.584)\tPrec@5 96.875 (97.093)\n",
            "Epoch: [68][250/782]\tTime 0.023 (0.023)\tLoss (Class) 1.1078 (0.8730)\tLoss (Regu) 0.1900 (0.1887)\tPrec@1 71.875 (78.492)\tPrec@5 93.750 (97.087)\n",
            "Epoch: [68][300/782]\tTime 0.023 (0.023)\tLoss (Class) 0.8841 (0.8733)\tLoss (Regu) 0.1892 (0.1888)\tPrec@1 78.125 (78.587)\tPrec@5 98.438 (97.098)\n",
            "Epoch: [68][350/782]\tTime 0.024 (0.023)\tLoss (Class) 0.7531 (0.8782)\tLoss (Regu) 0.1940 (0.1890)\tPrec@1 71.875 (78.450)\tPrec@5 100.000 (97.053)\n",
            "Epoch: [68][400/782]\tTime 0.032 (0.023)\tLoss (Class) 0.7920 (0.8835)\tLoss (Regu) 0.1932 (0.1891)\tPrec@1 81.250 (78.273)\tPrec@5 98.438 (97.027)\n",
            "Epoch: [68][450/782]\tTime 0.022 (0.024)\tLoss (Class) 0.7126 (0.8832)\tLoss (Regu) 0.1906 (0.1889)\tPrec@1 82.812 (78.257)\tPrec@5 98.438 (97.014)\n",
            "Epoch: [68][500/782]\tTime 0.032 (0.024)\tLoss (Class) 0.9983 (0.8847)\tLoss (Regu) 0.1908 (0.1890)\tPrec@1 76.562 (78.250)\tPrec@5 98.438 (97.037)\n",
            "Epoch: [68][550/782]\tTime 0.023 (0.024)\tLoss (Class) 0.9620 (0.8869)\tLoss (Regu) 0.1877 (0.1890)\tPrec@1 73.438 (78.151)\tPrec@5 100.000 (97.017)\n",
            "Epoch: [68][600/782]\tTime 0.032 (0.024)\tLoss (Class) 0.7345 (0.8890)\tLoss (Regu) 0.1933 (0.1889)\tPrec@1 79.688 (78.042)\tPrec@5 96.875 (97.005)\n",
            "Epoch: [68][650/782]\tTime 0.023 (0.024)\tLoss (Class) 0.9571 (0.8910)\tLoss (Regu) 0.1879 (0.1889)\tPrec@1 78.125 (78.017)\tPrec@5 95.312 (96.954)\n",
            "Epoch: [68][700/782]\tTime 0.023 (0.024)\tLoss (Class) 0.9446 (0.8954)\tLoss (Regu) 0.1893 (0.1887)\tPrec@1 79.688 (77.915)\tPrec@5 96.875 (96.926)\n",
            "Epoch: [68][750/782]\tTime 0.022 (0.024)\tLoss (Class) 0.9962 (0.8988)\tLoss (Regu) 0.1848 (0.1885)\tPrec@1 71.875 (77.848)\tPrec@5 96.875 (96.879)\n",
            "Test: [0/157]\tTime 0.121 (0.121)\tLoss (Class) 1.3283 (1.3283)\tLoss (Regu) 0.2046 (0.2046)\tPrec@1 68.750 (68.750)\tPrec@5 90.625 (90.625)\n",
            "Test: [50/157]\tTime 0.009 (0.010)\tLoss (Class) 1.4909 (1.5987)\tLoss (Regu) 0.2131 (0.2101)\tPrec@1 68.750 (64.001)\tPrec@5 93.750 (89.124)\n",
            "Test: [100/157]\tTime 0.008 (0.010)\tLoss (Class) 1.7119 (1.5808)\tLoss (Regu) 0.2104 (0.2102)\tPrec@1 64.062 (64.759)\tPrec@5 89.062 (88.892)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.6142 (1.5844)\tLoss (Regu) 0.2083 (0.2100)\tPrec@1 56.250 (64.538)\tPrec@5 84.375 (88.845)\n",
            " * Train[77.776 %, 96.824 %, 0.901 loss] Val [64.430 %, 88.800%, 1.586 loss] Best: 64.430 %\n",
            "Time for 68 / 150 20.241121768951416\n",
            "Learning rate:  0.03\n",
            "Epoch: [69][0/782]\tTime 0.157 (0.157)\tLoss (Class) 0.7665 (0.7665)\tLoss (Regu) 0.1869 (0.1869)\tPrec@1 75.000 (75.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [69][50/782]\tTime 0.022 (0.027)\tLoss (Class) 0.5275 (0.8073)\tLoss (Regu) 0.1943 (0.1892)\tPrec@1 92.188 (81.005)\tPrec@5 100.000 (97.518)\n",
            "Epoch: [69][100/782]\tTime 0.023 (0.026)\tLoss (Class) 0.9439 (0.8190)\tLoss (Regu) 0.1936 (0.1898)\tPrec@1 79.688 (80.801)\tPrec@5 93.750 (97.432)\n",
            "Epoch: [69][150/782]\tTime 0.023 (0.025)\tLoss (Class) 0.7887 (0.8369)\tLoss (Regu) 0.1915 (0.1895)\tPrec@1 82.812 (80.070)\tPrec@5 95.312 (97.361)\n",
            "Epoch: [69][200/782]\tTime 0.022 (0.024)\tLoss (Class) 1.0309 (0.8416)\tLoss (Regu) 0.1900 (0.1891)\tPrec@1 73.438 (79.866)\tPrec@5 96.875 (97.373)\n",
            "Epoch: [69][250/782]\tTime 0.023 (0.024)\tLoss (Class) 0.9830 (0.8373)\tLoss (Regu) 0.1895 (0.1893)\tPrec@1 73.438 (79.937)\tPrec@5 96.875 (97.473)\n",
            "Epoch: [69][300/782]\tTime 0.022 (0.024)\tLoss (Class) 0.6210 (0.8437)\tLoss (Regu) 0.1927 (0.1890)\tPrec@1 85.938 (79.750)\tPrec@5 98.438 (97.353)\n",
            "Epoch: [69][350/782]\tTime 0.022 (0.024)\tLoss (Class) 1.0120 (0.8465)\tLoss (Regu) 0.1904 (0.1889)\tPrec@1 73.438 (79.656)\tPrec@5 96.875 (97.325)\n",
            "Epoch: [69][400/782]\tTime 0.022 (0.024)\tLoss (Class) 0.8254 (0.8495)\tLoss (Regu) 0.1908 (0.1888)\tPrec@1 82.812 (79.524)\tPrec@5 95.312 (97.343)\n",
            "Epoch: [69][450/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6766 (0.8561)\tLoss (Regu) 0.1888 (0.1888)\tPrec@1 85.938 (79.396)\tPrec@5 98.438 (97.232)\n",
            "Epoch: [69][500/782]\tTime 0.022 (0.024)\tLoss (Class) 0.8088 (0.8578)\tLoss (Regu) 0.1899 (0.1887)\tPrec@1 82.812 (79.304)\tPrec@5 98.438 (97.221)\n",
            "Epoch: [69][550/782]\tTime 0.021 (0.024)\tLoss (Class) 0.8725 (0.8661)\tLoss (Regu) 0.1869 (0.1888)\tPrec@1 76.562 (78.976)\tPrec@5 96.875 (97.127)\n",
            "Epoch: [69][600/782]\tTime 0.030 (0.024)\tLoss (Class) 1.0361 (0.8717)\tLoss (Regu) 0.1859 (0.1886)\tPrec@1 78.125 (78.853)\tPrec@5 92.188 (97.034)\n",
            "Epoch: [69][650/782]\tTime 0.023 (0.024)\tLoss (Class) 1.0317 (0.8757)\tLoss (Regu) 0.1860 (0.1885)\tPrec@1 71.875 (78.643)\tPrec@5 96.875 (97.007)\n",
            "Epoch: [69][700/782]\tTime 0.032 (0.024)\tLoss (Class) 1.0732 (0.8783)\tLoss (Regu) 0.1870 (0.1883)\tPrec@1 75.000 (78.562)\tPrec@5 96.875 (96.955)\n",
            "Epoch: [69][750/782]\tTime 0.025 (0.024)\tLoss (Class) 0.9036 (0.8817)\tLoss (Regu) 0.1898 (0.1885)\tPrec@1 78.125 (78.456)\tPrec@5 100.000 (96.925)\n",
            "Test: [0/157]\tTime 0.112 (0.112)\tLoss (Class) 2.1815 (2.1815)\tLoss (Regu) 0.2124 (0.2124)\tPrec@1 48.438 (48.438)\tPrec@5 85.938 (85.938)\n",
            "Test: [50/157]\tTime 0.008 (0.010)\tLoss (Class) 2.3000 (1.7923)\tLoss (Regu) 0.2142 (0.2133)\tPrec@1 57.812 (61.458)\tPrec@5 81.250 (86.857)\n",
            "Test: [100/157]\tTime 0.007 (0.009)\tLoss (Class) 1.5000 (1.7375)\tLoss (Regu) 0.2101 (0.2134)\tPrec@1 65.625 (62.624)\tPrec@5 95.312 (87.686)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 2.0424 (1.7204)\tLoss (Regu) 0.2078 (0.2133)\tPrec@1 57.812 (62.986)\tPrec@5 84.375 (87.831)\n",
            " * Train[78.392 %, 96.912 %, 0.885 loss] Val [63.180 %, 87.900%, 1.711 loss] Best: 64.430 %\n",
            "Time for 69 / 150 19.979084968566895\n",
            "Learning rate:  0.03\n",
            "Epoch: [70][0/782]\tTime 0.147 (0.147)\tLoss (Class) 0.9771 (0.9771)\tLoss (Regu) 0.1900 (0.1900)\tPrec@1 73.438 (73.438)\tPrec@5 96.875 (96.875)\n",
            "Epoch: [70][50/782]\tTime 0.023 (0.025)\tLoss (Class) 0.8159 (0.8649)\tLoss (Regu) 0.1921 (0.1886)\tPrec@1 81.250 (78.676)\tPrec@5 95.312 (96.599)\n",
            "Epoch: [70][100/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7396 (0.8372)\tLoss (Regu) 0.1889 (0.1897)\tPrec@1 81.250 (79.718)\tPrec@5 96.875 (96.890)\n",
            "Epoch: [70][150/782]\tTime 0.022 (0.024)\tLoss (Class) 0.6609 (0.8249)\tLoss (Regu) 0.1901 (0.1902)\tPrec@1 81.250 (79.936)\tPrec@5 100.000 (97.206)\n",
            "Epoch: [70][200/782]\tTime 0.023 (0.024)\tLoss (Class) 0.9433 (0.8294)\tLoss (Regu) 0.1924 (0.1897)\tPrec@1 76.562 (79.952)\tPrec@5 98.438 (97.240)\n",
            "Epoch: [70][250/782]\tTime 0.023 (0.023)\tLoss (Class) 0.9993 (0.8364)\tLoss (Regu) 0.1869 (0.1895)\tPrec@1 71.875 (79.831)\tPrec@5 95.312 (97.105)\n",
            "Epoch: [70][300/782]\tTime 0.023 (0.023)\tLoss (Class) 0.9441 (0.8446)\tLoss (Regu) 0.1912 (0.1895)\tPrec@1 79.688 (79.610)\tPrec@5 96.875 (97.015)\n",
            "Epoch: [70][350/782]\tTime 0.023 (0.023)\tLoss (Class) 0.8229 (0.8484)\tLoss (Regu) 0.1827 (0.1895)\tPrec@1 82.812 (79.576)\tPrec@5 96.875 (96.991)\n",
            "Epoch: [70][400/782]\tTime 0.022 (0.023)\tLoss (Class) 0.6680 (0.8494)\tLoss (Regu) 0.1883 (0.1892)\tPrec@1 85.938 (79.547)\tPrec@5 98.438 (97.019)\n",
            "Epoch: [70][450/782]\tTime 0.022 (0.023)\tLoss (Class) 0.9517 (0.8548)\tLoss (Regu) 0.1889 (0.1891)\tPrec@1 78.125 (79.286)\tPrec@5 96.875 (97.038)\n",
            "Epoch: [70][500/782]\tTime 0.022 (0.023)\tLoss (Class) 0.8830 (0.8632)\tLoss (Regu) 0.1846 (0.1888)\tPrec@1 75.000 (79.089)\tPrec@5 98.438 (96.975)\n",
            "Epoch: [70][550/782]\tTime 0.023 (0.023)\tLoss (Class) 0.8988 (0.8630)\tLoss (Regu) 0.1909 (0.1889)\tPrec@1 78.125 (79.075)\tPrec@5 98.438 (97.005)\n",
            "Epoch: [70][600/782]\tTime 0.022 (0.023)\tLoss (Class) 1.0291 (0.8682)\tLoss (Regu) 0.1882 (0.1890)\tPrec@1 79.688 (78.843)\tPrec@5 93.750 (96.966)\n",
            "Epoch: [70][650/782]\tTime 0.023 (0.023)\tLoss (Class) 0.8044 (0.8723)\tLoss (Regu) 0.1923 (0.1890)\tPrec@1 82.812 (78.711)\tPrec@5 96.875 (96.957)\n",
            "Epoch: [70][700/782]\tTime 0.023 (0.023)\tLoss (Class) 0.7295 (0.8767)\tLoss (Regu) 0.1905 (0.1890)\tPrec@1 85.938 (78.580)\tPrec@5 98.438 (96.908)\n",
            "Epoch: [70][750/782]\tTime 0.023 (0.023)\tLoss (Class) 0.9210 (0.8794)\tLoss (Regu) 0.1804 (0.1889)\tPrec@1 73.438 (78.506)\tPrec@5 96.875 (96.877)\n",
            "Test: [0/157]\tTime 0.125 (0.125)\tLoss (Class) 1.7591 (1.7591)\tLoss (Regu) 0.2156 (0.2156)\tPrec@1 60.938 (60.938)\tPrec@5 87.500 (87.500)\n",
            "Test: [50/157]\tTime 0.009 (0.010)\tLoss (Class) 1.9077 (1.7788)\tLoss (Regu) 0.2055 (0.2098)\tPrec@1 54.688 (62.316)\tPrec@5 85.938 (87.040)\n",
            "Test: [100/157]\tTime 0.008 (0.009)\tLoss (Class) 1.7861 (1.7711)\tLoss (Regu) 0.2126 (0.2101)\tPrec@1 68.750 (62.454)\tPrec@5 87.500 (87.067)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.3231 (1.7605)\tLoss (Regu) 0.2151 (0.2100)\tPrec@1 68.750 (62.231)\tPrec@5 89.062 (87.169)\n",
            " * Train[78.434 %, 96.872 %, 0.882 loss] Val [62.110 %, 87.220%, 1.762 loss] Best: 64.430 %\n",
            "Time for 70 / 150 19.447931051254272\n",
            "Learning rate:  0.03\n",
            "Epoch: [71][0/782]\tTime 0.157 (0.157)\tLoss (Class) 0.9372 (0.9372)\tLoss (Regu) 0.1838 (0.1838)\tPrec@1 73.438 (73.438)\tPrec@5 96.875 (96.875)\n",
            "Epoch: [71][50/782]\tTime 0.021 (0.027)\tLoss (Class) 0.8583 (0.8486)\tLoss (Regu) 0.1894 (0.1863)\tPrec@1 71.875 (79.779)\tPrec@5 100.000 (97.089)\n",
            "Epoch: [71][100/782]\tTime 0.022 (0.026)\tLoss (Class) 0.8120 (0.8401)\tLoss (Regu) 0.1901 (0.1885)\tPrec@1 82.812 (79.842)\tPrec@5 95.312 (97.324)\n",
            "Epoch: [71][150/782]\tTime 0.023 (0.026)\tLoss (Class) 0.9459 (0.8487)\tLoss (Regu) 0.1857 (0.1884)\tPrec@1 78.125 (79.698)\tPrec@5 96.875 (97.175)\n",
            "Epoch: [71][200/782]\tTime 0.023 (0.026)\tLoss (Class) 0.9031 (0.8546)\tLoss (Regu) 0.1929 (0.1890)\tPrec@1 75.000 (79.594)\tPrec@5 96.875 (97.093)\n",
            "Epoch: [71][250/782]\tTime 0.028 (0.026)\tLoss (Class) 0.7895 (0.8543)\tLoss (Regu) 0.1886 (0.1891)\tPrec@1 82.812 (79.582)\tPrec@5 100.000 (97.024)\n",
            "Epoch: [71][300/782]\tTime 0.032 (0.026)\tLoss (Class) 0.9259 (0.8601)\tLoss (Regu) 0.1879 (0.1889)\tPrec@1 79.688 (79.283)\tPrec@5 98.438 (97.000)\n",
            "Epoch: [71][350/782]\tTime 0.023 (0.026)\tLoss (Class) 0.8848 (0.8599)\tLoss (Regu) 0.1869 (0.1889)\tPrec@1 85.938 (79.336)\tPrec@5 96.875 (97.017)\n",
            "Epoch: [71][400/782]\tTime 0.022 (0.026)\tLoss (Class) 0.7557 (0.8608)\tLoss (Regu) 0.1892 (0.1890)\tPrec@1 76.562 (79.161)\tPrec@5 100.000 (97.070)\n",
            "Epoch: [71][450/782]\tTime 0.026 (0.025)\tLoss (Class) 0.9017 (0.8638)\tLoss (Regu) 0.1877 (0.1889)\tPrec@1 75.000 (79.078)\tPrec@5 98.438 (97.031)\n",
            "Epoch: [71][500/782]\tTime 0.023 (0.025)\tLoss (Class) 1.1796 (0.8661)\tLoss (Regu) 0.1878 (0.1888)\tPrec@1 70.312 (79.023)\tPrec@5 93.750 (97.009)\n",
            "Epoch: [71][550/782]\tTime 0.022 (0.025)\tLoss (Class) 0.7238 (0.8681)\tLoss (Regu) 0.1908 (0.1887)\tPrec@1 81.250 (78.959)\tPrec@5 98.438 (97.022)\n",
            "Epoch: [71][600/782]\tTime 0.023 (0.025)\tLoss (Class) 0.9209 (0.8696)\tLoss (Regu) 0.1888 (0.1888)\tPrec@1 78.125 (78.874)\tPrec@5 98.438 (97.041)\n",
            "Epoch: [71][650/782]\tTime 0.022 (0.025)\tLoss (Class) 0.8111 (0.8721)\tLoss (Regu) 0.1856 (0.1887)\tPrec@1 81.250 (78.744)\tPrec@5 93.750 (96.976)\n",
            "Epoch: [71][700/782]\tTime 0.023 (0.025)\tLoss (Class) 1.1320 (0.8748)\tLoss (Regu) 0.1873 (0.1889)\tPrec@1 75.000 (78.682)\tPrec@5 90.625 (96.942)\n",
            "Epoch: [71][750/782]\tTime 0.023 (0.025)\tLoss (Class) 1.0388 (0.8778)\tLoss (Regu) 0.1890 (0.1888)\tPrec@1 73.438 (78.620)\tPrec@5 96.875 (96.921)\n",
            "Test: [0/157]\tTime 0.116 (0.116)\tLoss (Class) 2.0977 (2.0977)\tLoss (Regu) 0.2127 (0.2127)\tPrec@1 59.375 (59.375)\tPrec@5 82.812 (82.812)\n",
            "Test: [50/157]\tTime 0.008 (0.010)\tLoss (Class) 1.4460 (1.8326)\tLoss (Regu) 0.2096 (0.2151)\tPrec@1 65.625 (60.386)\tPrec@5 93.750 (87.439)\n",
            "Test: [100/157]\tTime 0.009 (0.009)\tLoss (Class) 1.6441 (1.7918)\tLoss (Regu) 0.2194 (0.2154)\tPrec@1 60.938 (61.742)\tPrec@5 89.062 (87.701)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.5839 (1.8029)\tLoss (Regu) 0.2196 (0.2155)\tPrec@1 60.938 (61.455)\tPrec@5 90.625 (87.593)\n",
            " * Train[78.532 %, 96.918 %, 0.880 loss] Val [61.360 %, 87.480%, 1.808 loss] Best: 64.430 %\n",
            "Time for 71 / 150 20.60436511039734\n",
            "Learning rate:  0.03\n",
            "Epoch: [72][0/782]\tTime 0.149 (0.149)\tLoss (Class) 0.7408 (0.7408)\tLoss (Regu) 0.1874 (0.1874)\tPrec@1 78.125 (78.125)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [72][50/782]\tTime 0.023 (0.025)\tLoss (Class) 0.7301 (0.8350)\tLoss (Regu) 0.1938 (0.1902)\tPrec@1 87.500 (79.994)\tPrec@5 96.875 (97.396)\n",
            "Epoch: [72][100/782]\tTime 0.022 (0.024)\tLoss (Class) 0.9462 (0.8210)\tLoss (Regu) 0.1927 (0.1908)\tPrec@1 78.125 (80.384)\tPrec@5 93.750 (97.355)\n",
            "Epoch: [72][150/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7815 (0.8239)\tLoss (Regu) 0.1909 (0.1908)\tPrec@1 79.688 (80.308)\tPrec@5 95.312 (97.310)\n",
            "Epoch: [72][200/782]\tTime 0.024 (0.024)\tLoss (Class) 0.8052 (0.8347)\tLoss (Regu) 0.1874 (0.1907)\tPrec@1 85.938 (80.006)\tPrec@5 93.750 (97.310)\n",
            "Epoch: [72][250/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6539 (0.8377)\tLoss (Regu) 0.1877 (0.1901)\tPrec@1 84.375 (79.937)\tPrec@5 100.000 (97.280)\n",
            "Epoch: [72][300/782]\tTime 0.030 (0.024)\tLoss (Class) 1.0314 (0.8354)\tLoss (Regu) 0.1900 (0.1898)\tPrec@1 68.750 (79.900)\tPrec@5 98.438 (97.327)\n",
            "Epoch: [72][350/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7797 (0.8385)\tLoss (Regu) 0.1871 (0.1896)\tPrec@1 76.562 (79.759)\tPrec@5 100.000 (97.396)\n",
            "Epoch: [72][400/782]\tTime 0.030 (0.024)\tLoss (Class) 1.0648 (0.8500)\tLoss (Regu) 0.1920 (0.1892)\tPrec@1 71.875 (79.286)\tPrec@5 93.750 (97.269)\n",
            "Epoch: [72][450/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8723 (0.8577)\tLoss (Regu) 0.1891 (0.1889)\tPrec@1 78.125 (79.015)\tPrec@5 95.312 (97.159)\n",
            "Epoch: [72][500/782]\tTime 0.030 (0.024)\tLoss (Class) 0.8536 (0.8602)\tLoss (Regu) 0.1896 (0.1890)\tPrec@1 81.250 (78.905)\tPrec@5 96.875 (97.146)\n",
            "Epoch: [72][550/782]\tTime 0.022 (0.024)\tLoss (Class) 0.9473 (0.8623)\tLoss (Regu) 0.1908 (0.1891)\tPrec@1 78.125 (78.902)\tPrec@5 98.438 (97.113)\n",
            "Epoch: [72][600/782]\tTime 0.029 (0.024)\tLoss (Class) 0.8639 (0.8659)\tLoss (Regu) 0.1863 (0.1889)\tPrec@1 76.562 (78.822)\tPrec@5 96.875 (97.088)\n",
            "Epoch: [72][650/782]\tTime 0.022 (0.024)\tLoss (Class) 0.8829 (0.8680)\tLoss (Regu) 0.1830 (0.1887)\tPrec@1 78.125 (78.778)\tPrec@5 95.312 (97.074)\n",
            "Epoch: [72][700/782]\tTime 0.023 (0.024)\tLoss (Class) 1.4083 (0.8730)\tLoss (Regu) 0.1846 (0.1883)\tPrec@1 65.625 (78.622)\tPrec@5 95.312 (97.053)\n",
            "Epoch: [72][750/782]\tTime 0.022 (0.024)\tLoss (Class) 0.8609 (0.8780)\tLoss (Regu) 0.1876 (0.1882)\tPrec@1 81.250 (78.458)\tPrec@5 96.875 (96.998)\n",
            "Test: [0/157]\tTime 0.116 (0.116)\tLoss (Class) 1.6197 (1.6197)\tLoss (Regu) 0.2132 (0.2132)\tPrec@1 65.625 (65.625)\tPrec@5 90.625 (90.625)\n",
            "Test: [50/157]\tTime 0.009 (0.011)\tLoss (Class) 1.3464 (1.7633)\tLoss (Regu) 0.2156 (0.2146)\tPrec@1 68.750 (61.612)\tPrec@5 93.750 (88.051)\n",
            "Test: [100/157]\tTime 0.008 (0.010)\tLoss (Class) 1.9727 (1.7263)\tLoss (Regu) 0.2133 (0.2143)\tPrec@1 62.500 (62.949)\tPrec@5 84.375 (88.335)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.3038 (1.7127)\tLoss (Regu) 0.2098 (0.2140)\tPrec@1 62.500 (62.904)\tPrec@5 90.625 (88.328)\n",
            " * Train[78.408 %, 96.992 %, 0.879 loss] Val [62.960 %, 88.280%, 1.713 loss] Best: 64.430 %\n",
            "Time for 72 / 150 20.415029764175415\n",
            "Learning rate:  0.03\n",
            "Epoch: [73][0/782]\tTime 0.146 (0.146)\tLoss (Class) 0.7785 (0.7785)\tLoss (Regu) 0.1899 (0.1899)\tPrec@1 82.812 (82.812)\tPrec@5 98.438 (98.438)\n",
            "Epoch: [73][50/782]\tTime 0.023 (0.028)\tLoss (Class) 0.9701 (0.8546)\tLoss (Regu) 0.1859 (0.1900)\tPrec@1 76.562 (79.320)\tPrec@5 98.438 (97.457)\n",
            "Epoch: [73][100/782]\tTime 0.022 (0.026)\tLoss (Class) 0.7804 (0.8419)\tLoss (Regu) 0.1868 (0.1890)\tPrec@1 79.688 (79.517)\tPrec@5 98.438 (97.478)\n",
            "Epoch: [73][150/782]\tTime 0.023 (0.026)\tLoss (Class) 0.8270 (0.8299)\tLoss (Regu) 0.1901 (0.1899)\tPrec@1 79.688 (79.967)\tPrec@5 98.438 (97.558)\n",
            "Epoch: [73][200/782]\tTime 0.022 (0.026)\tLoss (Class) 0.7714 (0.8326)\tLoss (Regu) 0.1926 (0.1897)\tPrec@1 82.812 (80.045)\tPrec@5 98.438 (97.427)\n",
            "Epoch: [73][250/782]\tTime 0.035 (0.026)\tLoss (Class) 0.8363 (0.8290)\tLoss (Regu) 0.1881 (0.1900)\tPrec@1 76.562 (79.912)\tPrec@5 95.312 (97.491)\n",
            "Epoch: [73][300/782]\tTime 0.023 (0.025)\tLoss (Class) 0.6879 (0.8314)\tLoss (Regu) 0.1859 (0.1899)\tPrec@1 84.375 (79.911)\tPrec@5 100.000 (97.404)\n",
            "Epoch: [73][350/782]\tTime 0.022 (0.025)\tLoss (Class) 1.0539 (0.8356)\tLoss (Regu) 0.1869 (0.1898)\tPrec@1 75.000 (79.808)\tPrec@5 95.312 (97.347)\n",
            "Epoch: [73][400/782]\tTime 0.023 (0.025)\tLoss (Class) 1.0229 (0.8381)\tLoss (Regu) 0.1936 (0.1897)\tPrec@1 76.562 (79.719)\tPrec@5 93.750 (97.346)\n",
            "Epoch: [73][450/782]\tTime 0.023 (0.025)\tLoss (Class) 0.7794 (0.8459)\tLoss (Regu) 0.1878 (0.1894)\tPrec@1 76.562 (79.493)\tPrec@5 100.000 (97.256)\n",
            "Epoch: [73][500/782]\tTime 0.024 (0.025)\tLoss (Class) 1.0508 (0.8507)\tLoss (Regu) 0.1841 (0.1890)\tPrec@1 75.000 (79.301)\tPrec@5 92.188 (97.178)\n",
            "Epoch: [73][550/782]\tTime 0.023 (0.025)\tLoss (Class) 0.6809 (0.8550)\tLoss (Regu) 0.1906 (0.1889)\tPrec@1 85.938 (79.231)\tPrec@5 100.000 (97.108)\n",
            "Epoch: [73][600/782]\tTime 0.021 (0.025)\tLoss (Class) 0.9496 (0.8582)\tLoss (Regu) 0.1840 (0.1889)\tPrec@1 81.250 (79.110)\tPrec@5 96.875 (97.083)\n",
            "Epoch: [73][650/782]\tTime 0.023 (0.025)\tLoss (Class) 0.9705 (0.8612)\tLoss (Regu) 0.1920 (0.1890)\tPrec@1 78.125 (79.027)\tPrec@5 93.750 (97.043)\n",
            "Epoch: [73][700/782]\tTime 0.032 (0.025)\tLoss (Class) 0.8330 (0.8612)\tLoss (Regu) 0.1836 (0.1889)\tPrec@1 78.125 (79.023)\tPrec@5 98.438 (97.047)\n",
            "Epoch: [73][750/782]\tTime 0.022 (0.025)\tLoss (Class) 0.9416 (0.8636)\tLoss (Regu) 0.1901 (0.1889)\tPrec@1 76.562 (78.907)\tPrec@5 96.875 (97.008)\n",
            "Test: [0/157]\tTime 0.112 (0.112)\tLoss (Class) 1.7750 (1.7750)\tLoss (Regu) 0.2092 (0.2092)\tPrec@1 62.500 (62.500)\tPrec@5 89.062 (89.062)\n",
            "Test: [50/157]\tTime 0.007 (0.012)\tLoss (Class) 1.3891 (1.6759)\tLoss (Regu) 0.2166 (0.2113)\tPrec@1 75.000 (62.929)\tPrec@5 87.500 (88.143)\n",
            "Test: [100/157]\tTime 0.009 (0.010)\tLoss (Class) 1.7264 (1.7383)\tLoss (Regu) 0.2098 (0.2111)\tPrec@1 59.375 (61.897)\tPrec@5 87.500 (87.500)\n",
            "Test: [150/157]\tTime 0.007 (0.010)\tLoss (Class) 1.7864 (1.7602)\tLoss (Regu) 0.2089 (0.2112)\tPrec@1 56.250 (61.579)\tPrec@5 87.500 (87.231)\n",
            " * Train[78.848 %, 96.964 %, 0.866 loss] Val [61.560 %, 87.250%, 1.763 loss] Best: 64.430 %\n",
            "Time for 73 / 150 21.12804889678955\n",
            "Learning rate:  0.03\n",
            "Epoch: [74][0/782]\tTime 0.158 (0.158)\tLoss (Class) 0.8058 (0.8058)\tLoss (Regu) 0.1854 (0.1854)\tPrec@1 79.688 (79.688)\tPrec@5 98.438 (98.438)\n",
            "Epoch: [74][50/782]\tTime 0.022 (0.025)\tLoss (Class) 0.9901 (0.8744)\tLoss (Regu) 0.1903 (0.1894)\tPrec@1 78.125 (78.707)\tPrec@5 93.750 (96.906)\n",
            "Epoch: [74][100/782]\tTime 0.024 (0.024)\tLoss (Class) 0.7205 (0.8350)\tLoss (Regu) 0.1905 (0.1914)\tPrec@1 84.375 (79.749)\tPrec@5 98.438 (97.339)\n",
            "Epoch: [74][150/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7401 (0.8381)\tLoss (Regu) 0.1887 (0.1914)\tPrec@1 84.375 (79.688)\tPrec@5 100.000 (97.310)\n",
            "Epoch: [74][200/782]\tTime 0.023 (0.024)\tLoss (Class) 1.0942 (0.8353)\tLoss (Regu) 0.1878 (0.1909)\tPrec@1 76.562 (80.006)\tPrec@5 96.875 (97.341)\n",
            "Epoch: [74][250/782]\tTime 0.023 (0.024)\tLoss (Class) 1.0328 (0.8330)\tLoss (Regu) 0.1915 (0.1908)\tPrec@1 75.000 (80.186)\tPrec@5 93.750 (97.354)\n",
            "Epoch: [74][300/782]\tTime 0.022 (0.023)\tLoss (Class) 0.9924 (0.8378)\tLoss (Regu) 0.1938 (0.1908)\tPrec@1 75.000 (80.040)\tPrec@5 96.875 (97.394)\n",
            "Epoch: [74][350/782]\tTime 0.022 (0.023)\tLoss (Class) 0.9280 (0.8407)\tLoss (Regu) 0.1889 (0.1907)\tPrec@1 78.125 (79.919)\tPrec@5 96.875 (97.374)\n",
            "Epoch: [74][400/782]\tTime 0.023 (0.023)\tLoss (Class) 0.8268 (0.8403)\tLoss (Regu) 0.1886 (0.1904)\tPrec@1 81.250 (79.886)\tPrec@5 96.875 (97.374)\n",
            "Epoch: [74][450/782]\tTime 0.023 (0.023)\tLoss (Class) 0.7615 (0.8424)\tLoss (Regu) 0.1892 (0.1903)\tPrec@1 82.812 (79.809)\tPrec@5 98.438 (97.395)\n",
            "Epoch: [74][500/782]\tTime 0.022 (0.023)\tLoss (Class) 1.0739 (0.8481)\tLoss (Regu) 0.1858 (0.1901)\tPrec@1 73.438 (79.578)\tPrec@5 93.750 (97.340)\n",
            "Epoch: [74][550/782]\tTime 0.023 (0.023)\tLoss (Class) 1.0018 (0.8501)\tLoss (Regu) 0.1862 (0.1898)\tPrec@1 79.688 (79.597)\tPrec@5 95.312 (97.295)\n",
            "Epoch: [74][600/782]\tTime 0.024 (0.024)\tLoss (Class) 0.8995 (0.8537)\tLoss (Regu) 0.1919 (0.1897)\tPrec@1 78.125 (79.433)\tPrec@5 96.875 (97.262)\n",
            "Epoch: [74][650/782]\tTime 0.022 (0.024)\tLoss (Class) 0.7651 (0.8547)\tLoss (Regu) 0.1939 (0.1898)\tPrec@1 84.375 (79.327)\tPrec@5 100.000 (97.276)\n",
            "Epoch: [74][700/782]\tTime 0.023 (0.024)\tLoss (Class) 1.0542 (0.8598)\tLoss (Regu) 0.1904 (0.1897)\tPrec@1 71.875 (79.188)\tPrec@5 95.312 (97.203)\n",
            "Epoch: [74][750/782]\tTime 0.021 (0.024)\tLoss (Class) 0.7235 (0.8629)\tLoss (Regu) 0.1893 (0.1897)\tPrec@1 85.938 (79.138)\tPrec@5 98.438 (97.156)\n",
            "Test: [0/157]\tTime 0.109 (0.109)\tLoss (Class) 1.9841 (1.9841)\tLoss (Regu) 0.2161 (0.2161)\tPrec@1 56.250 (56.250)\tPrec@5 79.688 (79.688)\n",
            "Test: [50/157]\tTime 0.008 (0.011)\tLoss (Class) 2.3734 (1.7005)\tLoss (Regu) 0.2252 (0.2169)\tPrec@1 53.125 (62.316)\tPrec@5 81.250 (88.327)\n",
            "Test: [100/157]\tTime 0.008 (0.010)\tLoss (Class) 1.9145 (1.7119)\tLoss (Regu) 0.2188 (0.2168)\tPrec@1 60.938 (62.949)\tPrec@5 89.062 (88.382)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.7750 (1.7281)\tLoss (Regu) 0.2184 (0.2169)\tPrec@1 54.688 (62.572)\tPrec@5 87.500 (88.111)\n",
            " * Train[79.070 %, 97.150 %, 0.864 loss] Val [62.700 %, 88.150%, 1.728 loss] Best: 64.430 %\n",
            "Time for 74 / 150 20.18044638633728\n",
            "Learning rate:  0.03\n",
            "Epoch: [75][0/782]\tTime 0.146 (0.146)\tLoss (Class) 1.0662 (1.0662)\tLoss (Regu) 0.1870 (0.1870)\tPrec@1 71.875 (71.875)\tPrec@5 96.875 (96.875)\n",
            "Epoch: [75][50/782]\tTime 0.022 (0.025)\tLoss (Class) 0.9631 (0.8248)\tLoss (Regu) 0.1938 (0.1905)\tPrec@1 73.438 (80.453)\tPrec@5 95.312 (97.396)\n",
            "Epoch: [75][100/782]\tTime 0.022 (0.025)\tLoss (Class) 0.8575 (0.8279)\tLoss (Regu) 0.1925 (0.1924)\tPrec@1 84.375 (80.739)\tPrec@5 96.875 (97.246)\n",
            "Epoch: [75][150/782]\tTime 0.030 (0.025)\tLoss (Class) 0.8291 (0.8144)\tLoss (Regu) 0.1875 (0.1916)\tPrec@1 76.562 (80.919)\tPrec@5 98.438 (97.403)\n",
            "Epoch: [75][200/782]\tTime 0.030 (0.025)\tLoss (Class) 0.8439 (0.8158)\tLoss (Regu) 0.1857 (0.1910)\tPrec@1 79.688 (80.636)\tPrec@5 98.438 (97.450)\n",
            "Epoch: [75][250/782]\tTime 0.022 (0.025)\tLoss (Class) 1.1536 (0.8144)\tLoss (Regu) 0.1869 (0.1906)\tPrec@1 75.000 (80.578)\tPrec@5 89.062 (97.491)\n",
            "Epoch: [75][300/782]\tTime 0.023 (0.025)\tLoss (Class) 0.6496 (0.8231)\tLoss (Regu) 0.1927 (0.1902)\tPrec@1 87.500 (80.316)\tPrec@5 100.000 (97.425)\n",
            "Epoch: [75][350/782]\tTime 0.022 (0.024)\tLoss (Class) 0.7829 (0.8242)\tLoss (Regu) 0.1892 (0.1898)\tPrec@1 81.250 (80.191)\tPrec@5 95.312 (97.436)\n",
            "Epoch: [75][400/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7974 (0.8293)\tLoss (Regu) 0.1904 (0.1898)\tPrec@1 71.875 (80.081)\tPrec@5 98.438 (97.397)\n",
            "Epoch: [75][450/782]\tTime 0.023 (0.024)\tLoss (Class) 0.9924 (0.8363)\tLoss (Regu) 0.1897 (0.1897)\tPrec@1 71.875 (79.819)\tPrec@5 100.000 (97.325)\n",
            "Epoch: [75][500/782]\tTime 0.022 (0.024)\tLoss (Class) 0.9572 (0.8403)\tLoss (Regu) 0.1872 (0.1895)\tPrec@1 70.312 (79.659)\tPrec@5 96.875 (97.318)\n",
            "Epoch: [75][550/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8510 (0.8464)\tLoss (Regu) 0.1854 (0.1893)\tPrec@1 78.125 (79.446)\tPrec@5 98.438 (97.235)\n",
            "Epoch: [75][600/782]\tTime 0.022 (0.024)\tLoss (Class) 1.0290 (0.8485)\tLoss (Regu) 0.1891 (0.1891)\tPrec@1 75.000 (79.391)\tPrec@5 95.312 (97.234)\n",
            "Epoch: [75][650/782]\tTime 0.022 (0.024)\tLoss (Class) 0.9152 (0.8484)\tLoss (Regu) 0.1882 (0.1891)\tPrec@1 73.438 (79.342)\tPrec@5 95.312 (97.230)\n",
            "Epoch: [75][700/782]\tTime 0.033 (0.024)\tLoss (Class) 0.8461 (0.8493)\tLoss (Regu) 0.1858 (0.1890)\tPrec@1 75.000 (79.358)\tPrec@5 100.000 (97.214)\n",
            "Epoch: [75][750/782]\tTime 0.023 (0.024)\tLoss (Class) 0.9088 (0.8523)\tLoss (Regu) 0.1910 (0.1891)\tPrec@1 79.688 (79.267)\tPrec@5 96.875 (97.210)\n",
            "Test: [0/157]\tTime 0.109 (0.109)\tLoss (Class) 1.3017 (1.3017)\tLoss (Regu) 0.2148 (0.2148)\tPrec@1 68.750 (68.750)\tPrec@5 96.875 (96.875)\n",
            "Test: [50/157]\tTime 0.008 (0.010)\tLoss (Class) 1.2263 (1.6578)\tLoss (Regu) 0.2153 (0.2129)\tPrec@1 70.312 (64.246)\tPrec@5 90.625 (87.898)\n",
            "Test: [100/157]\tTime 0.008 (0.009)\tLoss (Class) 1.6772 (1.7000)\tLoss (Regu) 0.2049 (0.2125)\tPrec@1 56.250 (62.655)\tPrec@5 95.312 (87.980)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.8254 (1.7059)\tLoss (Regu) 0.2112 (0.2123)\tPrec@1 59.375 (62.800)\tPrec@5 85.938 (87.831)\n",
            " * Train[79.176 %, 97.182 %, 0.855 loss] Val [63.020 %, 87.930%, 1.700 loss] Best: 64.430 %\n",
            "Time for 75 / 150 20.000157594680786\n",
            "Learning rate:  0.03\n",
            "Epoch: [76][0/782]\tTime 0.167 (0.167)\tLoss (Class) 0.6417 (0.6417)\tLoss (Regu) 0.1850 (0.1850)\tPrec@1 82.812 (82.812)\tPrec@5 98.438 (98.438)\n",
            "Epoch: [76][50/782]\tTime 0.023 (0.027)\tLoss (Class) 0.7116 (0.8609)\tLoss (Regu) 0.1900 (0.1890)\tPrec@1 90.625 (80.270)\tPrec@5 96.875 (96.998)\n",
            "Epoch: [76][100/782]\tTime 0.021 (0.027)\tLoss (Class) 0.6986 (0.8322)\tLoss (Regu) 0.1883 (0.1897)\tPrec@1 84.375 (80.523)\tPrec@5 98.438 (97.432)\n",
            "Epoch: [76][150/782]\tTime 0.023 (0.026)\tLoss (Class) 0.8709 (0.8304)\tLoss (Regu) 0.1924 (0.1884)\tPrec@1 81.250 (80.112)\tPrec@5 98.438 (97.444)\n",
            "Epoch: [76][200/782]\tTime 0.025 (0.025)\tLoss (Class) 0.7470 (0.8254)\tLoss (Regu) 0.1924 (0.1892)\tPrec@1 82.812 (80.434)\tPrec@5 100.000 (97.582)\n",
            "Epoch: [76][250/782]\tTime 0.026 (0.025)\tLoss (Class) 0.8042 (0.8275)\tLoss (Regu) 0.1884 (0.1893)\tPrec@1 78.125 (80.260)\tPrec@5 98.438 (97.560)\n",
            "Epoch: [76][300/782]\tTime 0.022 (0.025)\tLoss (Class) 0.8824 (0.8294)\tLoss (Regu) 0.1863 (0.1894)\tPrec@1 81.250 (80.139)\tPrec@5 93.750 (97.503)\n",
            "Epoch: [76][350/782]\tTime 0.025 (0.025)\tLoss (Class) 0.7431 (0.8288)\tLoss (Regu) 0.1914 (0.1895)\tPrec@1 81.250 (80.070)\tPrec@5 100.000 (97.476)\n",
            "Epoch: [76][400/782]\tTime 0.023 (0.025)\tLoss (Class) 0.8246 (0.8287)\tLoss (Regu) 0.1909 (0.1898)\tPrec@1 81.250 (80.093)\tPrec@5 93.750 (97.463)\n",
            "Epoch: [76][450/782]\tTime 0.023 (0.025)\tLoss (Class) 1.2929 (0.8345)\tLoss (Regu) 0.1868 (0.1897)\tPrec@1 67.188 (79.906)\tPrec@5 93.750 (97.436)\n",
            "Epoch: [76][500/782]\tTime 0.032 (0.025)\tLoss (Class) 0.8561 (0.8375)\tLoss (Regu) 0.1901 (0.1895)\tPrec@1 78.125 (79.918)\tPrec@5 96.875 (97.390)\n",
            "Epoch: [76][550/782]\tTime 0.022 (0.025)\tLoss (Class) 0.8643 (0.8398)\tLoss (Regu) 0.1868 (0.1897)\tPrec@1 78.125 (79.832)\tPrec@5 95.312 (97.354)\n",
            "Epoch: [76][600/782]\tTime 0.023 (0.025)\tLoss (Class) 0.9164 (0.8395)\tLoss (Regu) 0.1911 (0.1896)\tPrec@1 73.438 (79.836)\tPrec@5 98.438 (97.382)\n",
            "Epoch: [76][650/782]\tTime 0.022 (0.024)\tLoss (Class) 0.9815 (0.8434)\tLoss (Regu) 0.1873 (0.1896)\tPrec@1 71.875 (79.704)\tPrec@5 98.438 (97.329)\n",
            "Epoch: [76][700/782]\tTime 0.024 (0.024)\tLoss (Class) 0.8609 (0.8463)\tLoss (Regu) 0.1907 (0.1896)\tPrec@1 79.688 (79.625)\tPrec@5 98.438 (97.299)\n",
            "Epoch: [76][750/782]\tTime 0.022 (0.024)\tLoss (Class) 0.9646 (0.8502)\tLoss (Regu) 0.1854 (0.1896)\tPrec@1 70.312 (79.509)\tPrec@5 95.312 (97.225)\n",
            "Test: [0/157]\tTime 0.108 (0.108)\tLoss (Class) 1.4101 (1.4101)\tLoss (Regu) 0.2086 (0.2086)\tPrec@1 60.938 (60.938)\tPrec@5 90.625 (90.625)\n",
            "Test: [50/157]\tTime 0.008 (0.011)\tLoss (Class) 1.8230 (1.7103)\tLoss (Regu) 0.2179 (0.2141)\tPrec@1 62.500 (62.561)\tPrec@5 89.062 (88.388)\n",
            "Test: [100/157]\tTime 0.007 (0.010)\tLoss (Class) 1.7293 (1.7087)\tLoss (Regu) 0.2126 (0.2143)\tPrec@1 59.375 (62.515)\tPrec@5 84.375 (88.490)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.6114 (1.7325)\tLoss (Regu) 0.2223 (0.2145)\tPrec@1 62.500 (62.107)\tPrec@5 84.375 (88.059)\n",
            " * Train[79.396 %, 97.200 %, 0.852 loss] Val [62.070 %, 88.080%, 1.732 loss] Best: 64.430 %\n",
            "Time for 76 / 150 20.425133228302002\n",
            "Learning rate:  0.03\n",
            "Epoch: [77][0/782]\tTime 0.147 (0.147)\tLoss (Class) 0.9468 (0.9468)\tLoss (Regu) 0.1901 (0.1901)\tPrec@1 76.562 (76.562)\tPrec@5 96.875 (96.875)\n",
            "Epoch: [77][50/782]\tTime 0.023 (0.027)\tLoss (Class) 0.5878 (0.8030)\tLoss (Regu) 0.1966 (0.1914)\tPrec@1 89.062 (81.005)\tPrec@5 96.875 (97.335)\n",
            "Epoch: [77][100/782]\tTime 0.022 (0.026)\tLoss (Class) 0.6907 (0.8057)\tLoss (Regu) 0.1894 (0.1912)\tPrec@1 84.375 (80.848)\tPrec@5 98.438 (97.602)\n",
            "Epoch: [77][150/782]\tTime 0.023 (0.026)\tLoss (Class) 0.7622 (0.8097)\tLoss (Regu) 0.1925 (0.1911)\tPrec@1 82.812 (80.681)\tPrec@5 98.438 (97.599)\n",
            "Epoch: [77][200/782]\tTime 0.022 (0.025)\tLoss (Class) 0.5683 (0.8099)\tLoss (Regu) 0.1889 (0.1907)\tPrec@1 82.812 (80.574)\tPrec@5 100.000 (97.582)\n",
            "Epoch: [77][250/782]\tTime 0.021 (0.025)\tLoss (Class) 0.6486 (0.8208)\tLoss (Regu) 0.1893 (0.1900)\tPrec@1 84.375 (80.260)\tPrec@5 98.438 (97.429)\n",
            "Epoch: [77][300/782]\tTime 0.023 (0.025)\tLoss (Class) 0.8975 (0.8259)\tLoss (Regu) 0.1897 (0.1898)\tPrec@1 75.000 (80.066)\tPrec@5 98.438 (97.379)\n",
            "Epoch: [77][350/782]\tTime 0.022 (0.025)\tLoss (Class) 0.8452 (0.8310)\tLoss (Regu) 0.1879 (0.1898)\tPrec@1 79.688 (79.888)\tPrec@5 93.750 (97.329)\n",
            "Epoch: [77][400/782]\tTime 0.022 (0.025)\tLoss (Class) 0.7364 (0.8338)\tLoss (Regu) 0.1909 (0.1896)\tPrec@1 85.938 (79.871)\tPrec@5 98.438 (97.288)\n",
            "Epoch: [77][450/782]\tTime 0.022 (0.025)\tLoss (Class) 1.1164 (0.8368)\tLoss (Regu) 0.1868 (0.1897)\tPrec@1 76.562 (79.785)\tPrec@5 96.875 (97.235)\n",
            "Epoch: [77][500/782]\tTime 0.022 (0.025)\tLoss (Class) 0.6749 (0.8344)\tLoss (Regu) 0.1965 (0.1896)\tPrec@1 84.375 (79.809)\tPrec@5 98.438 (97.259)\n",
            "Epoch: [77][550/782]\tTime 0.030 (0.024)\tLoss (Class) 1.1580 (0.8383)\tLoss (Regu) 0.1895 (0.1898)\tPrec@1 73.438 (79.753)\tPrec@5 92.188 (97.195)\n",
            "Epoch: [77][600/782]\tTime 0.022 (0.025)\tLoss (Class) 0.9815 (0.8411)\tLoss (Regu) 0.1869 (0.1896)\tPrec@1 73.438 (79.641)\tPrec@5 95.312 (97.145)\n",
            "Epoch: [77][650/782]\tTime 0.023 (0.025)\tLoss (Class) 0.8857 (0.8440)\tLoss (Regu) 0.1920 (0.1895)\tPrec@1 75.000 (79.560)\tPrec@5 98.438 (97.153)\n",
            "Epoch: [77][700/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8785 (0.8451)\tLoss (Regu) 0.1915 (0.1895)\tPrec@1 78.125 (79.534)\tPrec@5 95.312 (97.145)\n",
            "Epoch: [77][750/782]\tTime 0.022 (0.024)\tLoss (Class) 0.8973 (0.8461)\tLoss (Regu) 0.1860 (0.1895)\tPrec@1 75.000 (79.450)\tPrec@5 98.438 (97.143)\n",
            "Test: [0/157]\tTime 0.116 (0.116)\tLoss (Class) 1.3494 (1.3494)\tLoss (Regu) 0.2113 (0.2113)\tPrec@1 67.188 (67.188)\tPrec@5 90.625 (90.625)\n",
            "Test: [50/157]\tTime 0.007 (0.010)\tLoss (Class) 1.3583 (1.7289)\tLoss (Regu) 0.2126 (0.2118)\tPrec@1 68.750 (63.358)\tPrec@5 89.062 (88.235)\n",
            "Test: [100/157]\tTime 0.007 (0.009)\tLoss (Class) 1.7253 (1.7210)\tLoss (Regu) 0.2121 (0.2118)\tPrec@1 67.188 (63.351)\tPrec@5 84.375 (88.134)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.9686 (1.7206)\tLoss (Regu) 0.2127 (0.2117)\tPrec@1 51.562 (63.214)\tPrec@5 87.500 (87.986)\n",
            " * Train[79.374 %, 97.114 %, 0.849 loss] Val [63.080 %, 87.970%, 1.724 loss] Best: 64.430 %\n",
            "Time for 77 / 150 20.425836324691772\n",
            "Learning rate:  0.03\n",
            "Epoch: [78][0/782]\tTime 0.146 (0.146)\tLoss (Class) 0.6851 (0.6851)\tLoss (Regu) 0.1837 (0.1837)\tPrec@1 84.375 (84.375)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [78][50/782]\tTime 0.022 (0.027)\tLoss (Class) 0.8582 (0.8132)\tLoss (Regu) 0.1911 (0.1908)\tPrec@1 78.125 (80.178)\tPrec@5 96.875 (97.794)\n",
            "Epoch: [78][100/782]\tTime 0.023 (0.025)\tLoss (Class) 0.6751 (0.8186)\tLoss (Regu) 0.1878 (0.1901)\tPrec@1 87.500 (80.183)\tPrec@5 100.000 (97.571)\n",
            "Epoch: [78][150/782]\tTime 0.023 (0.025)\tLoss (Class) 0.8427 (0.8033)\tLoss (Regu) 0.1924 (0.1897)\tPrec@1 82.812 (80.691)\tPrec@5 98.438 (97.755)\n",
            "Epoch: [78][200/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8670 (0.8150)\tLoss (Regu) 0.1852 (0.1895)\tPrec@1 81.250 (80.434)\tPrec@5 96.875 (97.629)\n",
            "Epoch: [78][250/782]\tTime 0.022 (0.024)\tLoss (Class) 0.8562 (0.8118)\tLoss (Regu) 0.1921 (0.1891)\tPrec@1 78.125 (80.497)\tPrec@5 96.875 (97.597)\n",
            "Epoch: [78][300/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7619 (0.8108)\tLoss (Regu) 0.1929 (0.1894)\tPrec@1 82.812 (80.637)\tPrec@5 98.438 (97.550)\n",
            "Epoch: [78][350/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6904 (0.8157)\tLoss (Regu) 0.1890 (0.1896)\tPrec@1 87.500 (80.484)\tPrec@5 96.875 (97.476)\n",
            "Epoch: [78][400/782]\tTime 0.023 (0.023)\tLoss (Class) 0.8659 (0.8230)\tLoss (Regu) 0.1872 (0.1892)\tPrec@1 75.000 (80.241)\tPrec@5 96.875 (97.436)\n",
            "Epoch: [78][450/782]\tTime 0.022 (0.023)\tLoss (Class) 0.9099 (0.8253)\tLoss (Regu) 0.1882 (0.1891)\tPrec@1 75.000 (80.207)\tPrec@5 96.875 (97.426)\n",
            "Epoch: [78][500/782]\tTime 0.022 (0.023)\tLoss (Class) 0.7668 (0.8291)\tLoss (Regu) 0.1901 (0.1888)\tPrec@1 79.688 (80.046)\tPrec@5 98.438 (97.380)\n",
            "Epoch: [78][550/782]\tTime 0.023 (0.023)\tLoss (Class) 0.7880 (0.8328)\tLoss (Regu) 0.1844 (0.1888)\tPrec@1 78.125 (79.948)\tPrec@5 98.438 (97.326)\n",
            "Epoch: [78][600/782]\tTime 0.023 (0.023)\tLoss (Class) 1.0149 (0.8370)\tLoss (Regu) 0.1876 (0.1886)\tPrec@1 71.875 (79.864)\tPrec@5 100.000 (97.296)\n",
            "Epoch: [78][650/782]\tTime 0.032 (0.024)\tLoss (Class) 0.8565 (0.8405)\tLoss (Regu) 0.1894 (0.1885)\tPrec@1 79.688 (79.745)\tPrec@5 96.875 (97.305)\n",
            "Epoch: [78][700/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6357 (0.8417)\tLoss (Regu) 0.1854 (0.1884)\tPrec@1 87.500 (79.652)\tPrec@5 98.438 (97.305)\n",
            "Epoch: [78][750/782]\tTime 0.022 (0.024)\tLoss (Class) 0.8466 (0.8465)\tLoss (Regu) 0.1873 (0.1884)\tPrec@1 81.250 (79.459)\tPrec@5 95.312 (97.283)\n",
            "Test: [0/157]\tTime 0.112 (0.112)\tLoss (Class) 1.1603 (1.1603)\tLoss (Regu) 0.2130 (0.2130)\tPrec@1 76.562 (76.562)\tPrec@5 95.312 (95.312)\n",
            "Test: [50/157]\tTime 0.009 (0.011)\tLoss (Class) 1.9079 (1.6873)\tLoss (Regu) 0.2183 (0.2130)\tPrec@1 57.812 (64.032)\tPrec@5 89.062 (88.450)\n",
            "Test: [100/157]\tTime 0.008 (0.010)\tLoss (Class) 1.3836 (1.6612)\tLoss (Regu) 0.2128 (0.2134)\tPrec@1 71.875 (64.140)\tPrec@5 90.625 (88.815)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.9921 (1.6860)\tLoss (Regu) 0.2150 (0.2129)\tPrec@1 62.500 (63.618)\tPrec@5 87.500 (88.545)\n",
            " * Train[79.430 %, 97.284 %, 0.847 loss] Val [63.510 %, 88.550%, 1.687 loss] Best: 64.430 %\n",
            "Time for 78 / 150 19.87776470184326\n",
            "Learning rate:  0.03\n",
            "Epoch: [79][0/782]\tTime 0.140 (0.140)\tLoss (Class) 0.8747 (0.8747)\tLoss (Regu) 0.1853 (0.1853)\tPrec@1 76.562 (76.562)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [79][50/782]\tTime 0.023 (0.025)\tLoss (Class) 0.8900 (0.8392)\tLoss (Regu) 0.1905 (0.1920)\tPrec@1 79.688 (80.208)\tPrec@5 93.750 (97.089)\n",
            "Epoch: [79][100/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7887 (0.8225)\tLoss (Regu) 0.1896 (0.1912)\tPrec@1 78.125 (80.337)\tPrec@5 100.000 (97.246)\n",
            "Epoch: [79][150/782]\tTime 0.021 (0.024)\tLoss (Class) 0.7688 (0.8083)\tLoss (Regu) 0.1912 (0.1913)\tPrec@1 78.125 (80.505)\tPrec@5 98.438 (97.486)\n",
            "Epoch: [79][200/782]\tTime 0.024 (0.024)\tLoss (Class) 1.0849 (0.8055)\tLoss (Regu) 0.1884 (0.1911)\tPrec@1 75.000 (80.752)\tPrec@5 95.312 (97.512)\n",
            "Epoch: [79][250/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7465 (0.8073)\tLoss (Regu) 0.1921 (0.1909)\tPrec@1 81.250 (80.659)\tPrec@5 100.000 (97.529)\n",
            "Epoch: [79][300/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6669 (0.8102)\tLoss (Regu) 0.1885 (0.1905)\tPrec@1 90.625 (80.549)\tPrec@5 96.875 (97.482)\n",
            "Epoch: [79][350/782]\tTime 0.022 (0.024)\tLoss (Class) 0.9932 (0.8135)\tLoss (Regu) 0.1880 (0.1902)\tPrec@1 76.562 (80.395)\tPrec@5 95.312 (97.445)\n",
            "Epoch: [79][400/782]\tTime 0.024 (0.024)\tLoss (Class) 0.8199 (0.8149)\tLoss (Regu) 0.1927 (0.1901)\tPrec@1 81.250 (80.369)\tPrec@5 100.000 (97.448)\n",
            "Epoch: [79][450/782]\tTime 0.023 (0.024)\tLoss (Class) 1.0109 (0.8165)\tLoss (Regu) 0.1876 (0.1900)\tPrec@1 76.562 (80.367)\tPrec@5 98.438 (97.426)\n",
            "Epoch: [79][500/782]\tTime 0.023 (0.024)\tLoss (Class) 0.9260 (0.8192)\tLoss (Regu) 0.1881 (0.1900)\tPrec@1 78.125 (80.289)\tPrec@5 98.438 (97.408)\n",
            "Epoch: [79][550/782]\tTime 0.022 (0.024)\tLoss (Class) 0.8930 (0.8276)\tLoss (Regu) 0.1833 (0.1896)\tPrec@1 73.438 (79.999)\tPrec@5 100.000 (97.351)\n",
            "Epoch: [79][600/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7973 (0.8305)\tLoss (Regu) 0.1885 (0.1894)\tPrec@1 78.125 (79.950)\tPrec@5 98.438 (97.325)\n",
            "Epoch: [79][650/782]\tTime 0.022 (0.023)\tLoss (Class) 0.7160 (0.8312)\tLoss (Regu) 0.1886 (0.1893)\tPrec@1 78.125 (79.908)\tPrec@5 98.438 (97.331)\n",
            "Epoch: [79][700/782]\tTime 0.022 (0.023)\tLoss (Class) 0.7614 (0.8339)\tLoss (Regu) 0.1856 (0.1892)\tPrec@1 85.938 (79.779)\tPrec@5 95.312 (97.312)\n",
            "Epoch: [79][750/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7015 (0.8362)\tLoss (Regu) 0.1902 (0.1892)\tPrec@1 82.812 (79.677)\tPrec@5 100.000 (97.289)\n",
            "Test: [0/157]\tTime 0.115 (0.115)\tLoss (Class) 1.1330 (1.1330)\tLoss (Regu) 0.2204 (0.2204)\tPrec@1 71.875 (71.875)\tPrec@5 96.875 (96.875)\n",
            "Test: [50/157]\tTime 0.008 (0.012)\tLoss (Class) 2.3209 (1.7665)\tLoss (Regu) 0.2198 (0.2189)\tPrec@1 54.688 (61.979)\tPrec@5 82.812 (88.082)\n",
            "Test: [100/157]\tTime 0.014 (0.011)\tLoss (Class) 1.9928 (1.7559)\tLoss (Regu) 0.2153 (0.2185)\tPrec@1 62.500 (61.866)\tPrec@5 84.375 (87.949)\n",
            "Test: [150/157]\tTime 0.007 (0.011)\tLoss (Class) 1.9240 (1.7371)\tLoss (Regu) 0.2174 (0.2185)\tPrec@1 65.625 (62.355)\tPrec@5 85.938 (87.997)\n",
            " * Train[79.560 %, 97.230 %, 0.840 loss] Val [62.320 %, 87.880%, 1.745 loss] Best: 64.430 %\n",
            "Time for 79 / 150 20.182684421539307\n",
            "Learning rate:  0.03\n",
            "Epoch: [80][0/782]\tTime 0.156 (0.156)\tLoss (Class) 0.7312 (0.7312)\tLoss (Regu) 0.1878 (0.1878)\tPrec@1 73.438 (73.438)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [80][50/782]\tTime 0.023 (0.026)\tLoss (Class) 0.7768 (0.8390)\tLoss (Regu) 0.1917 (0.1901)\tPrec@1 82.812 (79.350)\tPrec@5 98.438 (97.243)\n",
            "Epoch: [80][100/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7044 (0.8030)\tLoss (Regu) 0.1925 (0.1911)\tPrec@1 82.812 (80.801)\tPrec@5 100.000 (97.478)\n",
            "Epoch: [80][150/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7761 (0.8077)\tLoss (Regu) 0.1910 (0.1903)\tPrec@1 76.562 (80.639)\tPrec@5 96.875 (97.330)\n",
            "Epoch: [80][200/782]\tTime 0.023 (0.024)\tLoss (Class) 0.9415 (0.8072)\tLoss (Regu) 0.1861 (0.1905)\tPrec@1 78.125 (80.550)\tPrec@5 90.625 (97.365)\n",
            "Epoch: [80][250/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8935 (0.8052)\tLoss (Regu) 0.1851 (0.1898)\tPrec@1 76.562 (80.584)\tPrec@5 96.875 (97.441)\n",
            "Epoch: [80][300/782]\tTime 0.022 (0.023)\tLoss (Class) 0.9909 (0.8069)\tLoss (Regu) 0.1866 (0.1897)\tPrec@1 68.750 (80.471)\tPrec@5 96.875 (97.451)\n",
            "Epoch: [80][350/782]\tTime 0.023 (0.023)\tLoss (Class) 1.0012 (0.8116)\tLoss (Regu) 0.1883 (0.1893)\tPrec@1 71.875 (80.342)\tPrec@5 98.438 (97.440)\n",
            "Epoch: [80][400/782]\tTime 0.023 (0.023)\tLoss (Class) 0.6959 (0.8162)\tLoss (Regu) 0.1875 (0.1890)\tPrec@1 81.250 (80.229)\tPrec@5 98.438 (97.382)\n",
            "Epoch: [80][450/782]\tTime 0.023 (0.023)\tLoss (Class) 0.8812 (0.8184)\tLoss (Regu) 0.1872 (0.1889)\tPrec@1 78.125 (80.207)\tPrec@5 98.438 (97.384)\n",
            "Epoch: [80][500/782]\tTime 0.030 (0.023)\tLoss (Class) 1.1450 (0.8196)\tLoss (Regu) 0.1876 (0.1889)\tPrec@1 68.750 (80.211)\tPrec@5 92.188 (97.352)\n",
            "Epoch: [80][550/782]\tTime 0.022 (0.023)\tLoss (Class) 0.8442 (0.8225)\tLoss (Regu) 0.1917 (0.1890)\tPrec@1 76.562 (80.172)\tPrec@5 98.438 (97.357)\n",
            "Epoch: [80][600/782]\tTime 0.023 (0.023)\tLoss (Class) 0.7995 (0.8238)\tLoss (Regu) 0.1887 (0.1890)\tPrec@1 81.250 (80.155)\tPrec@5 95.312 (97.327)\n",
            "Epoch: [80][650/782]\tTime 0.022 (0.023)\tLoss (Class) 0.6115 (0.8287)\tLoss (Regu) 0.1876 (0.1889)\tPrec@1 85.938 (79.992)\tPrec@5 100.000 (97.326)\n",
            "Epoch: [80][700/782]\tTime 0.023 (0.023)\tLoss (Class) 0.8557 (0.8328)\tLoss (Regu) 0.1865 (0.1888)\tPrec@1 79.688 (79.817)\tPrec@5 96.875 (97.312)\n",
            "Epoch: [80][750/782]\tTime 0.022 (0.023)\tLoss (Class) 1.0099 (0.8361)\tLoss (Regu) 0.1879 (0.1887)\tPrec@1 76.562 (79.735)\tPrec@5 92.188 (97.289)\n",
            "Test: [0/157]\tTime 0.114 (0.114)\tLoss (Class) 1.7703 (1.7703)\tLoss (Regu) 0.2192 (0.2192)\tPrec@1 56.250 (56.250)\tPrec@5 84.375 (84.375)\n",
            "Test: [50/157]\tTime 0.009 (0.011)\tLoss (Class) 1.9486 (1.7017)\tLoss (Regu) 0.2169 (0.2158)\tPrec@1 64.062 (63.940)\tPrec@5 85.938 (87.623)\n",
            "Test: [100/157]\tTime 0.008 (0.010)\tLoss (Class) 1.5925 (1.6866)\tLoss (Regu) 0.2186 (0.2161)\tPrec@1 71.875 (63.614)\tPrec@5 92.188 (88.382)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.6643 (1.6765)\tLoss (Regu) 0.2174 (0.2159)\tPrec@1 67.188 (63.835)\tPrec@5 89.062 (88.793)\n",
            " * Train[79.680 %, 97.276 %, 0.837 loss] Val [63.940 %, 88.900%, 1.671 loss] Best: 64.430 %\n",
            "Time for 80 / 150 19.746002197265625\n",
            "Learning rate:  0.03\n",
            "Epoch: [81][0/782]\tTime 0.153 (0.153)\tLoss (Class) 0.9031 (0.9031)\tLoss (Regu) 0.1884 (0.1884)\tPrec@1 76.562 (76.562)\tPrec@5 96.875 (96.875)\n",
            "Epoch: [81][50/782]\tTime 0.024 (0.027)\tLoss (Class) 0.7232 (0.7778)\tLoss (Regu) 0.1882 (0.1906)\tPrec@1 84.375 (82.169)\tPrec@5 100.000 (97.518)\n",
            "Epoch: [81][100/782]\tTime 0.021 (0.025)\tLoss (Class) 0.9059 (0.7797)\tLoss (Regu) 0.1845 (0.1900)\tPrec@1 82.812 (81.884)\tPrec@5 98.438 (97.679)\n",
            "Epoch: [81][150/782]\tTime 0.023 (0.025)\tLoss (Class) 0.6987 (0.7821)\tLoss (Regu) 0.1910 (0.1899)\tPrec@1 81.250 (81.829)\tPrec@5 100.000 (97.786)\n",
            "Epoch: [81][200/782]\tTime 0.023 (0.025)\tLoss (Class) 0.6630 (0.7896)\tLoss (Regu) 0.1836 (0.1896)\tPrec@1 85.938 (81.468)\tPrec@5 100.000 (97.730)\n",
            "Epoch: [81][250/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8704 (0.7898)\tLoss (Regu) 0.1893 (0.1897)\tPrec@1 76.562 (81.275)\tPrec@5 95.312 (97.690)\n",
            "Epoch: [81][300/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7714 (0.7910)\tLoss (Regu) 0.1882 (0.1898)\tPrec@1 82.812 (81.198)\tPrec@5 96.875 (97.732)\n",
            "Epoch: [81][350/782]\tTime 0.023 (0.024)\tLoss (Class) 1.0317 (0.7990)\tLoss (Regu) 0.1864 (0.1894)\tPrec@1 73.438 (81.103)\tPrec@5 95.312 (97.658)\n",
            "Epoch: [81][400/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8660 (0.7995)\tLoss (Regu) 0.1932 (0.1895)\tPrec@1 78.125 (81.075)\tPrec@5 96.875 (97.635)\n",
            "Epoch: [81][450/782]\tTime 0.023 (0.024)\tLoss (Class) 1.0228 (0.8024)\tLoss (Regu) 0.1894 (0.1893)\tPrec@1 75.000 (80.962)\tPrec@5 95.312 (97.582)\n",
            "Epoch: [81][500/782]\tTime 0.025 (0.024)\tLoss (Class) 0.7214 (0.8052)\tLoss (Regu) 0.1851 (0.1893)\tPrec@1 81.250 (80.863)\tPrec@5 100.000 (97.558)\n",
            "Epoch: [81][550/782]\tTime 0.022 (0.024)\tLoss (Class) 0.7072 (0.8086)\tLoss (Regu) 0.1853 (0.1892)\tPrec@1 84.375 (80.762)\tPrec@5 100.000 (97.570)\n",
            "Epoch: [81][600/782]\tTime 0.031 (0.024)\tLoss (Class) 0.8063 (0.8106)\tLoss (Regu) 0.1886 (0.1891)\tPrec@1 84.375 (80.699)\tPrec@5 96.875 (97.554)\n",
            "Epoch: [81][650/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6691 (0.8113)\tLoss (Regu) 0.1931 (0.1891)\tPrec@1 82.812 (80.686)\tPrec@5 96.875 (97.566)\n",
            "Epoch: [81][700/782]\tTime 0.032 (0.024)\tLoss (Class) 0.9423 (0.8183)\tLoss (Regu) 0.1863 (0.1892)\tPrec@1 75.000 (80.454)\tPrec@5 95.312 (97.508)\n",
            "Epoch: [81][750/782]\tTime 0.029 (0.024)\tLoss (Class) 0.9507 (0.8227)\tLoss (Regu) 0.1897 (0.1891)\tPrec@1 78.125 (80.293)\tPrec@5 95.312 (97.474)\n",
            "Test: [0/157]\tTime 0.112 (0.112)\tLoss (Class) 1.6408 (1.6408)\tLoss (Regu) 0.2165 (0.2165)\tPrec@1 73.438 (73.438)\tPrec@5 89.062 (89.062)\n",
            "Test: [50/157]\tTime 0.007 (0.010)\tLoss (Class) 2.1902 (1.7555)\tLoss (Regu) 0.2198 (0.2193)\tPrec@1 60.938 (62.990)\tPrec@5 78.125 (87.653)\n",
            "Test: [100/157]\tTime 0.008 (0.009)\tLoss (Class) 1.2944 (1.7315)\tLoss (Regu) 0.2133 (0.2190)\tPrec@1 67.188 (63.289)\tPrec@5 90.625 (88.165)\n",
            "Test: [150/157]\tTime 0.008 (0.009)\tLoss (Class) 1.5931 (1.7335)\tLoss (Regu) 0.2180 (0.2191)\tPrec@1 68.750 (63.504)\tPrec@5 90.625 (88.400)\n",
            " * Train[80.212 %, 97.464 %, 0.825 loss] Val [63.420 %, 88.320%, 1.742 loss] Best: 64.430 %\n",
            "Time for 81 / 150 20.244713306427002\n",
            "Learning rate:  0.03\n",
            "Epoch: [82][0/782]\tTime 0.153 (0.153)\tLoss (Class) 0.5652 (0.5652)\tLoss (Regu) 0.1917 (0.1917)\tPrec@1 92.188 (92.188)\tPrec@5 98.438 (98.438)\n",
            "Epoch: [82][50/782]\tTime 0.030 (0.027)\tLoss (Class) 0.7016 (0.7708)\tLoss (Regu) 0.1891 (0.1905)\tPrec@1 84.375 (81.679)\tPrec@5 98.438 (98.039)\n",
            "Epoch: [82][100/782]\tTime 0.023 (0.026)\tLoss (Class) 0.9119 (0.7806)\tLoss (Regu) 0.1891 (0.1907)\tPrec@1 76.562 (81.436)\tPrec@5 100.000 (97.834)\n",
            "Epoch: [82][150/782]\tTime 0.032 (0.025)\tLoss (Class) 0.7301 (0.7786)\tLoss (Regu) 0.1882 (0.1909)\tPrec@1 81.250 (81.550)\tPrec@5 100.000 (97.786)\n",
            "Epoch: [82][200/782]\tTime 0.025 (0.026)\tLoss (Class) 0.8156 (0.7824)\tLoss (Regu) 0.1934 (0.1907)\tPrec@1 78.125 (81.569)\tPrec@5 96.875 (97.730)\n",
            "Epoch: [82][250/782]\tTime 0.023 (0.025)\tLoss (Class) 0.6530 (0.7825)\tLoss (Regu) 0.1895 (0.1905)\tPrec@1 85.938 (81.624)\tPrec@5 98.438 (97.759)\n",
            "Epoch: [82][300/782]\tTime 0.023 (0.025)\tLoss (Class) 0.7787 (0.7876)\tLoss (Regu) 0.1845 (0.1899)\tPrec@1 79.688 (81.401)\tPrec@5 98.438 (97.773)\n",
            "Epoch: [82][350/782]\tTime 0.023 (0.025)\tLoss (Class) 0.8159 (0.7920)\tLoss (Regu) 0.1844 (0.1896)\tPrec@1 82.812 (81.321)\tPrec@5 96.875 (97.734)\n",
            "Epoch: [82][400/782]\tTime 0.030 (0.025)\tLoss (Class) 1.1156 (0.7983)\tLoss (Regu) 0.1880 (0.1893)\tPrec@1 73.438 (81.149)\tPrec@5 93.750 (97.678)\n",
            "Epoch: [82][450/782]\tTime 0.030 (0.025)\tLoss (Class) 0.6949 (0.8029)\tLoss (Regu) 0.1855 (0.1890)\tPrec@1 81.250 (80.994)\tPrec@5 98.438 (97.630)\n",
            "Epoch: [82][500/782]\tTime 0.023 (0.025)\tLoss (Class) 0.8151 (0.8097)\tLoss (Regu) 0.1861 (0.1889)\tPrec@1 81.250 (80.745)\tPrec@5 96.875 (97.577)\n",
            "Epoch: [82][550/782]\tTime 0.030 (0.025)\tLoss (Class) 0.8978 (0.8118)\tLoss (Regu) 0.1851 (0.1888)\tPrec@1 84.375 (80.612)\tPrec@5 96.875 (97.573)\n",
            "Epoch: [82][600/782]\tTime 0.022 (0.025)\tLoss (Class) 1.0281 (0.8163)\tLoss (Regu) 0.1816 (0.1887)\tPrec@1 67.188 (80.413)\tPrec@5 95.312 (97.543)\n",
            "Epoch: [82][650/782]\tTime 0.030 (0.025)\tLoss (Class) 0.8010 (0.8195)\tLoss (Regu) 0.1919 (0.1886)\tPrec@1 82.812 (80.321)\tPrec@5 96.875 (97.487)\n",
            "Epoch: [82][700/782]\tTime 0.023 (0.025)\tLoss (Class) 0.9096 (0.8242)\tLoss (Regu) 0.1881 (0.1887)\tPrec@1 76.562 (80.153)\tPrec@5 96.875 (97.452)\n",
            "Epoch: [82][750/782]\tTime 0.023 (0.025)\tLoss (Class) 0.9292 (0.8272)\tLoss (Regu) 0.1864 (0.1888)\tPrec@1 81.250 (80.033)\tPrec@5 95.312 (97.439)\n",
            "Test: [0/157]\tTime 0.117 (0.117)\tLoss (Class) 1.7552 (1.7552)\tLoss (Regu) 0.2077 (0.2077)\tPrec@1 65.625 (65.625)\tPrec@5 89.062 (89.062)\n",
            "Test: [50/157]\tTime 0.008 (0.011)\tLoss (Class) 1.3247 (1.6454)\tLoss (Regu) 0.2145 (0.2120)\tPrec@1 67.188 (65.165)\tPrec@5 92.188 (88.756)\n",
            "Test: [100/157]\tTime 0.010 (0.010)\tLoss (Class) 1.6888 (1.6752)\tLoss (Regu) 0.2079 (0.2120)\tPrec@1 60.938 (63.954)\tPrec@5 87.500 (88.506)\n",
            "Test: [150/157]\tTime 0.007 (0.010)\tLoss (Class) 1.8498 (1.6932)\tLoss (Regu) 0.2108 (0.2121)\tPrec@1 57.812 (63.421)\tPrec@5 81.250 (88.193)\n",
            " * Train[80.010 %, 97.430 %, 0.828 loss] Val [63.390 %, 88.200%, 1.692 loss] Best: 64.430 %\n",
            "Time for 82 / 150 20.855045318603516\n",
            "Learning rate:  0.03\n",
            "Epoch: [83][0/782]\tTime 0.144 (0.144)\tLoss (Class) 1.2959 (1.2959)\tLoss (Regu) 0.1869 (0.1869)\tPrec@1 68.750 (68.750)\tPrec@5 93.750 (93.750)\n",
            "Epoch: [83][50/782]\tTime 0.024 (0.026)\tLoss (Class) 0.6339 (0.8136)\tLoss (Regu) 0.1873 (0.1888)\tPrec@1 81.250 (80.699)\tPrec@5 98.438 (97.212)\n",
            "Epoch: [83][100/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7348 (0.7717)\tLoss (Regu) 0.1879 (0.1905)\tPrec@1 84.375 (81.791)\tPrec@5 100.000 (97.819)\n",
            "Epoch: [83][150/782]\tTime 0.024 (0.024)\tLoss (Class) 0.5718 (0.7681)\tLoss (Regu) 0.1887 (0.1904)\tPrec@1 85.938 (81.881)\tPrec@5 100.000 (97.982)\n",
            "Epoch: [83][200/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8592 (0.7762)\tLoss (Regu) 0.1908 (0.1902)\tPrec@1 85.938 (81.491)\tPrec@5 93.750 (97.924)\n",
            "Epoch: [83][250/782]\tTime 0.032 (0.024)\tLoss (Class) 0.7369 (0.7810)\tLoss (Regu) 0.1892 (0.1902)\tPrec@1 82.812 (81.462)\tPrec@5 98.438 (97.915)\n",
            "Epoch: [83][300/782]\tTime 0.030 (0.024)\tLoss (Class) 0.9250 (0.7876)\tLoss (Regu) 0.1909 (0.1900)\tPrec@1 76.562 (81.281)\tPrec@5 95.312 (97.866)\n",
            "Epoch: [83][350/782]\tTime 0.031 (0.025)\tLoss (Class) 0.8848 (0.7924)\tLoss (Regu) 0.1889 (0.1898)\tPrec@1 78.125 (81.063)\tPrec@5 93.750 (97.828)\n",
            "Epoch: [83][400/782]\tTime 0.021 (0.025)\tLoss (Class) 0.9212 (0.7959)\tLoss (Regu) 0.1924 (0.1899)\tPrec@1 71.875 (80.977)\tPrec@5 96.875 (97.767)\n",
            "Epoch: [83][450/782]\tTime 0.030 (0.025)\tLoss (Class) 0.7921 (0.8000)\tLoss (Regu) 0.1885 (0.1898)\tPrec@1 85.938 (80.782)\tPrec@5 96.875 (97.706)\n",
            "Epoch: [83][500/782]\tTime 0.022 (0.025)\tLoss (Class) 0.9024 (0.8035)\tLoss (Regu) 0.1893 (0.1898)\tPrec@1 71.875 (80.676)\tPrec@5 100.000 (97.670)\n",
            "Epoch: [83][550/782]\tTime 0.030 (0.025)\tLoss (Class) 0.8411 (0.8079)\tLoss (Regu) 0.1864 (0.1898)\tPrec@1 76.562 (80.496)\tPrec@5 100.000 (97.635)\n",
            "Epoch: [83][600/782]\tTime 0.022 (0.025)\tLoss (Class) 0.7230 (0.8134)\tLoss (Regu) 0.1898 (0.1896)\tPrec@1 78.125 (80.330)\tPrec@5 96.875 (97.569)\n",
            "Epoch: [83][650/782]\tTime 0.022 (0.025)\tLoss (Class) 0.8441 (0.8179)\tLoss (Regu) 0.1909 (0.1895)\tPrec@1 76.562 (80.134)\tPrec@5 98.438 (97.559)\n",
            "Epoch: [83][700/782]\tTime 0.022 (0.025)\tLoss (Class) 0.8830 (0.8216)\tLoss (Regu) 0.1905 (0.1895)\tPrec@1 73.438 (80.006)\tPrec@5 96.875 (97.557)\n",
            "Epoch: [83][750/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6553 (0.8245)\tLoss (Regu) 0.1814 (0.1894)\tPrec@1 90.625 (79.935)\tPrec@5 98.438 (97.512)\n",
            "Test: [0/157]\tTime 0.110 (0.110)\tLoss (Class) 1.8261 (1.8261)\tLoss (Regu) 0.2148 (0.2148)\tPrec@1 59.375 (59.375)\tPrec@5 85.938 (85.938)\n",
            "Test: [50/157]\tTime 0.009 (0.010)\tLoss (Class) 1.5409 (1.7281)\tLoss (Regu) 0.2169 (0.2152)\tPrec@1 70.312 (63.419)\tPrec@5 89.062 (88.174)\n",
            "Test: [100/157]\tTime 0.008 (0.009)\tLoss (Class) 1.5325 (1.7230)\tLoss (Regu) 0.2139 (0.2151)\tPrec@1 65.625 (63.444)\tPrec@5 87.500 (87.840)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.4923 (1.7266)\tLoss (Regu) 0.2167 (0.2153)\tPrec@1 67.188 (63.204)\tPrec@5 92.188 (88.121)\n",
            " * Train[79.920 %, 97.508 %, 0.825 loss] Val [63.240 %, 88.120%, 1.727 loss] Best: 64.430 %\n",
            "Time for 83 / 150 20.513482570648193\n",
            "Learning rate:  0.03\n",
            "Epoch: [84][0/782]\tTime 0.154 (0.154)\tLoss (Class) 0.7327 (0.7327)\tLoss (Regu) 0.1849 (0.1849)\tPrec@1 84.375 (84.375)\tPrec@5 98.438 (98.438)\n",
            "Epoch: [84][50/782]\tTime 0.023 (0.027)\tLoss (Class) 0.9095 (0.8041)\tLoss (Regu) 0.1868 (0.1890)\tPrec@1 75.000 (80.576)\tPrec@5 100.000 (97.917)\n",
            "Epoch: [84][100/782]\tTime 0.023 (0.026)\tLoss (Class) 0.8256 (0.7895)\tLoss (Regu) 0.1858 (0.1895)\tPrec@1 79.688 (81.219)\tPrec@5 93.750 (97.912)\n",
            "Epoch: [84][150/782]\tTime 0.023 (0.025)\tLoss (Class) 0.8231 (0.7899)\tLoss (Regu) 0.1880 (0.1894)\tPrec@1 81.250 (81.343)\tPrec@5 96.875 (97.796)\n",
            "Epoch: [84][200/782]\tTime 0.023 (0.025)\tLoss (Class) 0.8254 (0.7855)\tLoss (Regu) 0.1944 (0.1896)\tPrec@1 78.125 (81.390)\tPrec@5 98.438 (97.878)\n",
            "Epoch: [84][250/782]\tTime 0.022 (0.024)\tLoss (Class) 0.8405 (0.7940)\tLoss (Regu) 0.1899 (0.1897)\tPrec@1 76.562 (81.020)\tPrec@5 98.438 (97.815)\n",
            "Epoch: [84][300/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8138 (0.7993)\tLoss (Regu) 0.1871 (0.1898)\tPrec@1 82.812 (80.881)\tPrec@5 96.875 (97.726)\n",
            "Epoch: [84][350/782]\tTime 0.023 (0.024)\tLoss (Class) 0.9136 (0.8008)\tLoss (Regu) 0.1919 (0.1899)\tPrec@1 76.562 (80.769)\tPrec@5 98.438 (97.761)\n",
            "Epoch: [84][400/782]\tTime 0.023 (0.024)\tLoss (Class) 1.0321 (0.8089)\tLoss (Regu) 0.1892 (0.1898)\tPrec@1 73.438 (80.490)\tPrec@5 95.312 (97.728)\n",
            "Epoch: [84][450/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8311 (0.8109)\tLoss (Regu) 0.1885 (0.1898)\tPrec@1 79.688 (80.464)\tPrec@5 98.438 (97.717)\n",
            "Epoch: [84][500/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8675 (0.8101)\tLoss (Regu) 0.1932 (0.1898)\tPrec@1 76.562 (80.501)\tPrec@5 96.875 (97.695)\n",
            "Epoch: [84][550/782]\tTime 0.023 (0.023)\tLoss (Class) 0.7004 (0.8129)\tLoss (Regu) 0.1926 (0.1898)\tPrec@1 75.000 (80.433)\tPrec@5 100.000 (97.646)\n",
            "Epoch: [84][600/782]\tTime 0.023 (0.023)\tLoss (Class) 0.7747 (0.8151)\tLoss (Regu) 0.1919 (0.1899)\tPrec@1 81.250 (80.330)\tPrec@5 98.438 (97.634)\n",
            "Epoch: [84][650/782]\tTime 0.023 (0.023)\tLoss (Class) 0.9734 (0.8173)\tLoss (Regu) 0.1919 (0.1900)\tPrec@1 79.688 (80.266)\tPrec@5 98.438 (97.619)\n",
            "Epoch: [84][700/782]\tTime 0.022 (0.023)\tLoss (Class) 0.7673 (0.8190)\tLoss (Regu) 0.1896 (0.1900)\tPrec@1 82.812 (80.211)\tPrec@5 98.438 (97.622)\n",
            "Epoch: [84][750/782]\tTime 0.023 (0.023)\tLoss (Class) 0.9613 (0.8231)\tLoss (Regu) 0.1906 (0.1900)\tPrec@1 78.125 (80.070)\tPrec@5 93.750 (97.593)\n",
            "Test: [0/157]\tTime 0.109 (0.109)\tLoss (Class) 1.7067 (1.7067)\tLoss (Regu) 0.2061 (0.2061)\tPrec@1 57.812 (57.812)\tPrec@5 89.062 (89.062)\n",
            "Test: [50/157]\tTime 0.010 (0.011)\tLoss (Class) 1.4369 (1.6328)\tLoss (Regu) 0.2100 (0.2091)\tPrec@1 65.625 (64.308)\tPrec@5 90.625 (88.572)\n",
            "Test: [100/157]\tTime 0.008 (0.010)\tLoss (Class) 1.3559 (1.6042)\tLoss (Regu) 0.2091 (0.2092)\tPrec@1 67.188 (65.114)\tPrec@5 92.188 (89.140)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.3757 (1.6308)\tLoss (Regu) 0.2092 (0.2094)\tPrec@1 67.188 (64.735)\tPrec@5 93.750 (89.052)\n",
            " * Train[80.036 %, 97.588 %, 0.824 loss] Val [64.710 %, 88.980%, 1.636 loss] Best: 64.710 %\n",
            "Time for 84 / 150 19.762974500656128\n",
            "Learning rate:  0.03\n",
            "Epoch: [85][0/782]\tTime 0.168 (0.168)\tLoss (Class) 0.7684 (0.7684)\tLoss (Regu) 0.1896 (0.1896)\tPrec@1 82.812 (82.812)\tPrec@5 98.438 (98.438)\n",
            "Epoch: [85][50/782]\tTime 0.022 (0.029)\tLoss (Class) 0.8450 (0.7638)\tLoss (Regu) 0.1885 (0.1907)\tPrec@1 81.250 (81.985)\tPrec@5 96.875 (97.794)\n",
            "Epoch: [85][100/782]\tTime 0.022 (0.027)\tLoss (Class) 0.8120 (0.7640)\tLoss (Regu) 0.1867 (0.1898)\tPrec@1 79.688 (81.915)\tPrec@5 96.875 (97.757)\n",
            "Epoch: [85][150/782]\tTime 0.023 (0.027)\tLoss (Class) 0.7576 (0.7686)\tLoss (Regu) 0.1853 (0.1898)\tPrec@1 85.938 (81.778)\tPrec@5 96.875 (97.899)\n",
            "Epoch: [85][200/782]\tTime 0.022 (0.026)\tLoss (Class) 0.6943 (0.7764)\tLoss (Regu) 0.1902 (0.1897)\tPrec@1 84.375 (81.646)\tPrec@5 98.438 (97.886)\n",
            "Epoch: [85][250/782]\tTime 0.023 (0.025)\tLoss (Class) 0.8692 (0.7828)\tLoss (Regu) 0.1949 (0.1894)\tPrec@1 78.125 (81.431)\tPrec@5 98.438 (97.883)\n",
            "Epoch: [85][300/782]\tTime 0.022 (0.025)\tLoss (Class) 0.8508 (0.7898)\tLoss (Regu) 0.1888 (0.1895)\tPrec@1 79.688 (81.157)\tPrec@5 96.875 (97.835)\n",
            "Epoch: [85][350/782]\tTime 0.022 (0.025)\tLoss (Class) 0.8226 (0.7938)\tLoss (Regu) 0.1887 (0.1893)\tPrec@1 79.688 (81.045)\tPrec@5 96.875 (97.796)\n",
            "Epoch: [85][400/782]\tTime 0.023 (0.025)\tLoss (Class) 0.8336 (0.7913)\tLoss (Regu) 0.1920 (0.1893)\tPrec@1 79.688 (81.156)\tPrec@5 95.312 (97.806)\n",
            "Epoch: [85][450/782]\tTime 0.023 (0.025)\tLoss (Class) 0.8260 (0.7995)\tLoss (Regu) 0.1876 (0.1894)\tPrec@1 82.812 (80.852)\tPrec@5 96.875 (97.724)\n",
            "Epoch: [85][500/782]\tTime 0.022 (0.025)\tLoss (Class) 1.0105 (0.8054)\tLoss (Regu) 0.1893 (0.1893)\tPrec@1 76.562 (80.654)\tPrec@5 96.875 (97.689)\n",
            "Epoch: [85][550/782]\tTime 0.032 (0.025)\tLoss (Class) 0.5921 (0.8031)\tLoss (Regu) 0.1892 (0.1893)\tPrec@1 92.188 (80.677)\tPrec@5 100.000 (97.686)\n",
            "Epoch: [85][600/782]\tTime 0.032 (0.025)\tLoss (Class) 0.7784 (0.8061)\tLoss (Regu) 0.1907 (0.1893)\tPrec@1 84.375 (80.613)\tPrec@5 95.312 (97.658)\n",
            "Epoch: [85][650/782]\tTime 0.035 (0.025)\tLoss (Class) 0.7693 (0.8093)\tLoss (Regu) 0.1922 (0.1894)\tPrec@1 75.000 (80.530)\tPrec@5 98.438 (97.645)\n",
            "Epoch: [85][700/782]\tTime 0.032 (0.025)\tLoss (Class) 0.8833 (0.8136)\tLoss (Regu) 0.1861 (0.1893)\tPrec@1 81.250 (80.343)\tPrec@5 95.312 (97.619)\n",
            "Epoch: [85][750/782]\tTime 0.032 (0.025)\tLoss (Class) 0.8600 (0.8169)\tLoss (Regu) 0.1841 (0.1893)\tPrec@1 70.312 (80.270)\tPrec@5 100.000 (97.587)\n",
            "Test: [0/157]\tTime 0.113 (0.113)\tLoss (Class) 2.0729 (2.0729)\tLoss (Regu) 0.2049 (0.2049)\tPrec@1 59.375 (59.375)\tPrec@5 84.375 (84.375)\n",
            "Test: [50/157]\tTime 0.009 (0.010)\tLoss (Class) 1.4329 (1.7082)\tLoss (Regu) 0.2091 (0.2095)\tPrec@1 73.438 (63.143)\tPrec@5 92.188 (87.776)\n",
            "Test: [100/157]\tTime 0.009 (0.009)\tLoss (Class) 1.5300 (1.7273)\tLoss (Regu) 0.2086 (0.2100)\tPrec@1 70.312 (63.165)\tPrec@5 92.188 (87.794)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.5481 (1.7255)\tLoss (Regu) 0.2119 (0.2100)\tPrec@1 64.062 (62.893)\tPrec@5 90.625 (87.800)\n",
            " * Train[80.230 %, 97.564 %, 0.819 loss] Val [62.660 %, 87.750%, 1.729 loss] Best: 64.710 %\n",
            "Time for 85 / 150 21.331517934799194\n",
            "Learning rate:  0.03\n",
            "Epoch: [86][0/782]\tTime 0.151 (0.151)\tLoss (Class) 0.5966 (0.5966)\tLoss (Regu) 0.1864 (0.1864)\tPrec@1 87.500 (87.500)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [86][50/782]\tTime 0.023 (0.026)\tLoss (Class) 0.7089 (0.7986)\tLoss (Regu) 0.1905 (0.1899)\tPrec@1 87.500 (80.699)\tPrec@5 96.875 (97.947)\n",
            "Epoch: [86][100/782]\tTime 0.024 (0.025)\tLoss (Class) 0.8091 (0.7882)\tLoss (Regu) 0.1898 (0.1900)\tPrec@1 76.562 (81.296)\tPrec@5 96.875 (97.896)\n",
            "Epoch: [86][150/782]\tTime 0.022 (0.025)\tLoss (Class) 0.7003 (0.7885)\tLoss (Regu) 0.1927 (0.1903)\tPrec@1 81.250 (81.436)\tPrec@5 98.438 (97.827)\n",
            "Epoch: [86][200/782]\tTime 0.021 (0.025)\tLoss (Class) 1.0408 (0.7878)\tLoss (Regu) 0.1937 (0.1904)\tPrec@1 79.688 (81.437)\tPrec@5 95.312 (97.862)\n",
            "Epoch: [86][250/782]\tTime 0.024 (0.024)\tLoss (Class) 0.8298 (0.7881)\tLoss (Regu) 0.1944 (0.1900)\tPrec@1 82.812 (81.443)\tPrec@5 95.312 (97.796)\n",
            "Epoch: [86][300/782]\tTime 0.022 (0.024)\tLoss (Class) 0.6733 (0.7937)\tLoss (Regu) 0.1878 (0.1901)\tPrec@1 85.938 (81.297)\tPrec@5 100.000 (97.783)\n",
            "Epoch: [86][350/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8541 (0.7958)\tLoss (Regu) 0.1865 (0.1900)\tPrec@1 76.562 (81.099)\tPrec@5 96.875 (97.765)\n",
            "Epoch: [86][400/782]\tTime 0.022 (0.024)\tLoss (Class) 0.6414 (0.7961)\tLoss (Regu) 0.1880 (0.1897)\tPrec@1 87.500 (81.067)\tPrec@5 98.438 (97.763)\n",
            "Epoch: [86][450/782]\tTime 0.022 (0.024)\tLoss (Class) 1.2476 (0.8019)\tLoss (Regu) 0.1911 (0.1896)\tPrec@1 71.875 (80.859)\tPrec@5 93.750 (97.741)\n",
            "Epoch: [86][500/782]\tTime 0.022 (0.024)\tLoss (Class) 0.8196 (0.8035)\tLoss (Regu) 0.1884 (0.1896)\tPrec@1 78.125 (80.767)\tPrec@5 98.438 (97.748)\n",
            "Epoch: [86][550/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7143 (0.8045)\tLoss (Regu) 0.1900 (0.1896)\tPrec@1 84.375 (80.754)\tPrec@5 100.000 (97.717)\n",
            "Epoch: [86][600/782]\tTime 0.032 (0.024)\tLoss (Class) 0.8628 (0.8081)\tLoss (Regu) 0.1869 (0.1893)\tPrec@1 75.000 (80.623)\tPrec@5 96.875 (97.676)\n",
            "Epoch: [86][650/782]\tTime 0.023 (0.024)\tLoss (Class) 0.9119 (0.8097)\tLoss (Regu) 0.1849 (0.1893)\tPrec@1 78.125 (80.564)\tPrec@5 98.438 (97.677)\n",
            "Epoch: [86][700/782]\tTime 0.022 (0.024)\tLoss (Class) 1.0082 (0.8140)\tLoss (Regu) 0.1878 (0.1892)\tPrec@1 71.875 (80.425)\tPrec@5 96.875 (97.606)\n",
            "Epoch: [86][750/782]\tTime 0.024 (0.024)\tLoss (Class) 0.8711 (0.8165)\tLoss (Regu) 0.1891 (0.1891)\tPrec@1 75.000 (80.353)\tPrec@5 98.438 (97.566)\n",
            "Test: [0/157]\tTime 0.116 (0.116)\tLoss (Class) 1.9678 (1.9678)\tLoss (Regu) 0.2139 (0.2139)\tPrec@1 54.688 (54.688)\tPrec@5 85.938 (85.938)\n",
            "Test: [50/157]\tTime 0.008 (0.012)\tLoss (Class) 1.9170 (1.7837)\tLoss (Regu) 0.2130 (0.2133)\tPrec@1 64.062 (63.358)\tPrec@5 79.688 (87.898)\n",
            "Test: [100/157]\tTime 0.009 (0.010)\tLoss (Class) 1.6813 (1.7648)\tLoss (Regu) 0.2091 (0.2128)\tPrec@1 67.188 (63.413)\tPrec@5 87.500 (87.995)\n",
            "Test: [150/157]\tTime 0.007 (0.010)\tLoss (Class) 1.4437 (1.7580)\tLoss (Regu) 0.2132 (0.2129)\tPrec@1 64.062 (63.317)\tPrec@5 90.625 (88.017)\n",
            " * Train[80.338 %, 97.548 %, 0.818 loss] Val [63.280 %, 87.980%, 1.758 loss] Best: 64.710 %\n",
            "Time for 86 / 150 20.307385206222534\n",
            "Learning rate:  0.03\n",
            "Epoch: [87][0/782]\tTime 0.161 (0.161)\tLoss (Class) 0.7050 (0.7050)\tLoss (Regu) 0.1862 (0.1862)\tPrec@1 81.250 (81.250)\tPrec@5 96.875 (96.875)\n",
            "Epoch: [87][50/782]\tTime 0.022 (0.026)\tLoss (Class) 0.6929 (0.7757)\tLoss (Regu) 0.1914 (0.1911)\tPrec@1 81.250 (82.047)\tPrec@5 100.000 (98.039)\n",
            "Epoch: [87][100/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8228 (0.7489)\tLoss (Regu) 0.1900 (0.1911)\tPrec@1 81.250 (82.596)\tPrec@5 98.438 (98.205)\n",
            "Epoch: [87][150/782]\tTime 0.022 (0.024)\tLoss (Class) 0.7751 (0.7505)\tLoss (Regu) 0.1895 (0.1906)\tPrec@1 82.812 (82.554)\tPrec@5 95.312 (98.044)\n",
            "Epoch: [87][200/782]\tTime 0.022 (0.024)\tLoss (Class) 0.9857 (0.7644)\tLoss (Regu) 0.1890 (0.1902)\tPrec@1 73.438 (82.152)\tPrec@5 93.750 (97.886)\n",
            "Epoch: [87][250/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7570 (0.7736)\tLoss (Regu) 0.1909 (0.1902)\tPrec@1 79.688 (81.810)\tPrec@5 100.000 (97.803)\n",
            "Epoch: [87][300/782]\tTime 0.022 (0.023)\tLoss (Class) 0.9182 (0.7876)\tLoss (Regu) 0.1849 (0.1896)\tPrec@1 79.688 (81.333)\tPrec@5 92.188 (97.674)\n",
            "Epoch: [87][350/782]\tTime 0.022 (0.024)\tLoss (Class) 0.8191 (0.7935)\tLoss (Regu) 0.1914 (0.1893)\tPrec@1 81.250 (81.268)\tPrec@5 98.438 (97.641)\n",
            "Epoch: [87][400/782]\tTime 0.030 (0.024)\tLoss (Class) 0.8269 (0.7972)\tLoss (Regu) 0.1906 (0.1892)\tPrec@1 79.688 (81.184)\tPrec@5 98.438 (97.615)\n",
            "Epoch: [87][450/782]\tTime 0.026 (0.024)\tLoss (Class) 0.8616 (0.7957)\tLoss (Regu) 0.1912 (0.1892)\tPrec@1 81.250 (81.170)\tPrec@5 96.875 (97.623)\n",
            "Epoch: [87][500/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6692 (0.7971)\tLoss (Regu) 0.1904 (0.1892)\tPrec@1 87.500 (81.128)\tPrec@5 98.438 (97.611)\n",
            "Epoch: [87][550/782]\tTime 0.022 (0.024)\tLoss (Class) 0.7992 (0.7983)\tLoss (Regu) 0.1943 (0.1893)\tPrec@1 79.688 (81.051)\tPrec@5 100.000 (97.629)\n",
            "Epoch: [87][600/782]\tTime 0.022 (0.024)\tLoss (Class) 0.9470 (0.8003)\tLoss (Regu) 0.1927 (0.1892)\tPrec@1 76.562 (80.985)\tPrec@5 96.875 (97.608)\n",
            "Epoch: [87][650/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8383 (0.8025)\tLoss (Regu) 0.1879 (0.1893)\tPrec@1 78.125 (80.960)\tPrec@5 96.875 (97.549)\n",
            "Epoch: [87][700/782]\tTime 0.023 (0.024)\tLoss (Class) 1.0026 (0.8053)\tLoss (Regu) 0.1895 (0.1893)\tPrec@1 78.125 (80.849)\tPrec@5 95.312 (97.528)\n",
            "Epoch: [87][750/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8213 (0.8091)\tLoss (Regu) 0.1895 (0.1893)\tPrec@1 78.125 (80.680)\tPrec@5 96.875 (97.499)\n",
            "Test: [0/157]\tTime 0.109 (0.109)\tLoss (Class) 1.6810 (1.6810)\tLoss (Regu) 0.2117 (0.2117)\tPrec@1 59.375 (59.375)\tPrec@5 90.625 (90.625)\n",
            "Test: [50/157]\tTime 0.008 (0.010)\tLoss (Class) 1.9430 (1.8481)\tLoss (Regu) 0.2109 (0.2115)\tPrec@1 65.625 (61.366)\tPrec@5 81.250 (87.040)\n",
            "Test: [100/157]\tTime 0.007 (0.009)\tLoss (Class) 1.2902 (1.8011)\tLoss (Regu) 0.2115 (0.2119)\tPrec@1 71.875 (62.036)\tPrec@5 92.188 (87.407)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.7150 (1.7831)\tLoss (Regu) 0.2117 (0.2118)\tPrec@1 59.375 (62.303)\tPrec@5 82.812 (87.624)\n",
            " * Train[80.610 %, 97.478 %, 0.811 loss] Val [62.300 %, 87.690%, 1.784 loss] Best: 64.710 %\n",
            "Time for 87 / 150 19.877665519714355\n",
            "Learning rate:  0.03\n",
            "Epoch: [88][0/782]\tTime 0.146 (0.146)\tLoss (Class) 0.6401 (0.6401)\tLoss (Regu) 0.1891 (0.1891)\tPrec@1 84.375 (84.375)\tPrec@5 96.875 (96.875)\n",
            "Epoch: [88][50/782]\tTime 0.022 (0.026)\tLoss (Class) 0.6327 (0.7747)\tLoss (Regu) 0.1874 (0.1896)\tPrec@1 85.938 (81.495)\tPrec@5 100.000 (98.100)\n",
            "Epoch: [88][100/782]\tTime 0.030 (0.025)\tLoss (Class) 0.8857 (0.7942)\tLoss (Regu) 0.1921 (0.1895)\tPrec@1 79.688 (80.384)\tPrec@5 96.875 (98.097)\n",
            "Epoch: [88][150/782]\tTime 0.023 (0.025)\tLoss (Class) 0.5470 (0.7910)\tLoss (Regu) 0.1908 (0.1903)\tPrec@1 85.938 (80.453)\tPrec@5 98.438 (97.972)\n",
            "Epoch: [88][200/782]\tTime 0.031 (0.025)\tLoss (Class) 0.5031 (0.7844)\tLoss (Regu) 0.1876 (0.1905)\tPrec@1 89.062 (80.815)\tPrec@5 100.000 (98.064)\n",
            "Epoch: [88][250/782]\tTime 0.024 (0.025)\tLoss (Class) 0.7392 (0.7803)\tLoss (Regu) 0.1931 (0.1902)\tPrec@1 82.812 (81.082)\tPrec@5 96.875 (98.008)\n",
            "Epoch: [88][300/782]\tTime 0.023 (0.025)\tLoss (Class) 0.9290 (0.7837)\tLoss (Regu) 0.1885 (0.1902)\tPrec@1 76.562 (81.307)\tPrec@5 93.750 (97.929)\n",
            "Epoch: [88][350/782]\tTime 0.023 (0.025)\tLoss (Class) 0.7663 (0.7879)\tLoss (Regu) 0.1938 (0.1901)\tPrec@1 76.562 (81.219)\tPrec@5 100.000 (97.868)\n",
            "Epoch: [88][400/782]\tTime 0.021 (0.025)\tLoss (Class) 0.9429 (0.7918)\tLoss (Regu) 0.1903 (0.1902)\tPrec@1 76.562 (81.184)\tPrec@5 96.875 (97.775)\n",
            "Epoch: [88][450/782]\tTime 0.022 (0.025)\tLoss (Class) 0.7381 (0.7951)\tLoss (Regu) 0.1920 (0.1903)\tPrec@1 81.250 (81.125)\tPrec@5 98.438 (97.696)\n",
            "Epoch: [88][500/782]\tTime 0.022 (0.025)\tLoss (Class) 0.7762 (0.7958)\tLoss (Regu) 0.1937 (0.1904)\tPrec@1 79.688 (81.057)\tPrec@5 96.875 (97.692)\n",
            "Epoch: [88][550/782]\tTime 0.022 (0.024)\tLoss (Class) 0.7881 (0.7956)\tLoss (Regu) 0.1877 (0.1905)\tPrec@1 78.125 (81.003)\tPrec@5 98.438 (97.672)\n",
            "Epoch: [88][600/782]\tTime 0.022 (0.024)\tLoss (Class) 0.6984 (0.7977)\tLoss (Regu) 0.1847 (0.1904)\tPrec@1 85.938 (81.000)\tPrec@5 96.875 (97.632)\n",
            "Epoch: [88][650/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6291 (0.8006)\tLoss (Regu) 0.1898 (0.1901)\tPrec@1 85.938 (80.907)\tPrec@5 98.438 (97.576)\n",
            "Epoch: [88][700/782]\tTime 0.023 (0.024)\tLoss (Class) 1.1606 (0.8029)\tLoss (Regu) 0.1898 (0.1900)\tPrec@1 73.438 (80.795)\tPrec@5 95.312 (97.564)\n",
            "Epoch: [88][750/782]\tTime 0.023 (0.024)\tLoss (Class) 1.0697 (0.8057)\tLoss (Regu) 0.1864 (0.1898)\tPrec@1 78.125 (80.665)\tPrec@5 92.188 (97.541)\n",
            "Test: [0/157]\tTime 0.113 (0.113)\tLoss (Class) 1.8510 (1.8510)\tLoss (Regu) 0.2087 (0.2087)\tPrec@1 64.062 (64.062)\tPrec@5 85.938 (85.938)\n",
            "Test: [50/157]\tTime 0.014 (0.011)\tLoss (Class) 2.1072 (1.7610)\tLoss (Regu) 0.2067 (0.2113)\tPrec@1 62.500 (63.419)\tPrec@5 82.812 (87.806)\n",
            "Test: [100/157]\tTime 0.008 (0.010)\tLoss (Class) 1.4350 (1.7516)\tLoss (Regu) 0.2135 (0.2115)\tPrec@1 67.188 (63.428)\tPrec@5 92.188 (87.809)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 2.0255 (1.7324)\tLoss (Regu) 0.2151 (0.2115)\tPrec@1 51.562 (63.369)\tPrec@5 85.938 (88.059)\n",
            " * Train[80.604 %, 97.524 %, 0.808 loss] Val [63.390 %, 88.100%, 1.732 loss] Best: 64.710 %\n",
            "Time for 88 / 150 20.415854930877686\n",
            "Learning rate:  0.03\n",
            "Epoch: [89][0/782]\tTime 0.156 (0.156)\tLoss (Class) 1.0357 (1.0357)\tLoss (Regu) 0.1877 (0.1877)\tPrec@1 71.875 (71.875)\tPrec@5 96.875 (96.875)\n",
            "Epoch: [89][50/782]\tTime 0.030 (0.027)\tLoss (Class) 0.7211 (0.7723)\tLoss (Regu) 0.1930 (0.1897)\tPrec@1 87.500 (81.342)\tPrec@5 98.438 (97.733)\n",
            "Epoch: [89][100/782]\tTime 0.030 (0.026)\tLoss (Class) 0.7742 (0.7640)\tLoss (Regu) 0.1911 (0.1907)\tPrec@1 85.938 (81.668)\tPrec@5 98.438 (97.865)\n",
            "Epoch: [89][150/782]\tTime 0.022 (0.025)\tLoss (Class) 0.8916 (0.7710)\tLoss (Regu) 0.1926 (0.1911)\tPrec@1 76.562 (81.633)\tPrec@5 98.438 (97.930)\n",
            "Epoch: [89][200/782]\tTime 0.030 (0.025)\tLoss (Class) 0.5925 (0.7697)\tLoss (Regu) 0.1939 (0.1909)\tPrec@1 90.625 (81.755)\tPrec@5 98.438 (97.893)\n",
            "Epoch: [89][250/782]\tTime 0.023 (0.025)\tLoss (Class) 0.7490 (0.7704)\tLoss (Regu) 0.1911 (0.1910)\tPrec@1 84.375 (81.779)\tPrec@5 100.000 (97.983)\n",
            "Epoch: [89][300/782]\tTime 0.022 (0.024)\tLoss (Class) 0.8759 (0.7697)\tLoss (Regu) 0.1859 (0.1908)\tPrec@1 82.812 (81.847)\tPrec@5 92.188 (97.924)\n",
            "Epoch: [89][350/782]\tTime 0.024 (0.024)\tLoss (Class) 0.8705 (0.7752)\tLoss (Regu) 0.1885 (0.1906)\tPrec@1 78.125 (81.691)\tPrec@5 96.875 (97.912)\n",
            "Epoch: [89][400/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6294 (0.7760)\tLoss (Regu) 0.1871 (0.1904)\tPrec@1 90.625 (81.694)\tPrec@5 98.438 (97.880)\n",
            "Epoch: [89][450/782]\tTime 0.024 (0.024)\tLoss (Class) 0.7657 (0.7813)\tLoss (Regu) 0.1917 (0.1904)\tPrec@1 82.812 (81.496)\tPrec@5 98.438 (97.821)\n",
            "Epoch: [89][500/782]\tTime 0.022 (0.024)\tLoss (Class) 0.9635 (0.7858)\tLoss (Regu) 0.1896 (0.1904)\tPrec@1 78.125 (81.400)\tPrec@5 95.312 (97.754)\n",
            "Epoch: [89][550/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8767 (0.7913)\tLoss (Regu) 0.1879 (0.1902)\tPrec@1 75.000 (81.188)\tPrec@5 96.875 (97.712)\n",
            "Epoch: [89][600/782]\tTime 0.022 (0.024)\tLoss (Class) 0.6924 (0.7959)\tLoss (Regu) 0.1911 (0.1901)\tPrec@1 85.938 (81.107)\tPrec@5 98.438 (97.671)\n",
            "Epoch: [89][650/782]\tTime 0.023 (0.024)\tLoss (Class) 0.9034 (0.7980)\tLoss (Regu) 0.1893 (0.1900)\tPrec@1 78.125 (81.048)\tPrec@5 98.438 (97.657)\n",
            "Epoch: [89][700/782]\tTime 0.023 (0.024)\tLoss (Class) 0.9749 (0.8001)\tLoss (Regu) 0.1887 (0.1898)\tPrec@1 75.000 (80.969)\tPrec@5 96.875 (97.631)\n",
            "Epoch: [89][750/782]\tTime 0.022 (0.024)\tLoss (Class) 0.7599 (0.8033)\tLoss (Regu) 0.1861 (0.1897)\tPrec@1 82.812 (80.884)\tPrec@5 95.312 (97.616)\n",
            "Test: [0/157]\tTime 0.119 (0.119)\tLoss (Class) 1.9723 (1.9723)\tLoss (Regu) 0.2127 (0.2127)\tPrec@1 56.250 (56.250)\tPrec@5 85.938 (85.938)\n",
            "Test: [50/157]\tTime 0.008 (0.011)\tLoss (Class) 1.7960 (1.7734)\tLoss (Regu) 0.2256 (0.2168)\tPrec@1 57.812 (62.623)\tPrec@5 85.938 (88.634)\n",
            "Test: [100/157]\tTime 0.008 (0.010)\tLoss (Class) 1.9790 (1.7547)\tLoss (Regu) 0.2191 (0.2171)\tPrec@1 54.688 (63.072)\tPrec@5 84.375 (88.413)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.2641 (1.7265)\tLoss (Regu) 0.2136 (0.2172)\tPrec@1 75.000 (63.359)\tPrec@5 92.188 (88.555)\n",
            " * Train[80.814 %, 97.596 %, 0.805 loss] Val [63.540 %, 88.640%, 1.720 loss] Best: 64.710 %\n",
            "Time for 89 / 150 19.933037996292114\n",
            "Learning rate:  0.03\n",
            "Epoch: [90][0/782]\tTime 0.141 (0.141)\tLoss (Class) 0.9288 (0.9288)\tLoss (Regu) 0.1911 (0.1911)\tPrec@1 73.438 (73.438)\tPrec@5 93.750 (93.750)\n",
            "Epoch: [90][50/782]\tTime 0.023 (0.025)\tLoss (Class) 0.7590 (0.7955)\tLoss (Regu) 0.1889 (0.1911)\tPrec@1 85.938 (81.403)\tPrec@5 95.312 (97.580)\n",
            "Epoch: [90][100/782]\tTime 0.025 (0.025)\tLoss (Class) 1.0044 (0.7712)\tLoss (Regu) 0.1907 (0.1913)\tPrec@1 73.438 (81.869)\tPrec@5 95.312 (97.942)\n",
            "Epoch: [90][150/782]\tTime 0.023 (0.025)\tLoss (Class) 0.6425 (0.7672)\tLoss (Regu) 0.1904 (0.1908)\tPrec@1 85.938 (81.985)\tPrec@5 100.000 (98.065)\n",
            "Epoch: [90][200/782]\tTime 0.030 (0.025)\tLoss (Class) 0.9249 (0.7720)\tLoss (Regu) 0.1932 (0.1906)\tPrec@1 81.250 (81.934)\tPrec@5 95.312 (97.932)\n",
            "Epoch: [90][250/782]\tTime 0.022 (0.025)\tLoss (Class) 0.7938 (0.7753)\tLoss (Regu) 0.1875 (0.1896)\tPrec@1 85.938 (81.754)\tPrec@5 98.438 (97.859)\n",
            "Epoch: [90][300/782]\tTime 0.021 (0.025)\tLoss (Class) 0.6106 (0.7802)\tLoss (Regu) 0.1876 (0.1894)\tPrec@1 87.500 (81.546)\tPrec@5 100.000 (97.804)\n",
            "Epoch: [90][350/782]\tTime 0.023 (0.025)\tLoss (Class) 0.7227 (0.7852)\tLoss (Regu) 0.1912 (0.1893)\tPrec@1 78.125 (81.384)\tPrec@5 100.000 (97.752)\n",
            "Epoch: [90][400/782]\tTime 0.023 (0.025)\tLoss (Class) 0.8189 (0.7852)\tLoss (Regu) 0.1911 (0.1893)\tPrec@1 82.812 (81.254)\tPrec@5 95.312 (97.822)\n",
            "Epoch: [90][450/782]\tTime 0.023 (0.025)\tLoss (Class) 0.8546 (0.7895)\tLoss (Regu) 0.1942 (0.1894)\tPrec@1 84.375 (81.146)\tPrec@5 95.312 (97.741)\n",
            "Epoch: [90][500/782]\tTime 0.023 (0.024)\tLoss (Class) 1.1832 (0.7912)\tLoss (Regu) 0.1948 (0.1895)\tPrec@1 73.438 (81.122)\tPrec@5 95.312 (97.701)\n",
            "Epoch: [90][550/782]\tTime 0.022 (0.024)\tLoss (Class) 0.7367 (0.7938)\tLoss (Regu) 0.1917 (0.1897)\tPrec@1 84.375 (81.077)\tPrec@5 95.312 (97.714)\n",
            "Epoch: [90][600/782]\tTime 0.024 (0.024)\tLoss (Class) 0.7804 (0.7937)\tLoss (Regu) 0.1924 (0.1896)\tPrec@1 79.688 (81.089)\tPrec@5 96.875 (97.697)\n",
            "Epoch: [90][650/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8264 (0.7944)\tLoss (Regu) 0.1883 (0.1896)\tPrec@1 79.688 (81.039)\tPrec@5 98.438 (97.713)\n",
            "Epoch: [90][700/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8430 (0.7985)\tLoss (Regu) 0.1881 (0.1894)\tPrec@1 76.562 (80.920)\tPrec@5 98.438 (97.673)\n",
            "Epoch: [90][750/782]\tTime 0.023 (0.024)\tLoss (Class) 0.9005 (0.8015)\tLoss (Regu) 0.1866 (0.1894)\tPrec@1 76.562 (80.855)\tPrec@5 96.875 (97.628)\n",
            "Test: [0/157]\tTime 0.110 (0.110)\tLoss (Class) 1.2472 (1.2472)\tLoss (Regu) 0.2143 (0.2143)\tPrec@1 73.438 (73.438)\tPrec@5 90.625 (90.625)\n",
            "Test: [50/157]\tTime 0.008 (0.011)\tLoss (Class) 1.8002 (1.6903)\tLoss (Regu) 0.2187 (0.2184)\tPrec@1 64.062 (64.767)\tPrec@5 84.375 (88.879)\n",
            "Test: [100/157]\tTime 0.007 (0.009)\tLoss (Class) 1.5762 (1.7107)\tLoss (Regu) 0.2219 (0.2189)\tPrec@1 62.500 (64.062)\tPrec@5 93.750 (88.629)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.4564 (1.7241)\tLoss (Regu) 0.2187 (0.2190)\tPrec@1 64.062 (63.990)\tPrec@5 89.062 (88.245)\n",
            " * Train[80.736 %, 97.610 %, 0.804 loss] Val [64.080 %, 88.280%, 1.719 loss] Best: 64.710 %\n",
            "Time for 90 / 150 20.169761896133423\n",
            "Learning rate:  0.03\n",
            "Epoch: [91][0/782]\tTime 0.154 (0.154)\tLoss (Class) 0.5873 (0.5873)\tLoss (Regu) 0.1919 (0.1919)\tPrec@1 89.062 (89.062)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [91][50/782]\tTime 0.032 (0.028)\tLoss (Class) 0.8717 (0.7622)\tLoss (Regu) 0.1893 (0.1912)\tPrec@1 81.250 (81.955)\tPrec@5 95.312 (98.039)\n",
            "Epoch: [91][100/782]\tTime 0.022 (0.026)\tLoss (Class) 0.6568 (0.7632)\tLoss (Regu) 0.1944 (0.1915)\tPrec@1 85.938 (82.132)\tPrec@5 96.875 (98.035)\n",
            "Epoch: [91][150/782]\tTime 0.032 (0.026)\tLoss (Class) 0.7901 (0.7582)\tLoss (Regu) 0.1937 (0.1915)\tPrec@1 81.250 (82.171)\tPrec@5 98.438 (98.055)\n",
            "Epoch: [91][200/782]\tTime 0.032 (0.026)\tLoss (Class) 0.6582 (0.7632)\tLoss (Regu) 0.1883 (0.1910)\tPrec@1 85.938 (82.167)\tPrec@5 98.438 (98.002)\n",
            "Epoch: [91][250/782]\tTime 0.023 (0.025)\tLoss (Class) 0.8668 (0.7617)\tLoss (Regu) 0.1878 (0.1901)\tPrec@1 78.125 (82.184)\tPrec@5 98.438 (98.045)\n",
            "Epoch: [91][300/782]\tTime 0.023 (0.025)\tLoss (Class) 0.8031 (0.7608)\tLoss (Regu) 0.1879 (0.1900)\tPrec@1 85.938 (82.257)\tPrec@5 98.438 (98.033)\n",
            "Epoch: [91][350/782]\tTime 0.021 (0.025)\tLoss (Class) 0.6851 (0.7654)\tLoss (Regu) 0.1887 (0.1899)\tPrec@1 85.938 (82.047)\tPrec@5 98.438 (97.992)\n",
            "Epoch: [91][400/782]\tTime 0.030 (0.025)\tLoss (Class) 0.9105 (0.7720)\tLoss (Regu) 0.1884 (0.1896)\tPrec@1 76.562 (81.842)\tPrec@5 95.312 (97.884)\n",
            "Epoch: [91][450/782]\tTime 0.022 (0.025)\tLoss (Class) 0.8263 (0.7776)\tLoss (Regu) 0.1897 (0.1894)\tPrec@1 76.562 (81.673)\tPrec@5 100.000 (97.862)\n",
            "Epoch: [91][500/782]\tTime 0.022 (0.025)\tLoss (Class) 0.8071 (0.7813)\tLoss (Regu) 0.1908 (0.1896)\tPrec@1 81.250 (81.537)\tPrec@5 98.438 (97.826)\n",
            "Epoch: [91][550/782]\tTime 0.021 (0.025)\tLoss (Class) 0.9494 (0.7841)\tLoss (Regu) 0.1870 (0.1895)\tPrec@1 70.312 (81.451)\tPrec@5 98.438 (97.794)\n",
            "Epoch: [91][600/782]\tTime 0.022 (0.025)\tLoss (Class) 0.7646 (0.7867)\tLoss (Regu) 0.1899 (0.1895)\tPrec@1 84.375 (81.388)\tPrec@5 98.438 (97.777)\n",
            "Epoch: [91][650/782]\tTime 0.021 (0.025)\tLoss (Class) 0.9133 (0.7900)\tLoss (Regu) 0.1869 (0.1894)\tPrec@1 76.562 (81.238)\tPrec@5 95.312 (97.741)\n",
            "Epoch: [91][700/782]\tTime 0.022 (0.025)\tLoss (Class) 0.5965 (0.7910)\tLoss (Regu) 0.1899 (0.1893)\tPrec@1 85.938 (81.136)\tPrec@5 100.000 (97.735)\n",
            "Epoch: [91][750/782]\tTime 0.023 (0.025)\tLoss (Class) 1.0075 (0.7934)\tLoss (Regu) 0.1881 (0.1893)\tPrec@1 73.438 (81.000)\tPrec@5 93.750 (97.716)\n",
            "Test: [0/157]\tTime 0.112 (0.112)\tLoss (Class) 1.5794 (1.5794)\tLoss (Regu) 0.2137 (0.2137)\tPrec@1 67.188 (67.188)\tPrec@5 89.062 (89.062)\n",
            "Test: [50/157]\tTime 0.009 (0.011)\tLoss (Class) 1.5817 (1.7171)\tLoss (Regu) 0.2146 (0.2150)\tPrec@1 67.188 (62.990)\tPrec@5 90.625 (88.143)\n",
            "Test: [100/157]\tTime 0.009 (0.010)\tLoss (Class) 1.9402 (1.7661)\tLoss (Regu) 0.2178 (0.2157)\tPrec@1 59.375 (62.748)\tPrec@5 84.375 (87.252)\n",
            "Test: [150/157]\tTime 0.007 (0.010)\tLoss (Class) 1.8746 (1.7669)\tLoss (Regu) 0.2111 (0.2157)\tPrec@1 65.625 (62.748)\tPrec@5 82.812 (87.428)\n",
            " * Train[80.936 %, 97.696 %, 0.795 loss] Val [62.610 %, 87.360%, 1.768 loss] Best: 64.710 %\n",
            "Time for 91 / 150 21.17346429824829\n",
            "Learning rate:  0.03\n",
            "Epoch: [92][0/782]\tTime 0.157 (0.157)\tLoss (Class) 0.8256 (0.8256)\tLoss (Regu) 0.1896 (0.1896)\tPrec@1 84.375 (84.375)\tPrec@5 98.438 (98.438)\n",
            "Epoch: [92][50/782]\tTime 0.023 (0.026)\tLoss (Class) 0.5341 (0.7811)\tLoss (Regu) 0.1910 (0.1885)\tPrec@1 89.062 (82.047)\tPrec@5 100.000 (97.886)\n",
            "Epoch: [92][100/782]\tTime 0.022 (0.024)\tLoss (Class) 0.6835 (0.7541)\tLoss (Regu) 0.1898 (0.1894)\tPrec@1 87.500 (82.766)\tPrec@5 96.875 (98.066)\n",
            "Epoch: [92][150/782]\tTime 0.023 (0.024)\tLoss (Class) 0.9179 (0.7463)\tLoss (Regu) 0.1894 (0.1899)\tPrec@1 75.000 (82.812)\tPrec@5 96.875 (98.075)\n",
            "Epoch: [92][200/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7212 (0.7501)\tLoss (Regu) 0.1885 (0.1899)\tPrec@1 82.812 (82.517)\tPrec@5 100.000 (98.018)\n",
            "Epoch: [92][250/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8459 (0.7539)\tLoss (Regu) 0.1875 (0.1897)\tPrec@1 82.812 (82.464)\tPrec@5 100.000 (97.989)\n",
            "Epoch: [92][300/782]\tTime 0.033 (0.025)\tLoss (Class) 0.7003 (0.7563)\tLoss (Regu) 0.1891 (0.1896)\tPrec@1 84.375 (82.288)\tPrec@5 98.438 (97.991)\n",
            "Epoch: [92][350/782]\tTime 0.022 (0.025)\tLoss (Class) 0.7904 (0.7601)\tLoss (Regu) 0.1904 (0.1895)\tPrec@1 81.250 (82.060)\tPrec@5 98.438 (97.948)\n",
            "Epoch: [92][400/782]\tTime 0.023 (0.025)\tLoss (Class) 1.1393 (0.7676)\tLoss (Regu) 0.1843 (0.1894)\tPrec@1 65.625 (81.834)\tPrec@5 95.312 (97.861)\n",
            "Epoch: [92][450/782]\tTime 0.030 (0.025)\tLoss (Class) 0.6915 (0.7700)\tLoss (Regu) 0.1939 (0.1891)\tPrec@1 82.812 (81.742)\tPrec@5 100.000 (97.873)\n",
            "Epoch: [92][500/782]\tTime 0.022 (0.025)\tLoss (Class) 0.9640 (0.7759)\tLoss (Regu) 0.1883 (0.1892)\tPrec@1 75.000 (81.562)\tPrec@5 100.000 (97.826)\n",
            "Epoch: [92][550/782]\tTime 0.022 (0.025)\tLoss (Class) 0.6168 (0.7761)\tLoss (Regu) 0.1906 (0.1892)\tPrec@1 90.625 (81.556)\tPrec@5 98.438 (97.842)\n",
            "Epoch: [92][600/782]\tTime 0.023 (0.025)\tLoss (Class) 0.9048 (0.7786)\tLoss (Regu) 0.1910 (0.1891)\tPrec@1 81.250 (81.424)\tPrec@5 93.750 (97.847)\n",
            "Epoch: [92][650/782]\tTime 0.023 (0.025)\tLoss (Class) 0.7945 (0.7865)\tLoss (Regu) 0.1885 (0.1889)\tPrec@1 79.688 (81.214)\tPrec@5 98.438 (97.753)\n",
            "Epoch: [92][700/782]\tTime 0.023 (0.025)\tLoss (Class) 0.7069 (0.7898)\tLoss (Regu) 0.1884 (0.1888)\tPrec@1 82.812 (81.123)\tPrec@5 98.438 (97.722)\n",
            "Epoch: [92][750/782]\tTime 0.023 (0.025)\tLoss (Class) 0.9474 (0.7925)\tLoss (Regu) 0.1901 (0.1886)\tPrec@1 75.000 (81.013)\tPrec@5 96.875 (97.711)\n",
            "Test: [0/157]\tTime 0.109 (0.109)\tLoss (Class) 1.4243 (1.4243)\tLoss (Regu) 0.2114 (0.2114)\tPrec@1 73.438 (73.438)\tPrec@5 87.500 (87.500)\n",
            "Test: [50/157]\tTime 0.008 (0.010)\tLoss (Class) 1.7185 (1.7115)\tLoss (Regu) 0.2107 (0.2118)\tPrec@1 59.375 (63.480)\tPrec@5 92.188 (88.725)\n",
            "Test: [100/157]\tTime 0.008 (0.009)\tLoss (Class) 1.6864 (1.7255)\tLoss (Regu) 0.2108 (0.2121)\tPrec@1 60.938 (63.026)\tPrec@5 87.500 (88.444)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.6807 (1.7218)\tLoss (Regu) 0.2117 (0.2121)\tPrec@1 53.125 (63.235)\tPrec@5 93.750 (88.286)\n",
            " * Train[80.948 %, 97.698 %, 0.794 loss] Val [63.270 %, 88.240%, 1.722 loss] Best: 64.710 %\n",
            "Time for 92 / 150 20.90830945968628\n",
            "Learning rate:  0.03\n",
            "Epoch: [93][0/782]\tTime 0.149 (0.149)\tLoss (Class) 0.7831 (0.7831)\tLoss (Regu) 0.1863 (0.1863)\tPrec@1 82.812 (82.812)\tPrec@5 98.438 (98.438)\n",
            "Epoch: [93][50/782]\tTime 0.023 (0.025)\tLoss (Class) 0.8160 (0.7378)\tLoss (Regu) 0.1936 (0.1915)\tPrec@1 75.000 (82.537)\tPrec@5 98.438 (98.131)\n",
            "Epoch: [93][100/782]\tTime 0.022 (0.024)\tLoss (Class) 0.7922 (0.7355)\tLoss (Regu) 0.1896 (0.1910)\tPrec@1 82.812 (82.782)\tPrec@5 96.875 (98.082)\n",
            "Epoch: [93][150/782]\tTime 0.022 (0.024)\tLoss (Class) 0.7984 (0.7548)\tLoss (Regu) 0.1879 (0.1907)\tPrec@1 78.125 (82.337)\tPrec@5 96.875 (97.930)\n",
            "Epoch: [93][200/782]\tTime 0.023 (0.023)\tLoss (Class) 0.7579 (0.7509)\tLoss (Regu) 0.1885 (0.1908)\tPrec@1 76.562 (82.579)\tPrec@5 100.000 (97.979)\n",
            "Epoch: [93][250/782]\tTime 0.023 (0.023)\tLoss (Class) 0.8188 (0.7577)\tLoss (Regu) 0.1901 (0.1904)\tPrec@1 75.000 (82.277)\tPrec@5 98.438 (97.977)\n",
            "Epoch: [93][300/782]\tTime 0.022 (0.023)\tLoss (Class) 0.5973 (0.7582)\tLoss (Regu) 0.1888 (0.1900)\tPrec@1 85.938 (82.148)\tPrec@5 98.438 (97.991)\n",
            "Epoch: [93][350/782]\tTime 0.023 (0.023)\tLoss (Class) 1.1231 (0.7654)\tLoss (Regu) 0.1899 (0.1898)\tPrec@1 71.875 (81.878)\tPrec@5 95.312 (97.943)\n",
            "Epoch: [93][400/782]\tTime 0.023 (0.023)\tLoss (Class) 0.9389 (0.7729)\tLoss (Regu) 0.1882 (0.1895)\tPrec@1 71.875 (81.550)\tPrec@5 100.000 (97.911)\n",
            "Epoch: [93][450/782]\tTime 0.030 (0.023)\tLoss (Class) 0.6736 (0.7790)\tLoss (Regu) 0.1863 (0.1893)\tPrec@1 78.125 (81.361)\tPrec@5 100.000 (97.859)\n",
            "Epoch: [93][500/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6412 (0.7794)\tLoss (Regu) 0.1932 (0.1895)\tPrec@1 84.375 (81.347)\tPrec@5 100.000 (97.870)\n",
            "Epoch: [93][550/782]\tTime 0.023 (0.023)\tLoss (Class) 0.9614 (0.7824)\tLoss (Regu) 0.1860 (0.1893)\tPrec@1 71.875 (81.298)\tPrec@5 96.875 (97.794)\n",
            "Epoch: [93][600/782]\tTime 0.025 (0.023)\tLoss (Class) 0.6713 (0.7851)\tLoss (Regu) 0.1965 (0.1891)\tPrec@1 84.375 (81.247)\tPrec@5 98.438 (97.741)\n",
            "Epoch: [93][650/782]\tTime 0.023 (0.023)\tLoss (Class) 0.6668 (0.7900)\tLoss (Regu) 0.1910 (0.1891)\tPrec@1 89.062 (81.144)\tPrec@5 100.000 (97.717)\n",
            "Epoch: [93][700/782]\tTime 0.023 (0.023)\tLoss (Class) 0.8081 (0.7910)\tLoss (Regu) 0.1881 (0.1891)\tPrec@1 79.688 (81.074)\tPrec@5 98.438 (97.724)\n",
            "Epoch: [93][750/782]\tTime 0.023 (0.023)\tLoss (Class) 0.7846 (0.7913)\tLoss (Regu) 0.1905 (0.1891)\tPrec@1 78.125 (81.090)\tPrec@5 100.000 (97.703)\n",
            "Test: [0/157]\tTime 0.118 (0.118)\tLoss (Class) 1.4957 (1.4957)\tLoss (Regu) 0.2137 (0.2137)\tPrec@1 62.500 (62.500)\tPrec@5 89.062 (89.062)\n",
            "Test: [50/157]\tTime 0.007 (0.011)\tLoss (Class) 1.9639 (1.6642)\tLoss (Regu) 0.2086 (0.2130)\tPrec@1 59.375 (62.868)\tPrec@5 84.375 (89.154)\n",
            "Test: [100/157]\tTime 0.008 (0.010)\tLoss (Class) 1.6388 (1.6999)\tLoss (Regu) 0.2174 (0.2135)\tPrec@1 59.375 (63.088)\tPrec@5 87.500 (88.537)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.6882 (1.6971)\tLoss (Regu) 0.2107 (0.2133)\tPrec@1 57.812 (63.400)\tPrec@5 84.375 (88.493)\n",
            " * Train[81.066 %, 97.700 %, 0.792 loss] Val [63.280 %, 88.480%, 1.701 loss] Best: 64.710 %\n",
            "Time for 93 / 150 19.85975694656372\n",
            "Learning rate:  0.03\n",
            "Epoch: [94][0/782]\tTime 0.156 (0.156)\tLoss (Class) 0.9198 (0.9198)\tLoss (Regu) 0.1857 (0.1857)\tPrec@1 79.688 (79.688)\tPrec@5 96.875 (96.875)\n",
            "Epoch: [94][50/782]\tTime 0.033 (0.029)\tLoss (Class) 0.8449 (0.7306)\tLoss (Regu) 0.1910 (0.1886)\tPrec@1 81.250 (84.069)\tPrec@5 98.438 (98.070)\n",
            "Epoch: [94][100/782]\tTime 0.022 (0.027)\tLoss (Class) 0.5610 (0.7290)\tLoss (Regu) 0.1894 (0.1894)\tPrec@1 85.938 (83.153)\tPrec@5 100.000 (98.267)\n",
            "Epoch: [94][150/782]\tTime 0.022 (0.026)\tLoss (Class) 0.8557 (0.7378)\tLoss (Regu) 0.1886 (0.1905)\tPrec@1 78.125 (82.730)\tPrec@5 96.875 (98.241)\n",
            "Epoch: [94][200/782]\tTime 0.022 (0.025)\tLoss (Class) 0.9637 (0.7462)\tLoss (Regu) 0.1889 (0.1903)\tPrec@1 78.125 (82.478)\tPrec@5 93.750 (98.080)\n",
            "Epoch: [94][250/782]\tTime 0.032 (0.025)\tLoss (Class) 0.7387 (0.7505)\tLoss (Regu) 0.1940 (0.1902)\tPrec@1 82.812 (82.227)\tPrec@5 96.875 (98.101)\n",
            "Epoch: [94][300/782]\tTime 0.025 (0.025)\tLoss (Class) 0.8209 (0.7583)\tLoss (Regu) 0.1899 (0.1902)\tPrec@1 84.375 (81.992)\tPrec@5 98.438 (98.027)\n",
            "Epoch: [94][350/782]\tTime 0.022 (0.025)\tLoss (Class) 0.7958 (0.7663)\tLoss (Regu) 0.1908 (0.1902)\tPrec@1 82.812 (81.740)\tPrec@5 96.875 (97.970)\n",
            "Epoch: [94][400/782]\tTime 0.023 (0.025)\tLoss (Class) 0.9892 (0.7680)\tLoss (Regu) 0.1868 (0.1904)\tPrec@1 81.250 (81.702)\tPrec@5 95.312 (97.939)\n",
            "Epoch: [94][450/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7118 (0.7693)\tLoss (Regu) 0.1913 (0.1905)\tPrec@1 85.938 (81.759)\tPrec@5 100.000 (97.928)\n",
            "Epoch: [94][500/782]\tTime 0.024 (0.024)\tLoss (Class) 1.1547 (0.7729)\tLoss (Regu) 0.1917 (0.1906)\tPrec@1 73.438 (81.643)\tPrec@5 90.625 (97.910)\n",
            "Epoch: [94][550/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8455 (0.7780)\tLoss (Regu) 0.1906 (0.1907)\tPrec@1 79.688 (81.480)\tPrec@5 96.875 (97.907)\n",
            "Epoch: [94][600/782]\tTime 0.022 (0.024)\tLoss (Class) 0.8443 (0.7787)\tLoss (Regu) 0.1866 (0.1905)\tPrec@1 78.125 (81.455)\tPrec@5 96.875 (97.905)\n",
            "Epoch: [94][650/782]\tTime 0.023 (0.024)\tLoss (Class) 0.9928 (0.7834)\tLoss (Regu) 0.1896 (0.1905)\tPrec@1 79.688 (81.276)\tPrec@5 93.750 (97.890)\n",
            "Epoch: [94][700/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7262 (0.7869)\tLoss (Regu) 0.1875 (0.1904)\tPrec@1 79.688 (81.110)\tPrec@5 96.875 (97.862)\n",
            "Epoch: [94][750/782]\tTime 0.023 (0.024)\tLoss (Class) 0.5857 (0.7897)\tLoss (Regu) 0.1902 (0.1902)\tPrec@1 85.938 (81.036)\tPrec@5 100.000 (97.830)\n",
            "Test: [0/157]\tTime 0.112 (0.112)\tLoss (Class) 1.9152 (1.9152)\tLoss (Regu) 0.2127 (0.2127)\tPrec@1 65.625 (65.625)\tPrec@5 84.375 (84.375)\n",
            "Test: [50/157]\tTime 0.008 (0.010)\tLoss (Class) 1.6329 (1.6932)\tLoss (Regu) 0.2064 (0.2130)\tPrec@1 68.750 (64.062)\tPrec@5 87.500 (88.358)\n",
            "Test: [100/157]\tTime 0.009 (0.009)\tLoss (Class) 2.2890 (1.7199)\tLoss (Regu) 0.2110 (0.2128)\tPrec@1 53.125 (63.490)\tPrec@5 87.500 (88.150)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.7043 (1.7191)\tLoss (Regu) 0.2081 (0.2125)\tPrec@1 73.438 (63.214)\tPrec@5 89.062 (88.142)\n",
            " * Train[80.982 %, 97.814 %, 0.791 loss] Val [63.330 %, 88.200%, 1.714 loss] Best: 64.710 %\n",
            "Time for 94 / 150 20.18403720855713\n",
            "Learning rate:  0.03\n",
            "Epoch: [95][0/782]\tTime 0.146 (0.146)\tLoss (Class) 0.7876 (0.7876)\tLoss (Regu) 0.1892 (0.1892)\tPrec@1 82.812 (82.812)\tPrec@5 96.875 (96.875)\n",
            "Epoch: [95][50/782]\tTime 0.022 (0.026)\tLoss (Class) 0.8113 (0.7607)\tLoss (Regu) 0.1922 (0.1891)\tPrec@1 79.688 (81.985)\tPrec@5 98.438 (98.284)\n",
            "Epoch: [95][100/782]\tTime 0.032 (0.025)\tLoss (Class) 0.7314 (0.7494)\tLoss (Regu) 0.1914 (0.1905)\tPrec@1 79.688 (82.178)\tPrec@5 100.000 (98.298)\n",
            "Epoch: [95][150/782]\tTime 0.023 (0.025)\tLoss (Class) 0.7175 (0.7456)\tLoss (Regu) 0.1934 (0.1909)\tPrec@1 82.812 (82.430)\tPrec@5 96.875 (98.210)\n",
            "Epoch: [95][200/782]\tTime 0.024 (0.025)\tLoss (Class) 0.8115 (0.7450)\tLoss (Regu) 0.1884 (0.1911)\tPrec@1 78.125 (82.525)\tPrec@5 98.438 (98.197)\n",
            "Epoch: [95][250/782]\tTime 0.023 (0.025)\tLoss (Class) 0.8533 (0.7484)\tLoss (Regu) 0.1887 (0.1907)\tPrec@1 76.562 (82.408)\tPrec@5 98.438 (98.114)\n",
            "Epoch: [95][300/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6974 (0.7577)\tLoss (Regu) 0.1877 (0.1905)\tPrec@1 92.188 (82.096)\tPrec@5 98.438 (97.960)\n",
            "Epoch: [95][350/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7147 (0.7574)\tLoss (Regu) 0.1878 (0.1901)\tPrec@1 82.812 (82.145)\tPrec@5 100.000 (97.992)\n",
            "Epoch: [95][400/782]\tTime 0.023 (0.024)\tLoss (Class) 1.0402 (0.7616)\tLoss (Regu) 0.1863 (0.1899)\tPrec@1 76.562 (82.068)\tPrec@5 95.312 (97.919)\n",
            "Epoch: [95][450/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6437 (0.7648)\tLoss (Regu) 0.1912 (0.1899)\tPrec@1 85.938 (81.887)\tPrec@5 100.000 (97.862)\n",
            "Epoch: [95][500/782]\tTime 0.022 (0.024)\tLoss (Class) 0.8134 (0.7703)\tLoss (Regu) 0.1891 (0.1898)\tPrec@1 76.562 (81.684)\tPrec@5 98.438 (97.829)\n",
            "Epoch: [95][550/782]\tTime 0.024 (0.024)\tLoss (Class) 0.5244 (0.7749)\tLoss (Regu) 0.1889 (0.1896)\tPrec@1 90.625 (81.514)\tPrec@5 100.000 (97.794)\n",
            "Epoch: [95][600/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7393 (0.7777)\tLoss (Regu) 0.1885 (0.1895)\tPrec@1 73.438 (81.429)\tPrec@5 98.438 (97.759)\n",
            "Epoch: [95][650/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8471 (0.7784)\tLoss (Regu) 0.1872 (0.1896)\tPrec@1 78.125 (81.428)\tPrec@5 96.875 (97.770)\n",
            "Epoch: [95][700/782]\tTime 0.029 (0.024)\tLoss (Class) 0.7549 (0.7808)\tLoss (Regu) 0.1928 (0.1895)\tPrec@1 78.125 (81.341)\tPrec@5 96.875 (97.722)\n",
            "Epoch: [95][750/782]\tTime 0.032 (0.024)\tLoss (Class) 0.5816 (0.7820)\tLoss (Regu) 0.1899 (0.1896)\tPrec@1 89.062 (81.275)\tPrec@5 100.000 (97.686)\n",
            "Test: [0/157]\tTime 0.115 (0.115)\tLoss (Class) 2.0316 (2.0316)\tLoss (Regu) 0.2115 (0.2115)\tPrec@1 60.938 (60.938)\tPrec@5 82.812 (82.812)\n",
            "Test: [50/157]\tTime 0.008 (0.012)\tLoss (Class) 1.4877 (1.6768)\tLoss (Regu) 0.2144 (0.2105)\tPrec@1 71.875 (63.634)\tPrec@5 93.750 (89.277)\n",
            "Test: [100/157]\tTime 0.008 (0.010)\tLoss (Class) 1.3849 (1.6978)\tLoss (Regu) 0.2086 (0.2110)\tPrec@1 70.312 (63.660)\tPrec@5 95.312 (88.939)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.6293 (1.6917)\tLoss (Regu) 0.2141 (0.2110)\tPrec@1 62.500 (63.866)\tPrec@5 85.938 (88.742)\n",
            " * Train[81.142 %, 97.646 %, 0.785 loss] Val [63.960 %, 88.760%, 1.686 loss] Best: 64.710 %\n",
            "Time for 95 / 150 20.08500623703003\n",
            "Learning rate:  0.03\n",
            "Epoch: [96][0/782]\tTime 0.151 (0.151)\tLoss (Class) 0.5832 (0.5832)\tLoss (Regu) 0.1858 (0.1858)\tPrec@1 87.500 (87.500)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [96][50/782]\tTime 0.030 (0.027)\tLoss (Class) 0.7615 (0.7226)\tLoss (Regu) 0.1899 (0.1897)\tPrec@1 82.812 (83.150)\tPrec@5 98.438 (98.407)\n",
            "Epoch: [96][100/782]\tTime 0.022 (0.026)\tLoss (Class) 0.5228 (0.7309)\tLoss (Regu) 0.1920 (0.1902)\tPrec@1 90.625 (83.029)\tPrec@5 98.438 (98.097)\n",
            "Epoch: [96][150/782]\tTime 0.031 (0.026)\tLoss (Class) 0.8285 (0.7350)\tLoss (Regu) 0.1882 (0.1903)\tPrec@1 81.250 (82.885)\tPrec@5 98.438 (98.106)\n",
            "Epoch: [96][200/782]\tTime 0.023 (0.025)\tLoss (Class) 0.7884 (0.7410)\tLoss (Regu) 0.1881 (0.1901)\tPrec@1 81.250 (82.556)\tPrec@5 96.875 (98.127)\n",
            "Epoch: [96][250/782]\tTime 0.030 (0.025)\tLoss (Class) 0.5263 (0.7358)\tLoss (Regu) 0.1931 (0.1903)\tPrec@1 92.188 (82.769)\tPrec@5 100.000 (98.132)\n",
            "Epoch: [96][300/782]\tTime 0.022 (0.025)\tLoss (Class) 0.7894 (0.7438)\tLoss (Regu) 0.1882 (0.1900)\tPrec@1 78.125 (82.460)\tPrec@5 100.000 (98.079)\n",
            "Epoch: [96][350/782]\tTime 0.022 (0.025)\tLoss (Class) 0.6223 (0.7516)\tLoss (Regu) 0.1865 (0.1897)\tPrec@1 85.938 (82.216)\tPrec@5 98.438 (98.050)\n",
            "Epoch: [96][400/782]\tTime 0.023 (0.025)\tLoss (Class) 0.7973 (0.7562)\tLoss (Regu) 0.1931 (0.1896)\tPrec@1 79.688 (82.033)\tPrec@5 100.000 (98.048)\n",
            "Epoch: [96][450/782]\tTime 0.023 (0.025)\tLoss (Class) 0.8631 (0.7612)\tLoss (Regu) 0.1897 (0.1898)\tPrec@1 73.438 (81.912)\tPrec@5 98.438 (97.994)\n",
            "Epoch: [96][500/782]\tTime 0.022 (0.025)\tLoss (Class) 0.8105 (0.7702)\tLoss (Regu) 0.1937 (0.1899)\tPrec@1 81.250 (81.677)\tPrec@5 100.000 (97.932)\n",
            "Epoch: [96][550/782]\tTime 0.022 (0.024)\tLoss (Class) 0.7190 (0.7751)\tLoss (Regu) 0.1889 (0.1899)\tPrec@1 84.375 (81.519)\tPrec@5 98.438 (97.921)\n",
            "Epoch: [96][600/782]\tTime 0.022 (0.024)\tLoss (Class) 0.8031 (0.7749)\tLoss (Regu) 0.1907 (0.1898)\tPrec@1 81.250 (81.507)\tPrec@5 98.438 (97.946)\n",
            "Epoch: [96][650/782]\tTime 0.032 (0.024)\tLoss (Class) 0.8995 (0.7756)\tLoss (Regu) 0.1881 (0.1897)\tPrec@1 73.438 (81.492)\tPrec@5 96.875 (97.948)\n",
            "Epoch: [96][700/782]\tTime 0.024 (0.024)\tLoss (Class) 0.7868 (0.7783)\tLoss (Regu) 0.1890 (0.1897)\tPrec@1 76.562 (81.370)\tPrec@5 100.000 (97.934)\n",
            "Epoch: [96][750/782]\tTime 0.031 (0.024)\tLoss (Class) 0.8282 (0.7799)\tLoss (Regu) 0.1847 (0.1896)\tPrec@1 78.125 (81.271)\tPrec@5 93.750 (97.911)\n",
            "Test: [0/157]\tTime 0.112 (0.112)\tLoss (Class) 1.7791 (1.7791)\tLoss (Regu) 0.2123 (0.2123)\tPrec@1 56.250 (56.250)\tPrec@5 85.938 (85.938)\n",
            "Test: [50/157]\tTime 0.008 (0.010)\tLoss (Class) 1.7899 (1.5848)\tLoss (Regu) 0.2155 (0.2141)\tPrec@1 62.500 (65.809)\tPrec@5 87.500 (90.165)\n",
            "Test: [100/157]\tTime 0.009 (0.010)\tLoss (Class) 1.7890 (1.6292)\tLoss (Regu) 0.2177 (0.2138)\tPrec@1 64.062 (65.176)\tPrec@5 84.375 (89.356)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.5567 (1.6600)\tLoss (Regu) 0.2094 (0.2138)\tPrec@1 70.312 (64.611)\tPrec@5 89.062 (88.980)\n",
            " * Train[81.202 %, 97.892 %, 0.782 loss] Val [64.590 %, 88.950%, 1.665 loss] Best: 64.710 %\n",
            "Time for 96 / 150 20.56597900390625\n",
            "Learning rate:  0.03\n",
            "Epoch: [97][0/782]\tTime 0.156 (0.156)\tLoss (Class) 0.6629 (0.6629)\tLoss (Regu) 0.1883 (0.1883)\tPrec@1 85.938 (85.938)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [97][50/782]\tTime 0.022 (0.027)\tLoss (Class) 0.8945 (0.7420)\tLoss (Regu) 0.1910 (0.1903)\tPrec@1 76.562 (82.721)\tPrec@5 100.000 (98.284)\n",
            "Epoch: [97][100/782]\tTime 0.021 (0.026)\tLoss (Class) 0.7252 (0.7434)\tLoss (Regu) 0.1923 (0.1896)\tPrec@1 85.938 (83.029)\tPrec@5 98.438 (98.329)\n",
            "Epoch: [97][150/782]\tTime 0.022 (0.026)\tLoss (Class) 0.6676 (0.7364)\tLoss (Regu) 0.1899 (0.1903)\tPrec@1 84.375 (83.030)\tPrec@5 98.438 (98.334)\n",
            "Epoch: [97][200/782]\tTime 0.022 (0.026)\tLoss (Class) 0.9446 (0.7327)\tLoss (Regu) 0.1917 (0.1907)\tPrec@1 75.000 (83.038)\tPrec@5 95.312 (98.399)\n",
            "Epoch: [97][250/782]\tTime 0.022 (0.026)\tLoss (Class) 0.7009 (0.7410)\tLoss (Regu) 0.1862 (0.1908)\tPrec@1 85.938 (82.688)\tPrec@5 98.438 (98.332)\n",
            "Epoch: [97][300/782]\tTime 0.022 (0.026)\tLoss (Class) 0.6611 (0.7442)\tLoss (Regu) 0.1914 (0.1904)\tPrec@1 89.062 (82.626)\tPrec@5 98.438 (98.303)\n",
            "Epoch: [97][350/782]\tTime 0.023 (0.026)\tLoss (Class) 0.8325 (0.7468)\tLoss (Regu) 0.1896 (0.1903)\tPrec@1 82.812 (82.550)\tPrec@5 96.875 (98.255)\n",
            "Epoch: [97][400/782]\tTime 0.023 (0.026)\tLoss (Class) 0.9648 (0.7500)\tLoss (Regu) 0.1972 (0.1905)\tPrec@1 78.125 (82.466)\tPrec@5 95.312 (98.200)\n",
            "Epoch: [97][450/782]\tTime 0.023 (0.026)\tLoss (Class) 0.6828 (0.7500)\tLoss (Regu) 0.1898 (0.1905)\tPrec@1 85.938 (82.473)\tPrec@5 98.438 (98.174)\n",
            "Epoch: [97][500/782]\tTime 0.023 (0.026)\tLoss (Class) 0.6190 (0.7568)\tLoss (Regu) 0.1888 (0.1903)\tPrec@1 89.062 (82.317)\tPrec@5 100.000 (98.138)\n",
            "Epoch: [97][550/782]\tTime 0.026 (0.026)\tLoss (Class) 0.9757 (0.7617)\tLoss (Regu) 0.1868 (0.1902)\tPrec@1 70.312 (82.149)\tPrec@5 95.312 (98.058)\n",
            "Epoch: [97][600/782]\tTime 0.021 (0.026)\tLoss (Class) 0.7414 (0.7645)\tLoss (Regu) 0.1864 (0.1899)\tPrec@1 82.812 (82.059)\tPrec@5 98.438 (98.022)\n",
            "Epoch: [97][650/782]\tTime 0.023 (0.026)\tLoss (Class) 0.6070 (0.7651)\tLoss (Regu) 0.1882 (0.1898)\tPrec@1 85.938 (81.972)\tPrec@5 100.000 (98.020)\n",
            "Epoch: [97][700/782]\tTime 0.022 (0.026)\tLoss (Class) 0.6706 (0.7667)\tLoss (Regu) 0.1890 (0.1897)\tPrec@1 82.812 (81.945)\tPrec@5 98.438 (97.998)\n",
            "Epoch: [97][750/782]\tTime 0.023 (0.026)\tLoss (Class) 0.8125 (0.7698)\tLoss (Regu) 0.1914 (0.1898)\tPrec@1 79.688 (81.797)\tPrec@5 98.438 (97.988)\n",
            "Test: [0/157]\tTime 0.116 (0.116)\tLoss (Class) 1.6340 (1.6340)\tLoss (Regu) 0.2136 (0.2136)\tPrec@1 70.312 (70.312)\tPrec@5 87.500 (87.500)\n",
            "Test: [50/157]\tTime 0.008 (0.011)\tLoss (Class) 2.0124 (1.6680)\tLoss (Regu) 0.2099 (0.2134)\tPrec@1 62.500 (65.380)\tPrec@5 85.938 (88.358)\n",
            "Test: [100/157]\tTime 0.009 (0.010)\tLoss (Class) 1.6715 (1.6983)\tLoss (Regu) 0.2143 (0.2132)\tPrec@1 68.750 (64.511)\tPrec@5 87.500 (88.521)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.6189 (1.7012)\tLoss (Regu) 0.2156 (0.2134)\tPrec@1 64.062 (64.249)\tPrec@5 87.500 (88.400)\n",
            " * Train[81.772 %, 97.960 %, 0.772 loss] Val [64.460 %, 88.410%, 1.695 loss] Best: 64.710 %\n",
            "Time for 97 / 150 21.6170711517334\n",
            "Learning rate:  0.03\n",
            "Epoch: [98][0/782]\tTime 0.155 (0.155)\tLoss (Class) 0.6537 (0.6537)\tLoss (Regu) 0.1891 (0.1891)\tPrec@1 85.938 (85.938)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [98][50/782]\tTime 0.022 (0.028)\tLoss (Class) 0.5992 (0.7182)\tLoss (Regu) 0.1928 (0.1912)\tPrec@1 84.375 (82.904)\tPrec@5 98.438 (98.346)\n",
            "Epoch: [98][100/782]\tTime 0.024 (0.026)\tLoss (Class) 0.7395 (0.7306)\tLoss (Regu) 0.1890 (0.1911)\tPrec@1 82.812 (82.782)\tPrec@5 100.000 (98.190)\n",
            "Epoch: [98][150/782]\tTime 0.030 (0.026)\tLoss (Class) 0.8935 (0.7359)\tLoss (Regu) 0.1921 (0.1911)\tPrec@1 82.812 (82.792)\tPrec@5 98.438 (98.200)\n",
            "Epoch: [98][200/782]\tTime 0.030 (0.026)\tLoss (Class) 0.6005 (0.7358)\tLoss (Regu) 0.1898 (0.1912)\tPrec@1 87.500 (82.844)\tPrec@5 100.000 (98.173)\n",
            "Epoch: [98][250/782]\tTime 0.022 (0.025)\tLoss (Class) 0.9972 (0.7467)\tLoss (Regu) 0.1898 (0.1912)\tPrec@1 75.000 (82.358)\tPrec@5 95.312 (98.045)\n",
            "Epoch: [98][300/782]\tTime 0.030 (0.025)\tLoss (Class) 0.6670 (0.7517)\tLoss (Regu) 0.1937 (0.1912)\tPrec@1 85.938 (82.112)\tPrec@5 100.000 (98.033)\n",
            "Epoch: [98][350/782]\tTime 0.023 (0.025)\tLoss (Class) 0.9164 (0.7541)\tLoss (Regu) 0.1889 (0.1911)\tPrec@1 76.562 (81.931)\tPrec@5 95.312 (98.086)\n",
            "Epoch: [98][400/782]\tTime 0.023 (0.025)\tLoss (Class) 0.6929 (0.7541)\tLoss (Regu) 0.1897 (0.1910)\tPrec@1 82.812 (81.990)\tPrec@5 98.438 (98.095)\n",
            "Epoch: [98][450/782]\tTime 0.023 (0.025)\tLoss (Class) 0.6039 (0.7586)\tLoss (Regu) 0.1879 (0.1907)\tPrec@1 89.062 (81.887)\tPrec@5 100.000 (98.088)\n",
            "Epoch: [98][500/782]\tTime 0.030 (0.025)\tLoss (Class) 0.6224 (0.7622)\tLoss (Regu) 0.1916 (0.1906)\tPrec@1 89.062 (81.814)\tPrec@5 100.000 (98.035)\n",
            "Epoch: [98][550/782]\tTime 0.023 (0.025)\tLoss (Class) 0.8546 (0.7653)\tLoss (Regu) 0.1917 (0.1905)\tPrec@1 81.250 (81.738)\tPrec@5 98.438 (97.995)\n",
            "Epoch: [98][600/782]\tTime 0.022 (0.024)\tLoss (Class) 0.6417 (0.7697)\tLoss (Regu) 0.1912 (0.1904)\tPrec@1 87.500 (81.596)\tPrec@5 98.438 (97.962)\n",
            "Epoch: [98][650/782]\tTime 0.022 (0.024)\tLoss (Class) 0.8390 (0.7730)\tLoss (Regu) 0.1890 (0.1904)\tPrec@1 76.562 (81.454)\tPrec@5 96.875 (97.933)\n",
            "Epoch: [98][700/782]\tTime 0.022 (0.024)\tLoss (Class) 0.9535 (0.7752)\tLoss (Regu) 0.1903 (0.1903)\tPrec@1 76.562 (81.426)\tPrec@5 95.312 (97.880)\n",
            "Epoch: [98][750/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8042 (0.7769)\tLoss (Regu) 0.1919 (0.1902)\tPrec@1 78.125 (81.369)\tPrec@5 96.875 (97.872)\n",
            "Test: [0/157]\tTime 0.110 (0.110)\tLoss (Class) 1.4693 (1.4693)\tLoss (Regu) 0.2147 (0.2147)\tPrec@1 68.750 (68.750)\tPrec@5 87.500 (87.500)\n",
            "Test: [50/157]\tTime 0.008 (0.012)\tLoss (Class) 1.4754 (1.7337)\tLoss (Regu) 0.2118 (0.2227)\tPrec@1 60.938 (64.062)\tPrec@5 89.062 (88.450)\n",
            "Test: [100/157]\tTime 0.009 (0.010)\tLoss (Class) 2.3845 (1.7435)\tLoss (Regu) 0.2211 (0.2225)\tPrec@1 56.250 (63.784)\tPrec@5 87.500 (88.567)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.7661 (1.7164)\tLoss (Regu) 0.2281 (0.2225)\tPrec@1 67.188 (64.342)\tPrec@5 87.500 (88.638)\n",
            " * Train[81.308 %, 97.866 %, 0.778 loss] Val [64.240 %, 88.600%, 1.718 loss] Best: 64.710 %\n",
            "Time for 98 / 150 20.379649877548218\n",
            "Learning rate:  0.03\n",
            "Epoch: [99][0/782]\tTime 0.149 (0.149)\tLoss (Class) 0.6079 (0.6079)\tLoss (Regu) 0.1895 (0.1895)\tPrec@1 84.375 (84.375)\tPrec@5 98.438 (98.438)\n",
            "Epoch: [99][50/782]\tTime 0.022 (0.028)\tLoss (Class) 0.8332 (0.7522)\tLoss (Regu) 0.1853 (0.1899)\tPrec@1 78.125 (81.740)\tPrec@5 100.000 (98.009)\n",
            "Epoch: [99][100/782]\tTime 0.023 (0.027)\tLoss (Class) 0.7627 (0.7391)\tLoss (Regu) 0.1923 (0.1897)\tPrec@1 84.375 (82.287)\tPrec@5 98.438 (98.190)\n",
            "Epoch: [99][150/782]\tTime 0.022 (0.027)\tLoss (Class) 0.5919 (0.7352)\tLoss (Regu) 0.1888 (0.1898)\tPrec@1 84.375 (82.554)\tPrec@5 100.000 (98.220)\n",
            "Epoch: [99][200/782]\tTime 0.022 (0.027)\tLoss (Class) 0.5838 (0.7403)\tLoss (Regu) 0.1902 (0.1897)\tPrec@1 89.062 (82.486)\tPrec@5 98.438 (98.189)\n",
            "Epoch: [99][250/782]\tTime 0.023 (0.026)\tLoss (Class) 0.7752 (0.7382)\tLoss (Regu) 0.1903 (0.1902)\tPrec@1 82.812 (82.464)\tPrec@5 98.438 (98.269)\n",
            "Epoch: [99][300/782]\tTime 0.022 (0.026)\tLoss (Class) 0.7337 (0.7451)\tLoss (Regu) 0.1898 (0.1902)\tPrec@1 81.250 (82.200)\tPrec@5 98.438 (98.188)\n",
            "Epoch: [99][350/782]\tTime 0.022 (0.025)\tLoss (Class) 0.6814 (0.7462)\tLoss (Regu) 0.1937 (0.1904)\tPrec@1 82.812 (82.229)\tPrec@5 98.438 (98.197)\n",
            "Epoch: [99][400/782]\tTime 0.023 (0.025)\tLoss (Class) 0.8704 (0.7523)\tLoss (Regu) 0.1856 (0.1904)\tPrec@1 81.250 (82.150)\tPrec@5 98.438 (98.134)\n",
            "Epoch: [99][450/782]\tTime 0.023 (0.025)\tLoss (Class) 0.5980 (0.7524)\tLoss (Regu) 0.1917 (0.1904)\tPrec@1 87.500 (82.206)\tPrec@5 98.438 (98.136)\n",
            "Epoch: [99][500/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7121 (0.7578)\tLoss (Regu) 0.1858 (0.1905)\tPrec@1 87.500 (82.030)\tPrec@5 95.312 (98.069)\n",
            "Epoch: [99][550/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7175 (0.7621)\tLoss (Regu) 0.1886 (0.1905)\tPrec@1 79.688 (81.897)\tPrec@5 98.438 (98.046)\n",
            "Epoch: [99][600/782]\tTime 0.025 (0.024)\tLoss (Class) 0.7902 (0.7661)\tLoss (Regu) 0.1906 (0.1903)\tPrec@1 76.562 (81.791)\tPrec@5 98.438 (98.003)\n",
            "Epoch: [99][650/782]\tTime 0.021 (0.024)\tLoss (Class) 0.6255 (0.7688)\tLoss (Regu) 0.1924 (0.1902)\tPrec@1 87.500 (81.699)\tPrec@5 98.438 (97.962)\n",
            "Epoch: [99][700/782]\tTime 0.022 (0.024)\tLoss (Class) 0.6526 (0.7725)\tLoss (Regu) 0.1919 (0.1902)\tPrec@1 89.062 (81.587)\tPrec@5 98.438 (97.943)\n",
            "Epoch: [99][750/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8777 (0.7741)\tLoss (Regu) 0.1878 (0.1901)\tPrec@1 81.250 (81.541)\tPrec@5 98.438 (97.932)\n",
            "Test: [0/157]\tTime 0.115 (0.115)\tLoss (Class) 2.6110 (2.6110)\tLoss (Regu) 0.2111 (0.2111)\tPrec@1 45.312 (45.312)\tPrec@5 76.562 (76.562)\n",
            "Test: [50/157]\tTime 0.007 (0.010)\tLoss (Class) 1.8404 (1.8182)\tLoss (Regu) 0.2137 (0.2121)\tPrec@1 57.812 (62.347)\tPrec@5 87.500 (86.826)\n",
            "Test: [100/157]\tTime 0.008 (0.009)\tLoss (Class) 1.7512 (1.8296)\tLoss (Regu) 0.2127 (0.2124)\tPrec@1 70.312 (62.005)\tPrec@5 90.625 (86.866)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 2.4372 (1.8298)\tLoss (Regu) 0.2110 (0.2126)\tPrec@1 56.250 (62.159)\tPrec@5 78.125 (86.838)\n",
            " * Train[81.512 %, 97.948 %, 0.774 loss] Val [62.120 %, 86.780%, 1.833 loss] Best: 64.710 %\n",
            "Time for 99 / 150 20.49549126625061\n",
            "Learning rate:  0.03\n",
            "Epoch: [100][0/782]\tTime 0.145 (0.145)\tLoss (Class) 0.6668 (0.6668)\tLoss (Regu) 0.1909 (0.1909)\tPrec@1 85.938 (85.938)\tPrec@5 98.438 (98.438)\n",
            "Epoch: [100][50/782]\tTime 0.022 (0.026)\tLoss (Class) 0.8192 (0.6938)\tLoss (Regu) 0.1885 (0.1884)\tPrec@1 79.688 (84.038)\tPrec@5 95.312 (98.621)\n",
            "Epoch: [100][100/782]\tTime 0.032 (0.025)\tLoss (Class) 0.6617 (0.6928)\tLoss (Regu) 0.1900 (0.1891)\tPrec@1 84.375 (84.035)\tPrec@5 100.000 (98.623)\n",
            "Epoch: [100][150/782]\tTime 0.023 (0.025)\tLoss (Class) 0.9084 (0.7028)\tLoss (Regu) 0.1896 (0.1890)\tPrec@1 75.000 (83.754)\tPrec@5 95.312 (98.458)\n",
            "Epoch: [100][200/782]\tTime 0.022 (0.025)\tLoss (Class) 0.5713 (0.7091)\tLoss (Regu) 0.1921 (0.1892)\tPrec@1 89.062 (83.535)\tPrec@5 98.438 (98.383)\n",
            "Epoch: [100][250/782]\tTime 0.022 (0.025)\tLoss (Class) 0.7558 (0.7108)\tLoss (Regu) 0.1936 (0.1896)\tPrec@1 81.250 (83.429)\tPrec@5 96.875 (98.338)\n",
            "Epoch: [100][300/782]\tTime 0.030 (0.025)\tLoss (Class) 0.8405 (0.7207)\tLoss (Regu) 0.1884 (0.1899)\tPrec@1 79.688 (83.217)\tPrec@5 96.875 (98.225)\n",
            "Epoch: [100][350/782]\tTime 0.022 (0.025)\tLoss (Class) 0.7054 (0.7246)\tLoss (Regu) 0.1830 (0.1897)\tPrec@1 87.500 (83.124)\tPrec@5 98.438 (98.162)\n",
            "Epoch: [100][400/782]\tTime 0.022 (0.024)\tLoss (Class) 0.7044 (0.7337)\tLoss (Regu) 0.1888 (0.1894)\tPrec@1 82.812 (82.805)\tPrec@5 98.438 (98.137)\n",
            "Epoch: [100][450/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6735 (0.7409)\tLoss (Regu) 0.1909 (0.1894)\tPrec@1 85.938 (82.542)\tPrec@5 96.875 (98.091)\n",
            "Epoch: [100][500/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7947 (0.7442)\tLoss (Regu) 0.1890 (0.1894)\tPrec@1 81.250 (82.444)\tPrec@5 98.438 (98.048)\n",
            "Epoch: [100][550/782]\tTime 0.032 (0.024)\tLoss (Class) 0.8805 (0.7510)\tLoss (Regu) 0.1871 (0.1894)\tPrec@1 78.125 (82.234)\tPrec@5 95.312 (97.970)\n",
            "Epoch: [100][600/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8180 (0.7552)\tLoss (Regu) 0.1841 (0.1894)\tPrec@1 79.688 (82.085)\tPrec@5 98.438 (97.936)\n",
            "Epoch: [100][650/782]\tTime 0.034 (0.024)\tLoss (Class) 0.5501 (0.7608)\tLoss (Regu) 0.1893 (0.1893)\tPrec@1 89.062 (81.838)\tPrec@5 100.000 (97.933)\n",
            "Epoch: [100][700/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7378 (0.7635)\tLoss (Regu) 0.1894 (0.1893)\tPrec@1 79.688 (81.749)\tPrec@5 98.438 (97.925)\n",
            "Epoch: [100][750/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7592 (0.7659)\tLoss (Regu) 0.1892 (0.1893)\tPrec@1 85.938 (81.679)\tPrec@5 98.438 (97.922)\n",
            "Test: [0/157]\tTime 0.114 (0.114)\tLoss (Class) 1.6510 (1.6510)\tLoss (Regu) 0.2096 (0.2096)\tPrec@1 67.188 (67.188)\tPrec@5 90.625 (90.625)\n",
            "Test: [50/157]\tTime 0.010 (0.010)\tLoss (Class) 1.6298 (1.8037)\tLoss (Regu) 0.2060 (0.2103)\tPrec@1 64.062 (61.918)\tPrec@5 90.625 (87.132)\n",
            "Test: [100/157]\tTime 0.008 (0.009)\tLoss (Class) 1.3968 (1.8235)\tLoss (Regu) 0.2101 (0.2102)\tPrec@1 70.312 (61.974)\tPrec@5 89.062 (86.866)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.6227 (1.8075)\tLoss (Regu) 0.2142 (0.2099)\tPrec@1 68.750 (61.900)\tPrec@5 87.500 (87.034)\n",
            " * Train[81.582 %, 97.900 %, 0.769 loss] Val [61.870 %, 87.110%, 1.804 loss] Best: 64.710 %\n",
            "Time for 100 / 150 20.60762071609497\n",
            "Learning rate:  0.03\n",
            "Epoch: [101][0/782]\tTime 0.152 (0.152)\tLoss (Class) 0.6026 (0.6026)\tLoss (Regu) 0.1838 (0.1838)\tPrec@1 89.062 (89.062)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [101][50/782]\tTime 0.022 (0.027)\tLoss (Class) 0.4718 (0.7350)\tLoss (Regu) 0.1914 (0.1886)\tPrec@1 92.188 (82.659)\tPrec@5 100.000 (98.621)\n",
            "Epoch: [101][100/782]\tTime 0.023 (0.026)\tLoss (Class) 0.8032 (0.7343)\tLoss (Regu) 0.1954 (0.1898)\tPrec@1 79.688 (82.998)\tPrec@5 96.875 (98.530)\n",
            "Epoch: [101][150/782]\tTime 0.023 (0.025)\tLoss (Class) 0.8237 (0.7377)\tLoss (Regu) 0.1944 (0.1904)\tPrec@1 82.812 (82.999)\tPrec@5 100.000 (98.489)\n",
            "Epoch: [101][200/782]\tTime 0.032 (0.025)\tLoss (Class) 0.6926 (0.7369)\tLoss (Regu) 0.1915 (0.1905)\tPrec@1 79.688 (82.921)\tPrec@5 100.000 (98.484)\n",
            "Epoch: [101][250/782]\tTime 0.022 (0.025)\tLoss (Class) 0.7639 (0.7366)\tLoss (Regu) 0.1898 (0.1905)\tPrec@1 78.125 (82.881)\tPrec@5 96.875 (98.494)\n",
            "Epoch: [101][300/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7235 (0.7377)\tLoss (Regu) 0.1929 (0.1905)\tPrec@1 84.375 (82.740)\tPrec@5 95.312 (98.386)\n",
            "Epoch: [101][350/782]\tTime 0.023 (0.024)\tLoss (Class) 0.9129 (0.7394)\tLoss (Regu) 0.1857 (0.1903)\tPrec@1 76.562 (82.652)\tPrec@5 96.875 (98.357)\n",
            "Epoch: [101][400/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6413 (0.7436)\tLoss (Regu) 0.1847 (0.1901)\tPrec@1 84.375 (82.454)\tPrec@5 100.000 (98.297)\n",
            "Epoch: [101][450/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6913 (0.7457)\tLoss (Regu) 0.1897 (0.1902)\tPrec@1 85.938 (82.348)\tPrec@5 98.438 (98.292)\n",
            "Epoch: [101][500/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6577 (0.7528)\tLoss (Regu) 0.1874 (0.1900)\tPrec@1 84.375 (82.145)\tPrec@5 96.875 (98.238)\n",
            "Epoch: [101][550/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7715 (0.7574)\tLoss (Regu) 0.1912 (0.1899)\tPrec@1 81.250 (82.038)\tPrec@5 96.875 (98.177)\n",
            "Epoch: [101][600/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8260 (0.7611)\tLoss (Regu) 0.1854 (0.1899)\tPrec@1 76.562 (81.970)\tPrec@5 96.875 (98.105)\n",
            "Epoch: [101][650/782]\tTime 0.024 (0.024)\tLoss (Class) 0.6062 (0.7667)\tLoss (Regu) 0.1826 (0.1896)\tPrec@1 85.938 (81.797)\tPrec@5 98.438 (98.037)\n",
            "Epoch: [101][700/782]\tTime 0.023 (0.024)\tLoss (Class) 0.9965 (0.7691)\tLoss (Regu) 0.1898 (0.1894)\tPrec@1 71.875 (81.738)\tPrec@5 95.312 (98.030)\n",
            "Epoch: [101][750/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6615 (0.7719)\tLoss (Regu) 0.1857 (0.1894)\tPrec@1 87.500 (81.685)\tPrec@5 96.875 (97.994)\n",
            "Test: [0/157]\tTime 0.112 (0.112)\tLoss (Class) 2.1116 (2.1116)\tLoss (Regu) 0.2159 (0.2159)\tPrec@1 56.250 (56.250)\tPrec@5 84.375 (84.375)\n",
            "Test: [50/157]\tTime 0.008 (0.011)\tLoss (Class) 2.1220 (1.7141)\tLoss (Regu) 0.2154 (0.2165)\tPrec@1 59.375 (64.185)\tPrec@5 81.250 (88.879)\n",
            "Test: [100/157]\tTime 0.008 (0.010)\tLoss (Class) 2.1615 (1.7569)\tLoss (Regu) 0.2140 (0.2163)\tPrec@1 56.250 (63.243)\tPrec@5 84.375 (88.196)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.2265 (1.7419)\tLoss (Regu) 0.2184 (0.2163)\tPrec@1 70.312 (63.059)\tPrec@5 89.062 (88.193)\n",
            " * Train[81.606 %, 97.978 %, 0.774 loss] Val [62.940 %, 88.130%, 1.747 loss] Best: 64.710 %\n",
            "Time for 101 / 150 20.009512662887573\n",
            "Learning rate:  0.03\n",
            "Epoch: [102][0/782]\tTime 0.153 (0.153)\tLoss (Class) 0.5582 (0.5582)\tLoss (Regu) 0.1910 (0.1910)\tPrec@1 89.062 (89.062)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [102][50/782]\tTime 0.030 (0.028)\tLoss (Class) 0.8527 (0.7487)\tLoss (Regu) 0.1940 (0.1938)\tPrec@1 87.500 (82.843)\tPrec@5 98.438 (98.438)\n",
            "Epoch: [102][100/782]\tTime 0.023 (0.026)\tLoss (Class) 0.8375 (0.7420)\tLoss (Regu) 0.1911 (0.1934)\tPrec@1 76.562 (82.874)\tPrec@5 100.000 (98.345)\n",
            "Epoch: [102][150/782]\tTime 0.030 (0.025)\tLoss (Class) 0.5445 (0.7344)\tLoss (Regu) 0.1926 (0.1921)\tPrec@1 90.625 (83.030)\tPrec@5 98.438 (98.406)\n",
            "Epoch: [102][200/782]\tTime 0.022 (0.025)\tLoss (Class) 0.7210 (0.7302)\tLoss (Regu) 0.1915 (0.1915)\tPrec@1 81.250 (83.225)\tPrec@5 98.438 (98.329)\n",
            "Epoch: [102][250/782]\tTime 0.024 (0.024)\tLoss (Class) 0.8669 (0.7398)\tLoss (Regu) 0.1946 (0.1909)\tPrec@1 78.125 (82.775)\tPrec@5 100.000 (98.288)\n",
            "Epoch: [102][300/782]\tTime 0.022 (0.024)\tLoss (Class) 0.8948 (0.7440)\tLoss (Regu) 0.1879 (0.1908)\tPrec@1 81.250 (82.579)\tPrec@5 100.000 (98.235)\n",
            "Epoch: [102][350/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7254 (0.7408)\tLoss (Regu) 0.1894 (0.1907)\tPrec@1 82.812 (82.675)\tPrec@5 96.875 (98.268)\n",
            "Epoch: [102][400/782]\tTime 0.032 (0.024)\tLoss (Class) 0.8016 (0.7446)\tLoss (Regu) 0.1922 (0.1907)\tPrec@1 76.562 (82.489)\tPrec@5 98.438 (98.208)\n",
            "Epoch: [102][450/782]\tTime 0.024 (0.025)\tLoss (Class) 0.7814 (0.7444)\tLoss (Regu) 0.1927 (0.1907)\tPrec@1 78.125 (82.525)\tPrec@5 98.438 (98.171)\n",
            "Epoch: [102][500/782]\tTime 0.033 (0.025)\tLoss (Class) 0.6658 (0.7501)\tLoss (Regu) 0.1879 (0.1904)\tPrec@1 81.250 (82.320)\tPrec@5 100.000 (98.157)\n",
            "Epoch: [102][550/782]\tTime 0.023 (0.025)\tLoss (Class) 0.6221 (0.7525)\tLoss (Regu) 0.1851 (0.1903)\tPrec@1 87.500 (82.203)\tPrec@5 96.875 (98.137)\n",
            "Epoch: [102][600/782]\tTime 0.023 (0.025)\tLoss (Class) 0.7139 (0.7538)\tLoss (Regu) 0.1896 (0.1903)\tPrec@1 85.938 (82.152)\tPrec@5 98.438 (98.144)\n",
            "Epoch: [102][650/782]\tTime 0.024 (0.025)\tLoss (Class) 0.7094 (0.7565)\tLoss (Regu) 0.1863 (0.1902)\tPrec@1 84.375 (82.076)\tPrec@5 98.438 (98.118)\n",
            "Epoch: [102][700/782]\tTime 0.032 (0.025)\tLoss (Class) 0.7367 (0.7577)\tLoss (Regu) 0.1866 (0.1901)\tPrec@1 82.812 (82.059)\tPrec@5 98.438 (98.114)\n",
            "Epoch: [102][750/782]\tTime 0.023 (0.025)\tLoss (Class) 1.0891 (0.7607)\tLoss (Regu) 0.1896 (0.1901)\tPrec@1 67.188 (81.941)\tPrec@5 95.312 (98.075)\n",
            "Test: [0/157]\tTime 0.106 (0.106)\tLoss (Class) 1.8265 (1.8265)\tLoss (Regu) 0.2132 (0.2132)\tPrec@1 56.250 (56.250)\tPrec@5 84.375 (84.375)\n",
            "Test: [50/157]\tTime 0.008 (0.010)\tLoss (Class) 1.6837 (1.7167)\tLoss (Regu) 0.2171 (0.2143)\tPrec@1 64.062 (63.879)\tPrec@5 89.062 (88.419)\n",
            "Test: [100/157]\tTime 0.008 (0.009)\tLoss (Class) 1.6796 (1.6957)\tLoss (Regu) 0.2123 (0.2145)\tPrec@1 60.938 (63.800)\tPrec@5 84.375 (88.475)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.7077 (1.6929)\tLoss (Regu) 0.2184 (0.2141)\tPrec@1 59.375 (64.031)\tPrec@5 92.188 (88.711)\n",
            " * Train[81.886 %, 98.028 %, 0.764 loss] Val [64.040 %, 88.780%, 1.694 loss] Best: 64.710 %\n",
            "Time for 102 / 150 20.915405750274658\n",
            "Learning rate:  0.03\n",
            "Epoch: [103][0/782]\tTime 0.166 (0.166)\tLoss (Class) 0.5413 (0.5413)\tLoss (Regu) 0.1916 (0.1916)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [103][50/782]\tTime 0.022 (0.027)\tLoss (Class) 0.6972 (0.7176)\tLoss (Regu) 0.1960 (0.1929)\tPrec@1 82.812 (83.609)\tPrec@5 98.438 (98.100)\n",
            "Epoch: [103][100/782]\tTime 0.022 (0.025)\tLoss (Class) 0.7160 (0.7095)\tLoss (Regu) 0.1929 (0.1918)\tPrec@1 82.812 (83.725)\tPrec@5 100.000 (98.376)\n",
            "Epoch: [103][150/782]\tTime 0.023 (0.025)\tLoss (Class) 0.8570 (0.7192)\tLoss (Regu) 0.1877 (0.1915)\tPrec@1 76.562 (83.568)\tPrec@5 100.000 (98.293)\n",
            "Epoch: [103][200/782]\tTime 0.023 (0.025)\tLoss (Class) 0.6546 (0.7174)\tLoss (Regu) 0.1881 (0.1914)\tPrec@1 85.938 (83.776)\tPrec@5 98.438 (98.298)\n",
            "Epoch: [103][250/782]\tTime 0.023 (0.025)\tLoss (Class) 0.8276 (0.7225)\tLoss (Regu) 0.1919 (0.1909)\tPrec@1 78.125 (83.560)\tPrec@5 96.875 (98.238)\n",
            "Epoch: [103][300/782]\tTime 0.023 (0.025)\tLoss (Class) 0.7177 (0.7292)\tLoss (Regu) 0.1898 (0.1909)\tPrec@1 84.375 (83.321)\tPrec@5 98.438 (98.152)\n",
            "Epoch: [103][350/782]\tTime 0.023 (0.025)\tLoss (Class) 0.7102 (0.7364)\tLoss (Regu) 0.1896 (0.1905)\tPrec@1 79.688 (83.048)\tPrec@5 96.875 (98.086)\n",
            "Epoch: [103][400/782]\tTime 0.023 (0.025)\tLoss (Class) 0.7724 (0.7377)\tLoss (Regu) 0.1896 (0.1903)\tPrec@1 84.375 (82.957)\tPrec@5 98.438 (98.118)\n",
            "Epoch: [103][450/782]\tTime 0.023 (0.025)\tLoss (Class) 0.8098 (0.7439)\tLoss (Regu) 0.1913 (0.1904)\tPrec@1 87.500 (82.767)\tPrec@5 95.312 (98.060)\n",
            "Epoch: [103][500/782]\tTime 0.023 (0.025)\tLoss (Class) 1.0768 (0.7476)\tLoss (Regu) 0.1843 (0.1902)\tPrec@1 71.875 (82.638)\tPrec@5 96.875 (98.023)\n",
            "Epoch: [103][550/782]\tTime 0.023 (0.025)\tLoss (Class) 0.7172 (0.7512)\tLoss (Regu) 0.1889 (0.1900)\tPrec@1 79.688 (82.498)\tPrec@5 98.438 (98.012)\n",
            "Epoch: [103][600/782]\tTime 0.023 (0.025)\tLoss (Class) 0.6414 (0.7543)\tLoss (Regu) 0.1878 (0.1899)\tPrec@1 87.500 (82.371)\tPrec@5 96.875 (98.009)\n",
            "Epoch: [103][650/782]\tTime 0.023 (0.025)\tLoss (Class) 0.7729 (0.7555)\tLoss (Regu) 0.1892 (0.1898)\tPrec@1 84.375 (82.354)\tPrec@5 92.188 (97.989)\n",
            "Epoch: [103][700/782]\tTime 0.030 (0.025)\tLoss (Class) 0.8676 (0.7605)\tLoss (Regu) 0.1853 (0.1896)\tPrec@1 78.125 (82.168)\tPrec@5 96.875 (97.947)\n",
            "Epoch: [103][750/782]\tTime 0.023 (0.025)\tLoss (Class) 0.6953 (0.7643)\tLoss (Regu) 0.1833 (0.1895)\tPrec@1 87.500 (82.007)\tPrec@5 100.000 (97.911)\n",
            "Test: [0/157]\tTime 0.108 (0.108)\tLoss (Class) 2.1961 (2.1961)\tLoss (Regu) 0.2181 (0.2181)\tPrec@1 50.000 (50.000)\tPrec@5 82.812 (82.812)\n",
            "Test: [50/157]\tTime 0.008 (0.010)\tLoss (Class) 1.5054 (1.7185)\tLoss (Regu) 0.2183 (0.2185)\tPrec@1 64.062 (63.603)\tPrec@5 92.188 (88.388)\n",
            "Test: [100/157]\tTime 0.009 (0.009)\tLoss (Class) 1.5817 (1.6875)\tLoss (Regu) 0.2221 (0.2183)\tPrec@1 67.188 (64.356)\tPrec@5 92.188 (88.939)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.7497 (1.7209)\tLoss (Regu) 0.2212 (0.2184)\tPrec@1 62.500 (63.700)\tPrec@5 87.500 (88.659)\n",
            " * Train[82.010 %, 97.906 %, 0.765 loss] Val [63.810 %, 88.700%, 1.715 loss] Best: 64.710 %\n",
            "Time for 103 / 150 20.692713022232056\n",
            "Learning rate:  0.03\n",
            "Epoch: [104][0/782]\tTime 0.147 (0.147)\tLoss (Class) 0.6627 (0.6627)\tLoss (Regu) 0.1919 (0.1919)\tPrec@1 81.250 (81.250)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [104][50/782]\tTime 0.023 (0.025)\tLoss (Class) 0.7087 (0.7290)\tLoss (Regu) 0.1893 (0.1903)\tPrec@1 79.688 (82.506)\tPrec@5 100.000 (98.162)\n",
            "Epoch: [104][100/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7673 (0.7309)\tLoss (Regu) 0.1901 (0.1897)\tPrec@1 79.688 (82.859)\tPrec@5 93.750 (98.128)\n",
            "Epoch: [104][150/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6250 (0.7279)\tLoss (Regu) 0.1893 (0.1891)\tPrec@1 84.375 (83.071)\tPrec@5 100.000 (98.251)\n",
            "Epoch: [104][200/782]\tTime 0.027 (0.024)\tLoss (Class) 0.6114 (0.7267)\tLoss (Regu) 0.1889 (0.1898)\tPrec@1 81.250 (83.147)\tPrec@5 100.000 (98.336)\n",
            "Epoch: [104][250/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6951 (0.7195)\tLoss (Regu) 0.1873 (0.1893)\tPrec@1 82.812 (83.205)\tPrec@5 100.000 (98.375)\n",
            "Epoch: [104][300/782]\tTime 0.029 (0.024)\tLoss (Class) 0.6027 (0.7213)\tLoss (Regu) 0.1908 (0.1893)\tPrec@1 90.625 (83.165)\tPrec@5 96.875 (98.308)\n",
            "Epoch: [104][350/782]\tTime 0.022 (0.024)\tLoss (Class) 0.5851 (0.7293)\tLoss (Regu) 0.1885 (0.1890)\tPrec@1 89.062 (82.844)\tPrec@5 100.000 (98.277)\n",
            "Epoch: [104][400/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7128 (0.7344)\tLoss (Regu) 0.1876 (0.1889)\tPrec@1 85.938 (82.649)\tPrec@5 98.438 (98.231)\n",
            "Epoch: [104][450/782]\tTime 0.022 (0.024)\tLoss (Class) 0.6947 (0.7376)\tLoss (Regu) 0.1902 (0.1889)\tPrec@1 81.250 (82.605)\tPrec@5 100.000 (98.185)\n",
            "Epoch: [104][500/782]\tTime 0.024 (0.024)\tLoss (Class) 0.7013 (0.7418)\tLoss (Regu) 0.1909 (0.1890)\tPrec@1 82.812 (82.519)\tPrec@5 100.000 (98.132)\n",
            "Epoch: [104][550/782]\tTime 0.021 (0.024)\tLoss (Class) 0.6964 (0.7429)\tLoss (Regu) 0.1940 (0.1892)\tPrec@1 79.688 (82.481)\tPrec@5 98.438 (98.123)\n",
            "Epoch: [104][600/782]\tTime 0.032 (0.024)\tLoss (Class) 0.6897 (0.7473)\tLoss (Regu) 0.1874 (0.1891)\tPrec@1 84.375 (82.313)\tPrec@5 100.000 (98.115)\n",
            "Epoch: [104][650/782]\tTime 0.024 (0.024)\tLoss (Class) 0.9480 (0.7506)\tLoss (Regu) 0.1917 (0.1891)\tPrec@1 73.438 (82.246)\tPrec@5 98.438 (98.101)\n",
            "Epoch: [104][700/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6658 (0.7524)\tLoss (Regu) 0.1907 (0.1892)\tPrec@1 84.375 (82.166)\tPrec@5 98.438 (98.103)\n",
            "Epoch: [104][750/782]\tTime 0.022 (0.024)\tLoss (Class) 0.8017 (0.7558)\tLoss (Regu) 0.1913 (0.1892)\tPrec@1 78.125 (82.034)\tPrec@5 98.438 (98.107)\n",
            "Test: [0/157]\tTime 0.117 (0.117)\tLoss (Class) 1.6221 (1.6221)\tLoss (Regu) 0.2131 (0.2131)\tPrec@1 68.750 (68.750)\tPrec@5 90.625 (90.625)\n",
            "Test: [50/157]\tTime 0.008 (0.010)\tLoss (Class) 1.4169 (1.8193)\tLoss (Regu) 0.2126 (0.2148)\tPrec@1 70.312 (63.021)\tPrec@5 92.188 (87.469)\n",
            "Test: [100/157]\tTime 0.008 (0.009)\tLoss (Class) 1.9295 (1.8434)\tLoss (Regu) 0.2198 (0.2147)\tPrec@1 65.625 (61.788)\tPrec@5 84.375 (87.577)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 2.5258 (1.8334)\tLoss (Regu) 0.2163 (0.2145)\tPrec@1 54.688 (62.024)\tPrec@5 81.250 (87.614)\n",
            " * Train[81.998 %, 98.088 %, 0.757 loss] Val [61.960 %, 87.590%, 1.833 loss] Best: 64.710 %\n",
            "Time for 104 / 150 20.128944158554077\n",
            "Learning rate:  0.03\n",
            "Epoch: [105][0/782]\tTime 0.153 (0.153)\tLoss (Class) 0.7932 (0.7932)\tLoss (Regu) 0.1917 (0.1917)\tPrec@1 79.688 (79.688)\tPrec@5 98.438 (98.438)\n",
            "Epoch: [105][50/782]\tTime 0.022 (0.027)\tLoss (Class) 0.6045 (0.7350)\tLoss (Regu) 0.1879 (0.1871)\tPrec@1 85.938 (82.384)\tPrec@5 100.000 (98.254)\n",
            "Epoch: [105][100/782]\tTime 0.023 (0.025)\tLoss (Class) 0.6341 (0.7336)\tLoss (Regu) 0.1886 (0.1872)\tPrec@1 84.375 (82.782)\tPrec@5 98.438 (98.221)\n",
            "Epoch: [105][150/782]\tTime 0.024 (0.025)\tLoss (Class) 0.6646 (0.7236)\tLoss (Regu) 0.1950 (0.1893)\tPrec@1 89.062 (83.144)\tPrec@5 98.438 (98.355)\n",
            "Epoch: [105][200/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6160 (0.7296)\tLoss (Regu) 0.1888 (0.1893)\tPrec@1 82.812 (82.820)\tPrec@5 100.000 (98.344)\n",
            "Epoch: [105][250/782]\tTime 0.024 (0.024)\tLoss (Class) 0.8098 (0.7295)\tLoss (Regu) 0.1887 (0.1897)\tPrec@1 76.562 (82.837)\tPrec@5 96.875 (98.332)\n",
            "Epoch: [105][300/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6617 (0.7248)\tLoss (Regu) 0.1899 (0.1898)\tPrec@1 85.938 (82.979)\tPrec@5 98.438 (98.370)\n",
            "Epoch: [105][350/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7520 (0.7282)\tLoss (Regu) 0.1917 (0.1901)\tPrec@1 79.688 (82.772)\tPrec@5 98.438 (98.371)\n",
            "Epoch: [105][400/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6453 (0.7336)\tLoss (Regu) 0.1847 (0.1897)\tPrec@1 85.938 (82.583)\tPrec@5 100.000 (98.317)\n",
            "Epoch: [105][450/782]\tTime 0.022 (0.024)\tLoss (Class) 0.6879 (0.7355)\tLoss (Regu) 0.1847 (0.1895)\tPrec@1 79.688 (82.567)\tPrec@5 100.000 (98.295)\n",
            "Epoch: [105][500/782]\tTime 0.022 (0.024)\tLoss (Class) 0.7511 (0.7383)\tLoss (Regu) 0.1896 (0.1893)\tPrec@1 87.500 (82.535)\tPrec@5 96.875 (98.263)\n",
            "Epoch: [105][550/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7624 (0.7385)\tLoss (Regu) 0.1924 (0.1894)\tPrec@1 79.688 (82.574)\tPrec@5 96.875 (98.267)\n",
            "Epoch: [105][600/782]\tTime 0.023 (0.024)\tLoss (Class) 0.9451 (0.7431)\tLoss (Regu) 0.1865 (0.1893)\tPrec@1 84.375 (82.430)\tPrec@5 95.312 (98.222)\n",
            "Epoch: [105][650/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6512 (0.7457)\tLoss (Regu) 0.1891 (0.1891)\tPrec@1 89.062 (82.412)\tPrec@5 98.438 (98.202)\n",
            "Epoch: [105][700/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7049 (0.7493)\tLoss (Regu) 0.1943 (0.1893)\tPrec@1 81.250 (82.266)\tPrec@5 98.438 (98.174)\n",
            "Epoch: [105][750/782]\tTime 0.023 (0.023)\tLoss (Class) 1.0910 (0.7520)\tLoss (Regu) 0.1861 (0.1892)\tPrec@1 70.312 (82.184)\tPrec@5 96.875 (98.142)\n",
            "Test: [0/157]\tTime 0.124 (0.124)\tLoss (Class) 2.3306 (2.3306)\tLoss (Regu) 0.2151 (0.2151)\tPrec@1 56.250 (56.250)\tPrec@5 85.938 (85.938)\n",
            "Test: [50/157]\tTime 0.009 (0.011)\tLoss (Class) 1.7863 (1.7604)\tLoss (Regu) 0.2106 (0.2098)\tPrec@1 64.062 (64.001)\tPrec@5 89.062 (87.776)\n",
            "Test: [100/157]\tTime 0.009 (0.010)\tLoss (Class) 1.5259 (1.7406)\tLoss (Regu) 0.2125 (0.2099)\tPrec@1 67.188 (63.800)\tPrec@5 85.938 (88.103)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.5027 (1.7308)\tLoss (Regu) 0.2099 (0.2102)\tPrec@1 60.938 (64.218)\tPrec@5 89.062 (88.059)\n",
            " * Train[82.070 %, 98.144 %, 0.755 loss] Val [64.030 %, 88.030%, 1.735 loss] Best: 64.710 %\n",
            "Time for 105 / 150 19.880298852920532\n",
            "Learning rate:  0.03\n",
            "Epoch: [106][0/782]\tTime 0.157 (0.157)\tLoss (Class) 0.8338 (0.8338)\tLoss (Regu) 0.1886 (0.1886)\tPrec@1 81.250 (81.250)\tPrec@5 95.312 (95.312)\n",
            "Epoch: [106][50/782]\tTime 0.023 (0.025)\tLoss (Class) 0.6963 (0.7359)\tLoss (Regu) 0.1891 (0.1906)\tPrec@1 84.375 (82.812)\tPrec@5 98.438 (98.407)\n",
            "Epoch: [106][100/782]\tTime 0.022 (0.024)\tLoss (Class) 0.5885 (0.7218)\tLoss (Regu) 0.1952 (0.1906)\tPrec@1 85.938 (82.967)\tPrec@5 100.000 (98.407)\n",
            "Epoch: [106][150/782]\tTime 0.022 (0.023)\tLoss (Class) 0.7667 (0.7173)\tLoss (Regu) 0.1911 (0.1910)\tPrec@1 84.375 (83.454)\tPrec@5 98.438 (98.489)\n",
            "Epoch: [106][200/782]\tTime 0.023 (0.023)\tLoss (Class) 0.7734 (0.7146)\tLoss (Regu) 0.1869 (0.1907)\tPrec@1 76.562 (83.551)\tPrec@5 100.000 (98.554)\n",
            "Epoch: [106][250/782]\tTime 0.023 (0.023)\tLoss (Class) 1.1342 (0.7120)\tLoss (Regu) 0.1860 (0.1901)\tPrec@1 73.438 (83.553)\tPrec@5 90.625 (98.537)\n",
            "Epoch: [106][300/782]\tTime 0.022 (0.024)\tLoss (Class) 0.6853 (0.7186)\tLoss (Regu) 0.1838 (0.1897)\tPrec@1 85.938 (83.373)\tPrec@5 96.875 (98.479)\n",
            "Epoch: [106][350/782]\tTime 0.024 (0.024)\tLoss (Class) 0.7565 (0.7188)\tLoss (Regu) 0.1955 (0.1894)\tPrec@1 81.250 (83.320)\tPrec@5 98.438 (98.460)\n",
            "Epoch: [106][400/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7746 (0.7250)\tLoss (Regu) 0.1907 (0.1895)\tPrec@1 78.125 (83.171)\tPrec@5 96.875 (98.360)\n",
            "Epoch: [106][450/782]\tTime 0.024 (0.024)\tLoss (Class) 0.6987 (0.7277)\tLoss (Regu) 0.1887 (0.1894)\tPrec@1 79.688 (83.062)\tPrec@5 100.000 (98.330)\n",
            "Epoch: [106][500/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6003 (0.7306)\tLoss (Regu) 0.1892 (0.1894)\tPrec@1 85.938 (83.018)\tPrec@5 96.875 (98.253)\n",
            "Epoch: [106][550/782]\tTime 0.024 (0.024)\tLoss (Class) 0.5901 (0.7355)\tLoss (Regu) 0.1878 (0.1893)\tPrec@1 85.938 (82.810)\tPrec@5 98.438 (98.233)\n",
            "Epoch: [106][600/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8783 (0.7395)\tLoss (Regu) 0.1870 (0.1894)\tPrec@1 81.250 (82.711)\tPrec@5 95.312 (98.198)\n",
            "Epoch: [106][650/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6887 (0.7411)\tLoss (Regu) 0.1918 (0.1893)\tPrec@1 82.812 (82.647)\tPrec@5 100.000 (98.183)\n",
            "Epoch: [106][700/782]\tTime 0.030 (0.024)\tLoss (Class) 0.6318 (0.7438)\tLoss (Regu) 0.1901 (0.1895)\tPrec@1 84.375 (82.552)\tPrec@5 96.875 (98.177)\n",
            "Epoch: [106][750/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7230 (0.7479)\tLoss (Regu) 0.1850 (0.1895)\tPrec@1 81.250 (82.448)\tPrec@5 96.875 (98.140)\n",
            "Test: [0/157]\tTime 0.117 (0.117)\tLoss (Class) 0.9830 (0.9830)\tLoss (Regu) 0.2197 (0.2197)\tPrec@1 75.000 (75.000)\tPrec@5 96.875 (96.875)\n",
            "Test: [50/157]\tTime 0.013 (0.013)\tLoss (Class) 1.3956 (1.6208)\tLoss (Regu) 0.2150 (0.2167)\tPrec@1 65.625 (65.165)\tPrec@5 90.625 (89.491)\n",
            "Test: [100/157]\tTime 0.008 (0.011)\tLoss (Class) 1.6806 (1.6566)\tLoss (Regu) 0.2164 (0.2168)\tPrec@1 60.938 (64.650)\tPrec@5 87.500 (89.310)\n",
            "Test: [150/157]\tTime 0.007 (0.010)\tLoss (Class) 2.1331 (1.6593)\tLoss (Regu) 0.2128 (0.2167)\tPrec@1 45.312 (64.994)\tPrec@5 84.375 (89.176)\n",
            " * Train[82.376 %, 98.132 %, 0.749 loss] Val [64.960 %, 89.110%, 1.662 loss] Best: 64.960 %\n",
            "Time for 106 / 150 20.28315758705139\n",
            "Learning rate:  0.03\n",
            "Epoch: [107][0/782]\tTime 0.152 (0.152)\tLoss (Class) 0.6438 (0.6438)\tLoss (Regu) 0.1915 (0.1915)\tPrec@1 85.938 (85.938)\tPrec@5 98.438 (98.438)\n",
            "Epoch: [107][50/782]\tTime 0.022 (0.026)\tLoss (Class) 0.6018 (0.6752)\tLoss (Regu) 0.1882 (0.1906)\tPrec@1 89.062 (84.773)\tPrec@5 98.438 (98.468)\n",
            "Epoch: [107][100/782]\tTime 0.024 (0.026)\tLoss (Class) 0.6628 (0.7009)\tLoss (Regu) 0.1894 (0.1904)\tPrec@1 87.500 (83.880)\tPrec@5 100.000 (98.345)\n",
            "Epoch: [107][150/782]\tTime 0.023 (0.026)\tLoss (Class) 0.5879 (0.7161)\tLoss (Regu) 0.1905 (0.1903)\tPrec@1 92.188 (83.454)\tPrec@5 100.000 (98.375)\n",
            "Epoch: [107][200/782]\tTime 0.032 (0.026)\tLoss (Class) 0.7339 (0.7177)\tLoss (Regu) 0.1879 (0.1900)\tPrec@1 82.812 (83.302)\tPrec@5 100.000 (98.461)\n",
            "Epoch: [107][250/782]\tTime 0.022 (0.026)\tLoss (Class) 0.6556 (0.7153)\tLoss (Regu) 0.1899 (0.1900)\tPrec@1 82.812 (83.354)\tPrec@5 100.000 (98.500)\n",
            "Epoch: [107][300/782]\tTime 0.032 (0.025)\tLoss (Class) 0.6988 (0.7171)\tLoss (Regu) 0.1861 (0.1899)\tPrec@1 87.500 (83.368)\tPrec@5 100.000 (98.432)\n",
            "Epoch: [107][350/782]\tTime 0.022 (0.025)\tLoss (Class) 0.7020 (0.7183)\tLoss (Regu) 0.1896 (0.1900)\tPrec@1 82.812 (83.333)\tPrec@5 98.438 (98.464)\n",
            "Epoch: [107][400/782]\tTime 0.031 (0.025)\tLoss (Class) 0.8595 (0.7246)\tLoss (Regu) 0.1931 (0.1898)\tPrec@1 73.438 (83.113)\tPrec@5 98.438 (98.371)\n",
            "Epoch: [107][450/782]\tTime 0.022 (0.025)\tLoss (Class) 0.6348 (0.7294)\tLoss (Regu) 0.1887 (0.1897)\tPrec@1 84.375 (82.948)\tPrec@5 98.438 (98.361)\n",
            "Epoch: [107][500/782]\tTime 0.023 (0.025)\tLoss (Class) 1.0316 (0.7334)\tLoss (Regu) 0.1901 (0.1896)\tPrec@1 76.562 (82.875)\tPrec@5 100.000 (98.316)\n",
            "Epoch: [107][550/782]\tTime 0.023 (0.025)\tLoss (Class) 0.6803 (0.7369)\tLoss (Regu) 0.1890 (0.1895)\tPrec@1 82.812 (82.761)\tPrec@5 98.438 (98.299)\n",
            "Epoch: [107][600/782]\tTime 0.023 (0.025)\tLoss (Class) 0.6121 (0.7396)\tLoss (Regu) 0.1895 (0.1894)\tPrec@1 89.062 (82.641)\tPrec@5 100.000 (98.297)\n",
            "Epoch: [107][650/782]\tTime 0.022 (0.025)\tLoss (Class) 0.8415 (0.7440)\tLoss (Regu) 0.1891 (0.1893)\tPrec@1 78.125 (82.488)\tPrec@5 98.438 (98.260)\n",
            "Epoch: [107][700/782]\tTime 0.031 (0.025)\tLoss (Class) 0.8654 (0.7475)\tLoss (Regu) 0.1904 (0.1892)\tPrec@1 73.438 (82.434)\tPrec@5 96.875 (98.181)\n",
            "Epoch: [107][750/782]\tTime 0.024 (0.025)\tLoss (Class) 0.9609 (0.7494)\tLoss (Regu) 0.1907 (0.1892)\tPrec@1 75.000 (82.365)\tPrec@5 98.438 (98.167)\n",
            "Test: [0/157]\tTime 0.111 (0.111)\tLoss (Class) 1.3905 (1.3905)\tLoss (Regu) 0.2207 (0.2207)\tPrec@1 71.875 (71.875)\tPrec@5 93.750 (93.750)\n",
            "Test: [50/157]\tTime 0.009 (0.012)\tLoss (Class) 1.4221 (1.6729)\tLoss (Regu) 0.2141 (0.2145)\tPrec@1 65.625 (65.012)\tPrec@5 95.312 (89.093)\n",
            "Test: [100/157]\tTime 0.009 (0.010)\tLoss (Class) 1.5520 (1.6731)\tLoss (Regu) 0.2113 (0.2144)\tPrec@1 68.750 (64.759)\tPrec@5 92.188 (89.279)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.4251 (1.6694)\tLoss (Regu) 0.2170 (0.2143)\tPrec@1 76.562 (64.787)\tPrec@5 89.062 (89.104)\n",
            " * Train[82.276 %, 98.150 %, 0.752 loss] Val [64.810 %, 89.040%, 1.668 loss] Best: 64.960 %\n",
            "Time for 107 / 150 20.75076913833618\n",
            "Learning rate:  0.03\n",
            "Epoch: [108][0/782]\tTime 0.144 (0.144)\tLoss (Class) 0.5409 (0.5409)\tLoss (Regu) 0.1869 (0.1869)\tPrec@1 87.500 (87.500)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [108][50/782]\tTime 0.032 (0.027)\tLoss (Class) 0.6291 (0.6835)\tLoss (Regu) 0.1925 (0.1905)\tPrec@1 90.625 (85.141)\tPrec@5 98.438 (98.713)\n",
            "Epoch: [108][100/782]\tTime 0.022 (0.025)\tLoss (Class) 0.7003 (0.6955)\tLoss (Regu) 0.1899 (0.1909)\tPrec@1 82.812 (84.684)\tPrec@5 100.000 (98.530)\n",
            "Epoch: [108][150/782]\tTime 0.030 (0.025)\tLoss (Class) 0.6583 (0.6962)\tLoss (Regu) 0.1852 (0.1901)\tPrec@1 87.500 (84.582)\tPrec@5 96.875 (98.469)\n",
            "Epoch: [108][200/782]\tTime 0.023 (0.025)\tLoss (Class) 0.8078 (0.6993)\tLoss (Regu) 0.1868 (0.1899)\tPrec@1 81.250 (84.375)\tPrec@5 98.438 (98.336)\n",
            "Epoch: [108][250/782]\tTime 0.031 (0.025)\tLoss (Class) 0.5293 (0.7036)\tLoss (Regu) 0.1877 (0.1896)\tPrec@1 87.500 (84.145)\tPrec@5 100.000 (98.276)\n",
            "Epoch: [108][300/782]\tTime 0.032 (0.025)\tLoss (Class) 0.6850 (0.7114)\tLoss (Regu) 0.1898 (0.1894)\tPrec@1 85.938 (83.799)\tPrec@5 96.875 (98.245)\n",
            "Epoch: [108][350/782]\tTime 0.022 (0.025)\tLoss (Class) 0.6831 (0.7134)\tLoss (Regu) 0.1876 (0.1893)\tPrec@1 85.938 (83.707)\tPrec@5 96.875 (98.259)\n",
            "Epoch: [108][400/782]\tTime 0.030 (0.025)\tLoss (Class) 0.7042 (0.7170)\tLoss (Regu) 0.1829 (0.1891)\tPrec@1 82.812 (83.596)\tPrec@5 100.000 (98.250)\n",
            "Epoch: [108][450/782]\tTime 0.024 (0.025)\tLoss (Class) 0.5942 (0.7198)\tLoss (Regu) 0.1877 (0.1889)\tPrec@1 85.938 (83.498)\tPrec@5 100.000 (98.240)\n",
            "Epoch: [108][500/782]\tTime 0.023 (0.025)\tLoss (Class) 0.8221 (0.7251)\tLoss (Regu) 0.1910 (0.1890)\tPrec@1 85.938 (83.305)\tPrec@5 96.875 (98.200)\n",
            "Epoch: [108][550/782]\tTime 0.023 (0.025)\tLoss (Class) 0.8723 (0.7321)\tLoss (Regu) 0.1867 (0.1891)\tPrec@1 81.250 (83.068)\tPrec@5 96.875 (98.188)\n",
            "Epoch: [108][600/782]\tTime 0.024 (0.025)\tLoss (Class) 0.7472 (0.7372)\tLoss (Regu) 0.1889 (0.1890)\tPrec@1 81.250 (82.890)\tPrec@5 98.438 (98.162)\n",
            "Epoch: [108][650/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8042 (0.7423)\tLoss (Regu) 0.1901 (0.1890)\tPrec@1 84.375 (82.697)\tPrec@5 95.312 (98.130)\n",
            "Epoch: [108][700/782]\tTime 0.021 (0.024)\tLoss (Class) 0.6690 (0.7470)\tLoss (Regu) 0.1922 (0.1889)\tPrec@1 84.375 (82.534)\tPrec@5 98.438 (98.094)\n",
            "Epoch: [108][750/782]\tTime 0.024 (0.025)\tLoss (Class) 0.7796 (0.7487)\tLoss (Regu) 0.1901 (0.1891)\tPrec@1 81.250 (82.471)\tPrec@5 98.438 (98.075)\n",
            "Test: [0/157]\tTime 0.110 (0.110)\tLoss (Class) 2.0473 (2.0473)\tLoss (Regu) 0.2105 (0.2105)\tPrec@1 64.062 (64.062)\tPrec@5 82.812 (82.812)\n",
            "Test: [50/157]\tTime 0.008 (0.011)\tLoss (Class) 1.6095 (1.7670)\tLoss (Regu) 0.2116 (0.2158)\tPrec@1 64.062 (62.286)\tPrec@5 89.062 (88.572)\n",
            "Test: [100/157]\tTime 0.008 (0.009)\tLoss (Class) 1.6924 (1.7745)\tLoss (Regu) 0.2129 (0.2157)\tPrec@1 65.625 (62.144)\tPrec@5 92.188 (88.274)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.5284 (1.7473)\tLoss (Regu) 0.2155 (0.2157)\tPrec@1 64.062 (62.769)\tPrec@5 92.188 (88.566)\n",
            " * Train[82.402 %, 98.068 %, 0.750 loss] Val [62.740 %, 88.550%, 1.746 loss] Best: 64.960 %\n",
            "Time for 108 / 150 20.640257358551025\n",
            "Learning rate:  0.03\n",
            "Epoch: [109][0/782]\tTime 0.145 (0.145)\tLoss (Class) 0.7128 (0.7128)\tLoss (Regu) 0.1894 (0.1894)\tPrec@1 82.812 (82.812)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [109][50/782]\tTime 0.022 (0.027)\tLoss (Class) 0.6207 (0.7064)\tLoss (Regu) 0.1886 (0.1909)\tPrec@1 87.500 (83.977)\tPrec@5 98.438 (98.284)\n",
            "Epoch: [109][100/782]\tTime 0.031 (0.026)\tLoss (Class) 0.9955 (0.7033)\tLoss (Regu) 0.1930 (0.1915)\tPrec@1 68.750 (83.601)\tPrec@5 95.312 (98.407)\n",
            "Epoch: [109][150/782]\tTime 0.030 (0.026)\tLoss (Class) 0.7703 (0.7127)\tLoss (Regu) 0.1903 (0.1910)\tPrec@1 78.125 (83.402)\tPrec@5 100.000 (98.396)\n",
            "Epoch: [109][200/782]\tTime 0.025 (0.025)\tLoss (Class) 0.6968 (0.7247)\tLoss (Regu) 0.1886 (0.1907)\tPrec@1 82.812 (83.038)\tPrec@5 95.312 (98.352)\n",
            "Epoch: [109][250/782]\tTime 0.023 (0.025)\tLoss (Class) 0.8440 (0.7242)\tLoss (Regu) 0.1901 (0.1902)\tPrec@1 82.812 (82.949)\tPrec@5 96.875 (98.307)\n",
            "Epoch: [109][300/782]\tTime 0.023 (0.025)\tLoss (Class) 0.7758 (0.7237)\tLoss (Regu) 0.1888 (0.1899)\tPrec@1 82.812 (82.968)\tPrec@5 96.875 (98.318)\n",
            "Epoch: [109][350/782]\tTime 0.023 (0.025)\tLoss (Class) 0.6111 (0.7324)\tLoss (Regu) 0.1895 (0.1898)\tPrec@1 84.375 (82.670)\tPrec@5 98.438 (98.291)\n",
            "Epoch: [109][400/782]\tTime 0.023 (0.025)\tLoss (Class) 0.8853 (0.7357)\tLoss (Regu) 0.1876 (0.1898)\tPrec@1 73.438 (82.602)\tPrec@5 96.875 (98.262)\n",
            "Epoch: [109][450/782]\tTime 0.024 (0.025)\tLoss (Class) 0.6936 (0.7371)\tLoss (Regu) 0.1877 (0.1899)\tPrec@1 84.375 (82.577)\tPrec@5 100.000 (98.268)\n",
            "Epoch: [109][500/782]\tTime 0.023 (0.025)\tLoss (Class) 0.7254 (0.7421)\tLoss (Regu) 0.1894 (0.1898)\tPrec@1 85.938 (82.432)\tPrec@5 98.438 (98.232)\n",
            "Epoch: [109][550/782]\tTime 0.022 (0.025)\tLoss (Class) 0.8842 (0.7464)\tLoss (Regu) 0.1888 (0.1899)\tPrec@1 78.125 (82.339)\tPrec@5 96.875 (98.191)\n",
            "Epoch: [109][600/782]\tTime 0.023 (0.025)\tLoss (Class) 0.6957 (0.7493)\tLoss (Regu) 0.1911 (0.1900)\tPrec@1 82.812 (82.202)\tPrec@5 96.875 (98.157)\n",
            "Epoch: [109][650/782]\tTime 0.022 (0.025)\tLoss (Class) 0.6726 (0.7517)\tLoss (Regu) 0.1864 (0.1900)\tPrec@1 89.062 (82.160)\tPrec@5 100.000 (98.121)\n",
            "Epoch: [109][700/782]\tTime 0.024 (0.025)\tLoss (Class) 0.7987 (0.7549)\tLoss (Regu) 0.1836 (0.1899)\tPrec@1 81.250 (82.066)\tPrec@5 98.438 (98.079)\n",
            "Epoch: [109][750/782]\tTime 0.030 (0.025)\tLoss (Class) 1.0116 (0.7574)\tLoss (Regu) 0.1856 (0.1899)\tPrec@1 75.000 (82.005)\tPrec@5 96.875 (98.075)\n",
            "Test: [0/157]\tTime 0.107 (0.107)\tLoss (Class) 1.6109 (1.6109)\tLoss (Regu) 0.2125 (0.2125)\tPrec@1 70.312 (70.312)\tPrec@5 92.188 (92.188)\n",
            "Test: [50/157]\tTime 0.008 (0.010)\tLoss (Class) 1.2933 (1.6536)\tLoss (Regu) 0.2187 (0.2143)\tPrec@1 71.875 (64.645)\tPrec@5 93.750 (89.277)\n",
            "Test: [100/157]\tTime 0.008 (0.009)\tLoss (Class) 0.8909 (1.6263)\tLoss (Regu) 0.2169 (0.2140)\tPrec@1 75.000 (65.316)\tPrec@5 98.438 (89.434)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 0.9835 (1.6414)\tLoss (Regu) 0.2208 (0.2139)\tPrec@1 79.688 (65.077)\tPrec@5 96.875 (89.062)\n",
            " * Train[81.938 %, 98.034 %, 0.760 loss] Val [65.190 %, 89.080%, 1.638 loss] Best: 65.190 %\n",
            "Time for 109 / 150 20.800062894821167\n",
            "Learning rate:  0.03\n",
            "Epoch: [110][0/782]\tTime 0.155 (0.155)\tLoss (Class) 0.8520 (0.8520)\tLoss (Regu) 0.1849 (0.1849)\tPrec@1 76.562 (76.562)\tPrec@5 98.438 (98.438)\n",
            "Epoch: [110][50/782]\tTime 0.023 (0.026)\tLoss (Class) 0.6567 (0.7088)\tLoss (Regu) 0.1936 (0.1917)\tPrec@1 84.375 (83.609)\tPrec@5 100.000 (98.376)\n",
            "Epoch: [110][100/782]\tTime 0.023 (0.025)\tLoss (Class) 0.7896 (0.6986)\tLoss (Regu) 0.1910 (0.1915)\tPrec@1 82.812 (83.973)\tPrec@5 96.875 (98.468)\n",
            "Epoch: [110][150/782]\tTime 0.022 (0.025)\tLoss (Class) 0.5710 (0.6953)\tLoss (Regu) 0.1941 (0.1911)\tPrec@1 90.625 (84.323)\tPrec@5 98.438 (98.365)\n",
            "Epoch: [110][200/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7476 (0.7004)\tLoss (Regu) 0.1921 (0.1907)\tPrec@1 78.125 (83.986)\tPrec@5 98.438 (98.305)\n",
            "Epoch: [110][250/782]\tTime 0.030 (0.024)\tLoss (Class) 0.6801 (0.7083)\tLoss (Regu) 0.1862 (0.1903)\tPrec@1 87.500 (83.653)\tPrec@5 98.438 (98.269)\n",
            "Epoch: [110][300/782]\tTime 0.022 (0.024)\tLoss (Class) 0.7919 (0.7166)\tLoss (Regu) 0.1876 (0.1902)\tPrec@1 82.812 (83.430)\tPrec@5 98.438 (98.230)\n",
            "Epoch: [110][350/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6712 (0.7200)\tLoss (Regu) 0.1907 (0.1903)\tPrec@1 82.812 (83.253)\tPrec@5 96.875 (98.233)\n",
            "Epoch: [110][400/782]\tTime 0.022 (0.024)\tLoss (Class) 0.6216 (0.7234)\tLoss (Regu) 0.1865 (0.1901)\tPrec@1 87.500 (83.089)\tPrec@5 100.000 (98.231)\n",
            "Epoch: [110][450/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6426 (0.7221)\tLoss (Regu) 0.1883 (0.1901)\tPrec@1 84.375 (83.159)\tPrec@5 100.000 (98.237)\n",
            "Epoch: [110][500/782]\tTime 0.030 (0.024)\tLoss (Class) 0.9535 (0.7239)\tLoss (Regu) 0.1872 (0.1901)\tPrec@1 76.562 (83.106)\tPrec@5 93.750 (98.216)\n",
            "Epoch: [110][550/782]\tTime 0.024 (0.024)\tLoss (Class) 0.6249 (0.7268)\tLoss (Regu) 0.1885 (0.1899)\tPrec@1 89.062 (83.037)\tPrec@5 96.875 (98.191)\n",
            "Epoch: [110][600/782]\tTime 0.022 (0.024)\tLoss (Class) 0.6988 (0.7302)\tLoss (Regu) 0.1898 (0.1899)\tPrec@1 89.062 (82.935)\tPrec@5 96.875 (98.172)\n",
            "Epoch: [110][650/782]\tTime 0.022 (0.024)\tLoss (Class) 0.9084 (0.7343)\tLoss (Regu) 0.1896 (0.1899)\tPrec@1 81.250 (82.798)\tPrec@5 100.000 (98.152)\n",
            "Epoch: [110][700/782]\tTime 0.023 (0.024)\tLoss (Class) 0.9316 (0.7401)\tLoss (Regu) 0.1866 (0.1898)\tPrec@1 75.000 (82.672)\tPrec@5 96.875 (98.125)\n",
            "Epoch: [110][750/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8413 (0.7447)\tLoss (Regu) 0.1929 (0.1898)\tPrec@1 81.250 (82.575)\tPrec@5 96.875 (98.086)\n",
            "Test: [0/157]\tTime 0.115 (0.115)\tLoss (Class) 1.8383 (1.8383)\tLoss (Regu) 0.2201 (0.2201)\tPrec@1 60.938 (60.938)\tPrec@5 87.500 (87.500)\n",
            "Test: [50/157]\tTime 0.009 (0.010)\tLoss (Class) 1.8998 (1.8076)\tLoss (Regu) 0.2242 (0.2194)\tPrec@1 57.812 (63.113)\tPrec@5 87.500 (87.990)\n",
            "Test: [100/157]\tTime 0.014 (0.009)\tLoss (Class) 2.7547 (1.7924)\tLoss (Regu) 0.2155 (0.2196)\tPrec@1 50.000 (63.165)\tPrec@5 70.312 (88.227)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 2.3671 (1.7903)\tLoss (Regu) 0.2207 (0.2193)\tPrec@1 57.812 (62.945)\tPrec@5 85.938 (88.069)\n",
            " * Train[82.540 %, 98.078 %, 0.745 loss] Val [62.830 %, 87.990%, 1.792 loss] Best: 65.190 %\n",
            "Time for 110 / 150 20.308143615722656\n",
            "Learning rate:  0.03\n",
            "Epoch: [111][0/782]\tTime 0.151 (0.151)\tLoss (Class) 0.7173 (0.7173)\tLoss (Regu) 0.1906 (0.1906)\tPrec@1 87.500 (87.500)\tPrec@5 95.312 (95.312)\n",
            "Epoch: [111][50/782]\tTime 0.023 (0.026)\tLoss (Class) 0.6798 (0.7055)\tLoss (Regu) 0.1917 (0.1899)\tPrec@1 81.250 (83.119)\tPrec@5 100.000 (98.131)\n",
            "Epoch: [111][100/782]\tTime 0.022 (0.025)\tLoss (Class) 0.8696 (0.6906)\tLoss (Regu) 0.1917 (0.1906)\tPrec@1 73.438 (83.803)\tPrec@5 96.875 (98.360)\n",
            "Epoch: [111][150/782]\tTime 0.023 (0.025)\tLoss (Class) 0.7499 (0.6944)\tLoss (Regu) 0.1908 (0.1905)\tPrec@1 82.812 (83.796)\tPrec@5 98.438 (98.365)\n",
            "Epoch: [111][200/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6709 (0.6953)\tLoss (Regu) 0.1895 (0.1906)\tPrec@1 84.375 (83.854)\tPrec@5 98.438 (98.438)\n",
            "Epoch: [111][250/782]\tTime 0.023 (0.024)\tLoss (Class) 0.5757 (0.7087)\tLoss (Regu) 0.1896 (0.1901)\tPrec@1 89.062 (83.491)\tPrec@5 98.438 (98.338)\n",
            "Epoch: [111][300/782]\tTime 0.022 (0.024)\tLoss (Class) 0.9898 (0.7154)\tLoss (Regu) 0.1885 (0.1896)\tPrec@1 75.000 (83.165)\tPrec@5 95.312 (98.354)\n",
            "Epoch: [111][350/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6120 (0.7199)\tLoss (Regu) 0.1920 (0.1895)\tPrec@1 87.500 (83.080)\tPrec@5 96.875 (98.322)\n",
            "Epoch: [111][400/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8179 (0.7252)\tLoss (Regu) 0.1904 (0.1897)\tPrec@1 78.125 (83.007)\tPrec@5 98.438 (98.282)\n",
            "Epoch: [111][450/782]\tTime 0.022 (0.024)\tLoss (Class) 0.6685 (0.7278)\tLoss (Regu) 0.1885 (0.1898)\tPrec@1 89.062 (83.027)\tPrec@5 98.438 (98.271)\n",
            "Epoch: [111][500/782]\tTime 0.024 (0.024)\tLoss (Class) 0.6983 (0.7277)\tLoss (Regu) 0.1932 (0.1898)\tPrec@1 87.500 (83.015)\tPrec@5 98.438 (98.278)\n",
            "Epoch: [111][550/782]\tTime 0.025 (0.023)\tLoss (Class) 0.7204 (0.7311)\tLoss (Regu) 0.1902 (0.1900)\tPrec@1 79.688 (82.920)\tPrec@5 98.438 (98.253)\n",
            "Epoch: [111][600/782]\tTime 0.022 (0.023)\tLoss (Class) 0.7067 (0.7392)\tLoss (Regu) 0.1863 (0.1899)\tPrec@1 84.375 (82.657)\tPrec@5 100.000 (98.165)\n",
            "Epoch: [111][650/782]\tTime 0.024 (0.023)\tLoss (Class) 0.5531 (0.7405)\tLoss (Regu) 0.1941 (0.1898)\tPrec@1 89.062 (82.637)\tPrec@5 100.000 (98.152)\n",
            "Epoch: [111][700/782]\tTime 0.022 (0.023)\tLoss (Class) 0.7779 (0.7432)\tLoss (Regu) 0.1870 (0.1898)\tPrec@1 78.125 (82.538)\tPrec@5 98.438 (98.150)\n",
            "Epoch: [111][750/782]\tTime 0.023 (0.023)\tLoss (Class) 0.8618 (0.7453)\tLoss (Regu) 0.1855 (0.1898)\tPrec@1 76.562 (82.440)\tPrec@5 96.875 (98.140)\n",
            "Test: [0/157]\tTime 0.115 (0.115)\tLoss (Class) 1.6106 (1.6106)\tLoss (Regu) 0.2155 (0.2155)\tPrec@1 62.500 (62.500)\tPrec@5 89.062 (89.062)\n",
            "Test: [50/157]\tTime 0.008 (0.010)\tLoss (Class) 1.6559 (1.8439)\tLoss (Regu) 0.2176 (0.2212)\tPrec@1 62.500 (61.949)\tPrec@5 89.062 (87.898)\n",
            "Test: [100/157]\tTime 0.008 (0.009)\tLoss (Class) 1.5246 (1.8185)\tLoss (Regu) 0.2228 (0.2214)\tPrec@1 67.188 (62.655)\tPrec@5 89.062 (87.809)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.5782 (1.8134)\tLoss (Regu) 0.2163 (0.2215)\tPrec@1 68.750 (63.038)\tPrec@5 90.625 (87.800)\n",
            " * Train[82.358 %, 98.130 %, 0.747 loss] Val [63.090 %, 87.790%, 1.815 loss] Best: 65.190 %\n",
            "Time for 111 / 150 19.6889431476593\n",
            "Learning rate:  0.03\n",
            "Epoch: [112][0/782]\tTime 0.159 (0.159)\tLoss (Class) 0.6471 (0.6471)\tLoss (Regu) 0.1927 (0.1927)\tPrec@1 84.375 (84.375)\tPrec@5 98.438 (98.438)\n",
            "Epoch: [112][50/782]\tTime 0.022 (0.027)\tLoss (Class) 0.6403 (0.7164)\tLoss (Regu) 0.1894 (0.1920)\tPrec@1 87.500 (82.904)\tPrec@5 100.000 (98.683)\n",
            "Epoch: [112][100/782]\tTime 0.023 (0.026)\tLoss (Class) 0.6598 (0.7037)\tLoss (Regu) 0.1862 (0.1911)\tPrec@1 90.625 (83.648)\tPrec@5 96.875 (98.700)\n",
            "Epoch: [112][150/782]\tTime 0.033 (0.025)\tLoss (Class) 1.0757 (0.7016)\tLoss (Regu) 0.1904 (0.1910)\tPrec@1 70.312 (83.578)\tPrec@5 98.438 (98.675)\n",
            "Epoch: [112][200/782]\tTime 0.023 (0.025)\tLoss (Class) 0.5960 (0.7102)\tLoss (Regu) 0.1929 (0.1902)\tPrec@1 89.062 (83.465)\tPrec@5 98.438 (98.577)\n",
            "Epoch: [112][250/782]\tTime 0.022 (0.025)\tLoss (Class) 0.7861 (0.7162)\tLoss (Regu) 0.1900 (0.1903)\tPrec@1 78.125 (83.323)\tPrec@5 98.438 (98.518)\n",
            "Epoch: [112][300/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7888 (0.7174)\tLoss (Regu) 0.1873 (0.1901)\tPrec@1 78.125 (83.280)\tPrec@5 100.000 (98.479)\n",
            "Epoch: [112][350/782]\tTime 0.023 (0.024)\tLoss (Class) 0.5361 (0.7171)\tLoss (Regu) 0.1896 (0.1898)\tPrec@1 90.625 (83.333)\tPrec@5 98.438 (98.464)\n",
            "Epoch: [112][400/782]\tTime 0.022 (0.024)\tLoss (Class) 0.7660 (0.7175)\tLoss (Regu) 0.1934 (0.1898)\tPrec@1 82.812 (83.292)\tPrec@5 98.438 (98.480)\n",
            "Epoch: [112][450/782]\tTime 0.024 (0.024)\tLoss (Class) 0.8508 (0.7221)\tLoss (Regu) 0.1850 (0.1899)\tPrec@1 78.125 (83.159)\tPrec@5 100.000 (98.431)\n",
            "Epoch: [112][500/782]\tTime 0.022 (0.024)\tLoss (Class) 0.6638 (0.7255)\tLoss (Regu) 0.1891 (0.1896)\tPrec@1 84.375 (83.037)\tPrec@5 98.438 (98.381)\n",
            "Epoch: [112][550/782]\tTime 0.022 (0.024)\tLoss (Class) 0.6088 (0.7247)\tLoss (Regu) 0.1891 (0.1895)\tPrec@1 89.062 (83.082)\tPrec@5 100.000 (98.364)\n",
            "Epoch: [112][600/782]\tTime 0.024 (0.024)\tLoss (Class) 0.8126 (0.7247)\tLoss (Regu) 0.1874 (0.1894)\tPrec@1 76.562 (83.085)\tPrec@5 96.875 (98.313)\n",
            "Epoch: [112][650/782]\tTime 0.022 (0.024)\tLoss (Class) 0.5993 (0.7269)\tLoss (Regu) 0.1878 (0.1894)\tPrec@1 84.375 (83.002)\tPrec@5 100.000 (98.296)\n",
            "Epoch: [112][700/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8483 (0.7312)\tLoss (Regu) 0.1845 (0.1892)\tPrec@1 76.562 (82.826)\tPrec@5 98.438 (98.252)\n",
            "Epoch: [112][750/782]\tTime 0.023 (0.024)\tLoss (Class) 0.5986 (0.7354)\tLoss (Regu) 0.1857 (0.1892)\tPrec@1 87.500 (82.694)\tPrec@5 98.438 (98.238)\n",
            "Test: [0/157]\tTime 0.106 (0.106)\tLoss (Class) 2.2882 (2.2882)\tLoss (Regu) 0.2145 (0.2145)\tPrec@1 59.375 (59.375)\tPrec@5 87.500 (87.500)\n",
            "Test: [50/157]\tTime 0.008 (0.010)\tLoss (Class) 1.1891 (1.6855)\tLoss (Regu) 0.2144 (0.2151)\tPrec@1 73.438 (65.564)\tPrec@5 95.312 (88.940)\n",
            "Test: [100/157]\tTime 0.009 (0.009)\tLoss (Class) 1.2273 (1.7212)\tLoss (Regu) 0.2190 (0.2146)\tPrec@1 75.000 (64.264)\tPrec@5 92.188 (88.753)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 2.1945 (1.7226)\tLoss (Regu) 0.2150 (0.2151)\tPrec@1 54.688 (64.259)\tPrec@5 82.812 (88.638)\n",
            " * Train[82.606 %, 98.214 %, 0.738 loss] Val [64.220 %, 88.630%, 1.726 loss] Best: 65.190 %\n",
            "Time for 112 / 150 20.03887939453125\n",
            "Learning rate:  0.03\n",
            "Epoch: [113][0/782]\tTime 0.150 (0.150)\tLoss (Class) 0.7025 (0.7025)\tLoss (Regu) 0.1895 (0.1895)\tPrec@1 82.812 (82.812)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [113][50/782]\tTime 0.023 (0.027)\tLoss (Class) 0.6533 (0.6691)\tLoss (Regu) 0.1910 (0.1923)\tPrec@1 81.250 (85.355)\tPrec@5 100.000 (98.499)\n",
            "Epoch: [113][100/782]\tTime 0.023 (0.025)\tLoss (Class) 0.7473 (0.6729)\tLoss (Regu) 0.1907 (0.1910)\tPrec@1 84.375 (84.994)\tPrec@5 98.438 (98.592)\n",
            "Epoch: [113][150/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6225 (0.6797)\tLoss (Regu) 0.1873 (0.1905)\tPrec@1 87.500 (84.561)\tPrec@5 98.438 (98.489)\n",
            "Epoch: [113][200/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7911 (0.6909)\tLoss (Regu) 0.1866 (0.1902)\tPrec@1 81.250 (84.150)\tPrec@5 96.875 (98.453)\n",
            "Epoch: [113][250/782]\tTime 0.024 (0.024)\tLoss (Class) 0.8527 (0.6933)\tLoss (Regu) 0.1888 (0.1904)\tPrec@1 79.688 (84.163)\tPrec@5 98.438 (98.419)\n",
            "Epoch: [113][300/782]\tTime 0.024 (0.024)\tLoss (Class) 0.6990 (0.7045)\tLoss (Regu) 0.1885 (0.1905)\tPrec@1 87.500 (83.721)\tPrec@5 98.438 (98.391)\n",
            "Epoch: [113][350/782]\tTime 0.022 (0.024)\tLoss (Class) 0.6840 (0.7124)\tLoss (Regu) 0.1884 (0.1909)\tPrec@1 82.812 (83.409)\tPrec@5 96.875 (98.375)\n",
            "Epoch: [113][400/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6486 (0.7162)\tLoss (Regu) 0.1925 (0.1907)\tPrec@1 87.500 (83.331)\tPrec@5 98.438 (98.328)\n",
            "Epoch: [113][450/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6920 (0.7147)\tLoss (Regu) 0.1925 (0.1909)\tPrec@1 84.375 (83.367)\tPrec@5 96.875 (98.340)\n",
            "Epoch: [113][500/782]\tTime 0.022 (0.024)\tLoss (Class) 0.7241 (0.7202)\tLoss (Regu) 0.1863 (0.1905)\tPrec@1 84.375 (83.252)\tPrec@5 98.438 (98.241)\n",
            "Epoch: [113][550/782]\tTime 0.022 (0.023)\tLoss (Class) 0.8178 (0.7220)\tLoss (Regu) 0.1893 (0.1904)\tPrec@1 71.875 (83.195)\tPrec@5 98.438 (98.219)\n",
            "Epoch: [113][600/782]\tTime 0.023 (0.023)\tLoss (Class) 0.9968 (0.7255)\tLoss (Regu) 0.1878 (0.1902)\tPrec@1 73.438 (83.109)\tPrec@5 96.875 (98.206)\n",
            "Epoch: [113][650/782]\tTime 0.022 (0.023)\tLoss (Class) 0.7973 (0.7293)\tLoss (Regu) 0.1906 (0.1901)\tPrec@1 81.250 (82.973)\tPrec@5 100.000 (98.183)\n",
            "Epoch: [113][700/782]\tTime 0.023 (0.023)\tLoss (Class) 0.8010 (0.7330)\tLoss (Regu) 0.1878 (0.1900)\tPrec@1 78.125 (82.899)\tPrec@5 98.438 (98.166)\n",
            "Epoch: [113][750/782]\tTime 0.024 (0.023)\tLoss (Class) 0.9571 (0.7374)\tLoss (Regu) 0.1874 (0.1899)\tPrec@1 75.000 (82.744)\tPrec@5 96.875 (98.146)\n",
            "Test: [0/157]\tTime 0.109 (0.109)\tLoss (Class) 1.7336 (1.7336)\tLoss (Regu) 0.2238 (0.2238)\tPrec@1 65.625 (65.625)\tPrec@5 92.188 (92.188)\n",
            "Test: [50/157]\tTime 0.008 (0.011)\tLoss (Class) 2.0598 (1.8218)\tLoss (Regu) 0.2265 (0.2240)\tPrec@1 57.812 (63.879)\tPrec@5 82.812 (88.113)\n",
            "Test: [100/157]\tTime 0.007 (0.010)\tLoss (Class) 1.3985 (1.8028)\tLoss (Regu) 0.2250 (0.2247)\tPrec@1 59.375 (63.815)\tPrec@5 96.875 (88.645)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.2551 (1.7988)\tLoss (Regu) 0.2243 (0.2248)\tPrec@1 76.562 (63.845)\tPrec@5 85.938 (88.317)\n",
            " * Train[82.670 %, 98.136 %, 0.738 loss] Val [63.810 %, 88.310%, 1.800 loss] Best: 65.190 %\n",
            "Time for 113 / 150 19.70062279701233\n",
            "Learning rate:  0.03\n",
            "Epoch: [114][0/782]\tTime 0.156 (0.156)\tLoss (Class) 0.5923 (0.5923)\tLoss (Regu) 0.1910 (0.1910)\tPrec@1 92.188 (92.188)\tPrec@5 98.438 (98.438)\n",
            "Epoch: [114][50/782]\tTime 0.023 (0.026)\tLoss (Class) 0.7145 (0.7140)\tLoss (Regu) 0.1941 (0.1911)\tPrec@1 75.000 (83.609)\tPrec@5 98.438 (98.713)\n",
            "Epoch: [114][100/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7439 (0.7201)\tLoss (Regu) 0.1898 (0.1907)\tPrec@1 81.250 (83.431)\tPrec@5 96.875 (98.422)\n",
            "Epoch: [114][150/782]\tTime 0.026 (0.025)\tLoss (Class) 0.5908 (0.6998)\tLoss (Regu) 0.1909 (0.1910)\tPrec@1 89.062 (84.106)\tPrec@5 100.000 (98.562)\n",
            "Epoch: [114][200/782]\tTime 0.023 (0.025)\tLoss (Class) 0.6462 (0.7085)\tLoss (Regu) 0.1891 (0.1918)\tPrec@1 84.375 (83.691)\tPrec@5 100.000 (98.523)\n",
            "Epoch: [114][250/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6846 (0.7023)\tLoss (Regu) 0.1900 (0.1915)\tPrec@1 84.375 (83.809)\tPrec@5 95.312 (98.581)\n",
            "Epoch: [114][300/782]\tTime 0.023 (0.024)\tLoss (Class) 0.9798 (0.7119)\tLoss (Regu) 0.1855 (0.1911)\tPrec@1 73.438 (83.627)\tPrec@5 95.312 (98.463)\n",
            "Epoch: [114][350/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8614 (0.7180)\tLoss (Regu) 0.1863 (0.1906)\tPrec@1 79.688 (83.480)\tPrec@5 96.875 (98.424)\n",
            "Epoch: [114][400/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8423 (0.7214)\tLoss (Regu) 0.1882 (0.1905)\tPrec@1 75.000 (83.389)\tPrec@5 98.438 (98.379)\n",
            "Epoch: [114][450/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7774 (0.7244)\tLoss (Regu) 0.1914 (0.1905)\tPrec@1 85.938 (83.339)\tPrec@5 100.000 (98.358)\n",
            "Epoch: [114][500/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6988 (0.7260)\tLoss (Regu) 0.1922 (0.1904)\tPrec@1 79.688 (83.202)\tPrec@5 98.438 (98.353)\n",
            "Epoch: [114][550/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7436 (0.7269)\tLoss (Regu) 0.1915 (0.1903)\tPrec@1 79.688 (83.133)\tPrec@5 98.438 (98.321)\n",
            "Epoch: [114][600/782]\tTime 0.024 (0.024)\tLoss (Class) 0.9061 (0.7276)\tLoss (Regu) 0.1866 (0.1901)\tPrec@1 73.438 (83.085)\tPrec@5 96.875 (98.331)\n",
            "Epoch: [114][650/782]\tTime 0.032 (0.024)\tLoss (Class) 0.7611 (0.7272)\tLoss (Regu) 0.1925 (0.1901)\tPrec@1 85.938 (83.120)\tPrec@5 98.438 (98.334)\n",
            "Epoch: [114][700/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6389 (0.7280)\tLoss (Regu) 0.1892 (0.1901)\tPrec@1 90.625 (83.064)\tPrec@5 100.000 (98.346)\n",
            "Epoch: [114][750/782]\tTime 0.031 (0.024)\tLoss (Class) 0.7644 (0.7308)\tLoss (Regu) 0.1894 (0.1902)\tPrec@1 84.375 (82.960)\tPrec@5 96.875 (98.319)\n",
            "Test: [0/157]\tTime 0.114 (0.114)\tLoss (Class) 1.8905 (1.8905)\tLoss (Regu) 0.2197 (0.2197)\tPrec@1 65.625 (65.625)\tPrec@5 85.938 (85.938)\n",
            "Test: [50/157]\tTime 0.009 (0.010)\tLoss (Class) 1.3360 (1.7252)\tLoss (Regu) 0.2213 (0.2177)\tPrec@1 68.750 (64.185)\tPrec@5 90.625 (89.369)\n",
            "Test: [100/157]\tTime 0.008 (0.009)\tLoss (Class) 1.5991 (1.7352)\tLoss (Regu) 0.2192 (0.2176)\tPrec@1 62.500 (64.171)\tPrec@5 93.750 (88.908)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 2.6255 (1.7432)\tLoss (Regu) 0.2140 (0.2178)\tPrec@1 43.750 (64.104)\tPrec@5 79.688 (89.011)\n",
            " * Train[82.904 %, 98.312 %, 0.733 loss] Val [64.020 %, 88.980%, 1.740 loss] Best: 65.190 %\n",
            "Time for 114 / 150 20.22794818878174\n",
            "Learning rate:  0.03\n",
            "Epoch: [115][0/782]\tTime 0.155 (0.155)\tLoss (Class) 0.6801 (0.6801)\tLoss (Regu) 0.1865 (0.1865)\tPrec@1 84.375 (84.375)\tPrec@5 98.438 (98.438)\n",
            "Epoch: [115][50/782]\tTime 0.023 (0.026)\tLoss (Class) 0.7520 (0.7156)\tLoss (Regu) 0.1909 (0.1896)\tPrec@1 85.938 (83.885)\tPrec@5 98.438 (97.763)\n",
            "Epoch: [115][100/782]\tTime 0.023 (0.025)\tLoss (Class) 0.6896 (0.7113)\tLoss (Regu) 0.1902 (0.1893)\tPrec@1 85.938 (83.895)\tPrec@5 96.875 (97.942)\n",
            "Epoch: [115][150/782]\tTime 0.022 (0.024)\tLoss (Class) 0.6858 (0.7062)\tLoss (Regu) 0.1912 (0.1891)\tPrec@1 84.375 (84.054)\tPrec@5 98.438 (98.117)\n",
            "Epoch: [115][200/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6860 (0.7035)\tLoss (Regu) 0.1887 (0.1894)\tPrec@1 82.812 (84.072)\tPrec@5 100.000 (98.165)\n",
            "Epoch: [115][250/782]\tTime 0.022 (0.024)\tLoss (Class) 0.8515 (0.7023)\tLoss (Regu) 0.1922 (0.1896)\tPrec@1 78.125 (84.126)\tPrec@5 96.875 (98.207)\n",
            "Epoch: [115][300/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7487 (0.6999)\tLoss (Regu) 0.1904 (0.1897)\tPrec@1 75.000 (84.178)\tPrec@5 98.438 (98.251)\n",
            "Epoch: [115][350/782]\tTime 0.022 (0.023)\tLoss (Class) 0.6757 (0.7009)\tLoss (Regu) 0.1893 (0.1893)\tPrec@1 82.812 (84.148)\tPrec@5 100.000 (98.277)\n",
            "Epoch: [115][400/782]\tTime 0.023 (0.023)\tLoss (Class) 0.8702 (0.6997)\tLoss (Regu) 0.1884 (0.1894)\tPrec@1 78.125 (84.052)\tPrec@5 98.438 (98.348)\n",
            "Epoch: [115][450/782]\tTime 0.023 (0.024)\tLoss (Class) 0.4588 (0.7038)\tLoss (Regu) 0.1841 (0.1890)\tPrec@1 95.312 (83.893)\tPrec@5 100.000 (98.347)\n",
            "Epoch: [115][500/782]\tTime 0.030 (0.024)\tLoss (Class) 0.6444 (0.7042)\tLoss (Regu) 0.1865 (0.1890)\tPrec@1 85.938 (83.851)\tPrec@5 100.000 (98.360)\n",
            "Epoch: [115][550/782]\tTime 0.022 (0.024)\tLoss (Class) 0.6294 (0.7087)\tLoss (Regu) 0.1936 (0.1888)\tPrec@1 87.500 (83.683)\tPrec@5 100.000 (98.335)\n",
            "Epoch: [115][600/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7170 (0.7141)\tLoss (Regu) 0.1875 (0.1888)\tPrec@1 84.375 (83.514)\tPrec@5 100.000 (98.297)\n",
            "Epoch: [115][650/782]\tTime 0.022 (0.024)\tLoss (Class) 0.8568 (0.7173)\tLoss (Regu) 0.1859 (0.1887)\tPrec@1 78.125 (83.403)\tPrec@5 96.875 (98.265)\n",
            "Epoch: [115][700/782]\tTime 0.023 (0.024)\tLoss (Class) 1.0063 (0.7197)\tLoss (Regu) 0.1893 (0.1888)\tPrec@1 79.688 (83.318)\tPrec@5 92.188 (98.268)\n",
            "Epoch: [115][750/782]\tTime 0.022 (0.024)\tLoss (Class) 0.7151 (0.7233)\tLoss (Regu) 0.1893 (0.1888)\tPrec@1 84.375 (83.204)\tPrec@5 96.875 (98.234)\n",
            "Test: [0/157]\tTime 0.117 (0.117)\tLoss (Class) 1.3865 (1.3865)\tLoss (Regu) 0.2124 (0.2124)\tPrec@1 70.312 (70.312)\tPrec@5 90.625 (90.625)\n",
            "Test: [50/157]\tTime 0.008 (0.012)\tLoss (Class) 1.5424 (1.7151)\tLoss (Regu) 0.2083 (0.2121)\tPrec@1 67.188 (63.572)\tPrec@5 89.062 (87.990)\n",
            "Test: [100/157]\tTime 0.007 (0.010)\tLoss (Class) 1.5118 (1.7244)\tLoss (Regu) 0.2094 (0.2122)\tPrec@1 64.062 (63.289)\tPrec@5 95.312 (87.871)\n",
            "Test: [150/157]\tTime 0.007 (0.010)\tLoss (Class) 1.4072 (1.7155)\tLoss (Regu) 0.2114 (0.2121)\tPrec@1 71.875 (63.918)\tPrec@5 89.062 (88.111)\n",
            " * Train[83.130 %, 98.216 %, 0.725 loss] Val [63.820 %, 88.220%, 1.719 loss] Best: 65.190 %\n",
            "Time for 115 / 150 19.994805812835693\n",
            "Learning rate:  0.03\n",
            "Epoch: [116][0/782]\tTime 0.156 (0.156)\tLoss (Class) 0.6442 (0.6442)\tLoss (Regu) 0.1881 (0.1881)\tPrec@1 87.500 (87.500)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [116][50/782]\tTime 0.023 (0.027)\tLoss (Class) 0.7733 (0.7103)\tLoss (Regu) 0.1919 (0.1910)\tPrec@1 82.812 (83.640)\tPrec@5 100.000 (98.315)\n",
            "Epoch: [116][100/782]\tTime 0.023 (0.025)\tLoss (Class) 0.7059 (0.6926)\tLoss (Regu) 0.1951 (0.1915)\tPrec@1 76.562 (84.267)\tPrec@5 98.438 (98.453)\n",
            "Epoch: [116][150/782]\tTime 0.030 (0.024)\tLoss (Class) 0.8071 (0.6971)\tLoss (Regu) 0.1957 (0.1917)\tPrec@1 81.250 (84.034)\tPrec@5 95.312 (98.562)\n",
            "Epoch: [116][200/782]\tTime 0.024 (0.025)\tLoss (Class) 0.6872 (0.6972)\tLoss (Regu) 0.1884 (0.1913)\tPrec@1 84.375 (84.041)\tPrec@5 98.438 (98.531)\n",
            "Epoch: [116][250/782]\tTime 0.022 (0.025)\tLoss (Class) 0.7805 (0.7007)\tLoss (Regu) 0.1926 (0.1908)\tPrec@1 79.688 (83.908)\tPrec@5 98.438 (98.512)\n",
            "Epoch: [116][300/782]\tTime 0.022 (0.025)\tLoss (Class) 0.6161 (0.7086)\tLoss (Regu) 0.1954 (0.1905)\tPrec@1 85.938 (83.669)\tPrec@5 96.875 (98.438)\n",
            "Epoch: [116][350/782]\tTime 0.022 (0.025)\tLoss (Class) 0.6686 (0.7159)\tLoss (Regu) 0.1854 (0.1906)\tPrec@1 87.500 (83.458)\tPrec@5 96.875 (98.348)\n",
            "Epoch: [116][400/782]\tTime 0.023 (0.025)\tLoss (Class) 0.8229 (0.7178)\tLoss (Regu) 0.1860 (0.1906)\tPrec@1 82.812 (83.475)\tPrec@5 96.875 (98.363)\n",
            "Epoch: [116][450/782]\tTime 0.021 (0.025)\tLoss (Class) 0.5356 (0.7180)\tLoss (Regu) 0.1906 (0.1904)\tPrec@1 85.938 (83.457)\tPrec@5 100.000 (98.351)\n",
            "Epoch: [116][500/782]\tTime 0.022 (0.025)\tLoss (Class) 0.6201 (0.7187)\tLoss (Regu) 0.1910 (0.1902)\tPrec@1 89.062 (83.439)\tPrec@5 98.438 (98.322)\n",
            "Epoch: [116][550/782]\tTime 0.021 (0.025)\tLoss (Class) 0.7164 (0.7193)\tLoss (Regu) 0.1908 (0.1900)\tPrec@1 82.812 (83.351)\tPrec@5 100.000 (98.318)\n",
            "Epoch: [116][600/782]\tTime 0.023 (0.025)\tLoss (Class) 0.9108 (0.7226)\tLoss (Regu) 0.1840 (0.1899)\tPrec@1 75.000 (83.226)\tPrec@5 100.000 (98.308)\n",
            "Epoch: [116][650/782]\tTime 0.022 (0.025)\tLoss (Class) 0.7754 (0.7250)\tLoss (Regu) 0.1931 (0.1898)\tPrec@1 76.562 (83.146)\tPrec@5 96.875 (98.277)\n",
            "Epoch: [116][700/782]\tTime 0.022 (0.025)\tLoss (Class) 0.6775 (0.7279)\tLoss (Regu) 0.1903 (0.1897)\tPrec@1 85.938 (83.067)\tPrec@5 96.875 (98.270)\n",
            "Epoch: [116][750/782]\tTime 0.023 (0.025)\tLoss (Class) 0.9611 (0.7330)\tLoss (Regu) 0.1874 (0.1895)\tPrec@1 73.438 (82.860)\tPrec@5 95.312 (98.217)\n",
            "Test: [0/157]\tTime 0.111 (0.111)\tLoss (Class) 1.7126 (1.7126)\tLoss (Regu) 0.2206 (0.2206)\tPrec@1 71.875 (71.875)\tPrec@5 85.938 (85.938)\n",
            "Test: [50/157]\tTime 0.009 (0.010)\tLoss (Class) 1.7629 (1.7478)\tLoss (Regu) 0.2215 (0.2176)\tPrec@1 59.375 (62.561)\tPrec@5 90.625 (89.124)\n",
            "Test: [100/157]\tTime 0.008 (0.009)\tLoss (Class) 1.6670 (1.7966)\tLoss (Regu) 0.2151 (0.2171)\tPrec@1 68.750 (62.299)\tPrec@5 89.062 (88.181)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.5436 (1.7807)\tLoss (Regu) 0.2142 (0.2169)\tPrec@1 64.062 (62.407)\tPrec@5 87.500 (88.224)\n",
            " * Train[82.802 %, 98.186 %, 0.735 loss] Val [62.330 %, 88.230%, 1.784 loss] Best: 65.190 %\n",
            "Time for 116 / 150 20.904163360595703\n",
            "Learning rate:  0.03\n",
            "Epoch: [117][0/782]\tTime 0.157 (0.157)\tLoss (Class) 0.6775 (0.6775)\tLoss (Regu) 0.1862 (0.1862)\tPrec@1 82.812 (82.812)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [117][50/782]\tTime 0.021 (0.028)\tLoss (Class) 0.8375 (0.6905)\tLoss (Regu) 0.1907 (0.1911)\tPrec@1 78.125 (83.977)\tPrec@5 98.438 (98.652)\n",
            "Epoch: [117][100/782]\tTime 0.023 (0.026)\tLoss (Class) 0.6136 (0.6921)\tLoss (Regu) 0.1894 (0.1904)\tPrec@1 85.938 (84.004)\tPrec@5 100.000 (98.499)\n",
            "Epoch: [117][150/782]\tTime 0.021 (0.026)\tLoss (Class) 0.6084 (0.6894)\tLoss (Regu) 0.1889 (0.1904)\tPrec@1 84.375 (84.013)\tPrec@5 100.000 (98.489)\n",
            "Epoch: [117][200/782]\tTime 0.022 (0.025)\tLoss (Class) 0.6911 (0.6901)\tLoss (Regu) 0.1878 (0.1902)\tPrec@1 85.938 (84.010)\tPrec@5 98.438 (98.539)\n",
            "Epoch: [117][250/782]\tTime 0.023 (0.025)\tLoss (Class) 0.5509 (0.6885)\tLoss (Regu) 0.1911 (0.1899)\tPrec@1 89.062 (84.107)\tPrec@5 100.000 (98.550)\n",
            "Epoch: [117][300/782]\tTime 0.025 (0.024)\tLoss (Class) 0.7713 (0.6953)\tLoss (Regu) 0.1901 (0.1901)\tPrec@1 84.375 (83.871)\tPrec@5 96.875 (98.521)\n",
            "Epoch: [117][350/782]\tTime 0.032 (0.025)\tLoss (Class) 0.6234 (0.7009)\tLoss (Regu) 0.1864 (0.1901)\tPrec@1 85.938 (83.618)\tPrec@5 100.000 (98.504)\n",
            "Epoch: [117][400/782]\tTime 0.025 (0.025)\tLoss (Class) 0.5204 (0.7052)\tLoss (Regu) 0.1880 (0.1900)\tPrec@1 87.500 (83.432)\tPrec@5 100.000 (98.476)\n",
            "Epoch: [117][450/782]\tTime 0.032 (0.025)\tLoss (Class) 0.5366 (0.7047)\tLoss (Regu) 0.1876 (0.1899)\tPrec@1 89.062 (83.474)\tPrec@5 100.000 (98.469)\n",
            "Epoch: [117][500/782]\tTime 0.023 (0.025)\tLoss (Class) 0.6827 (0.7081)\tLoss (Regu) 0.1914 (0.1898)\tPrec@1 79.688 (83.402)\tPrec@5 100.000 (98.441)\n",
            "Epoch: [117][550/782]\tTime 0.032 (0.025)\tLoss (Class) 0.7339 (0.7098)\tLoss (Regu) 0.1893 (0.1897)\tPrec@1 81.250 (83.400)\tPrec@5 100.000 (98.398)\n",
            "Epoch: [117][600/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7369 (0.7119)\tLoss (Regu) 0.1921 (0.1898)\tPrec@1 76.562 (83.335)\tPrec@5 98.438 (98.391)\n",
            "Epoch: [117][650/782]\tTime 0.023 (0.024)\tLoss (Class) 0.9302 (0.7149)\tLoss (Regu) 0.1888 (0.1899)\tPrec@1 76.562 (83.257)\tPrec@5 95.312 (98.377)\n",
            "Epoch: [117][700/782]\tTime 0.022 (0.024)\tLoss (Class) 0.6691 (0.7182)\tLoss (Regu) 0.1914 (0.1899)\tPrec@1 79.688 (83.154)\tPrec@5 100.000 (98.366)\n",
            "Epoch: [117][750/782]\tTime 0.023 (0.024)\tLoss (Class) 1.0025 (0.7219)\tLoss (Regu) 0.1866 (0.1899)\tPrec@1 71.875 (83.027)\tPrec@5 93.750 (98.358)\n",
            "Test: [0/157]\tTime 0.112 (0.112)\tLoss (Class) 1.4965 (1.4965)\tLoss (Regu) 0.2139 (0.2139)\tPrec@1 73.438 (73.438)\tPrec@5 92.188 (92.188)\n",
            "Test: [50/157]\tTime 0.008 (0.011)\tLoss (Class) 1.8140 (1.8749)\tLoss (Regu) 0.2226 (0.2190)\tPrec@1 68.750 (61.703)\tPrec@5 90.625 (87.347)\n",
            "Test: [100/157]\tTime 0.009 (0.009)\tLoss (Class) 2.2815 (1.8523)\tLoss (Regu) 0.2176 (0.2189)\tPrec@1 59.375 (62.345)\tPrec@5 81.250 (87.670)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 2.2946 (1.8726)\tLoss (Regu) 0.2173 (0.2188)\tPrec@1 53.125 (62.210)\tPrec@5 79.688 (87.417)\n",
            " * Train[82.980 %, 98.362 %, 0.723 loss] Val [62.190 %, 87.410%, 1.873 loss] Best: 65.190 %\n",
            "Time for 117 / 150 20.418818473815918\n",
            "Learning rate:  0.03\n",
            "Epoch: [118][0/782]\tTime 0.151 (0.151)\tLoss (Class) 0.5775 (0.5775)\tLoss (Regu) 0.1889 (0.1889)\tPrec@1 87.500 (87.500)\tPrec@5 98.438 (98.438)\n",
            "Epoch: [118][50/782]\tTime 0.022 (0.025)\tLoss (Class) 0.6825 (0.6891)\tLoss (Regu) 0.1909 (0.1911)\tPrec@1 82.812 (84.130)\tPrec@5 100.000 (98.652)\n",
            "Epoch: [118][100/782]\tTime 0.022 (0.024)\tLoss (Class) 0.5943 (0.6925)\tLoss (Regu) 0.1916 (0.1901)\tPrec@1 87.500 (84.004)\tPrec@5 100.000 (98.561)\n",
            "Epoch: [118][150/782]\tTime 0.032 (0.024)\tLoss (Class) 0.4817 (0.6772)\tLoss (Regu) 0.1924 (0.1909)\tPrec@1 89.062 (84.427)\tPrec@5 98.438 (98.696)\n",
            "Epoch: [118][200/782]\tTime 0.025 (0.024)\tLoss (Class) 0.5893 (0.6810)\tLoss (Regu) 0.1864 (0.1901)\tPrec@1 85.938 (84.429)\tPrec@5 100.000 (98.741)\n",
            "Epoch: [118][250/782]\tTime 0.032 (0.024)\tLoss (Class) 0.6107 (0.6812)\tLoss (Regu) 0.1895 (0.1899)\tPrec@1 90.625 (84.412)\tPrec@5 98.438 (98.693)\n",
            "Epoch: [118][300/782]\tTime 0.022 (0.024)\tLoss (Class) 0.9743 (0.6912)\tLoss (Regu) 0.1866 (0.1896)\tPrec@1 71.875 (84.043)\tPrec@5 95.312 (98.630)\n",
            "Epoch: [118][350/782]\tTime 0.023 (0.024)\tLoss (Class) 0.5507 (0.6933)\tLoss (Regu) 0.1915 (0.1896)\tPrec@1 87.500 (83.934)\tPrec@5 98.438 (98.616)\n",
            "Epoch: [118][400/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6715 (0.6984)\tLoss (Regu) 0.1901 (0.1897)\tPrec@1 82.812 (83.767)\tPrec@5 100.000 (98.586)\n",
            "Epoch: [118][450/782]\tTime 0.023 (0.024)\tLoss (Class) 0.5941 (0.7029)\tLoss (Regu) 0.1893 (0.1897)\tPrec@1 89.062 (83.672)\tPrec@5 100.000 (98.559)\n",
            "Epoch: [118][500/782]\tTime 0.022 (0.024)\tLoss (Class) 0.7113 (0.7049)\tLoss (Regu) 0.1909 (0.1898)\tPrec@1 84.375 (83.680)\tPrec@5 98.438 (98.565)\n",
            "Epoch: [118][550/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6889 (0.7053)\tLoss (Regu) 0.1921 (0.1898)\tPrec@1 78.125 (83.680)\tPrec@5 100.000 (98.545)\n",
            "Epoch: [118][600/782]\tTime 0.023 (0.024)\tLoss (Class) 0.5575 (0.7096)\tLoss (Regu) 0.1868 (0.1898)\tPrec@1 92.188 (83.592)\tPrec@5 96.875 (98.466)\n",
            "Epoch: [118][650/782]\tTime 0.022 (0.023)\tLoss (Class) 0.6141 (0.7124)\tLoss (Regu) 0.1853 (0.1898)\tPrec@1 89.062 (83.499)\tPrec@5 100.000 (98.423)\n",
            "Epoch: [118][700/782]\tTime 0.022 (0.023)\tLoss (Class) 0.7615 (0.7158)\tLoss (Regu) 0.1900 (0.1897)\tPrec@1 79.688 (83.423)\tPrec@5 100.000 (98.404)\n",
            "Epoch: [118][750/782]\tTime 0.022 (0.023)\tLoss (Class) 0.7650 (0.7206)\tLoss (Regu) 0.1839 (0.1897)\tPrec@1 82.812 (83.241)\tPrec@5 98.438 (98.358)\n",
            "Test: [0/157]\tTime 0.110 (0.110)\tLoss (Class) 1.6662 (1.6662)\tLoss (Regu) 0.2165 (0.2165)\tPrec@1 67.188 (67.188)\tPrec@5 89.062 (89.062)\n",
            "Test: [50/157]\tTime 0.008 (0.010)\tLoss (Class) 1.6099 (1.7280)\tLoss (Regu) 0.2211 (0.2168)\tPrec@1 65.625 (63.940)\tPrec@5 87.500 (88.021)\n",
            "Test: [100/157]\tTime 0.009 (0.009)\tLoss (Class) 1.4018 (1.7243)\tLoss (Regu) 0.2187 (0.2174)\tPrec@1 67.188 (64.124)\tPrec@5 93.750 (88.212)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.6803 (1.7352)\tLoss (Regu) 0.2161 (0.2175)\tPrec@1 60.938 (64.042)\tPrec@5 84.375 (87.997)\n",
            " * Train[83.174 %, 98.364 %, 0.722 loss] Val [64.020 %, 87.940%, 1.739 loss] Best: 65.190 %\n",
            "Time for 118 / 150 19.73139476776123\n",
            "Learning rate:  0.03\n",
            "Epoch: [119][0/782]\tTime 0.150 (0.150)\tLoss (Class) 0.7041 (0.7041)\tLoss (Regu) 0.1874 (0.1874)\tPrec@1 85.938 (85.938)\tPrec@5 98.438 (98.438)\n",
            "Epoch: [119][50/782]\tTime 0.023 (0.026)\tLoss (Class) 0.9749 (0.6885)\tLoss (Regu) 0.1895 (0.1895)\tPrec@1 81.250 (84.436)\tPrec@5 95.312 (98.284)\n",
            "Epoch: [119][100/782]\tTime 0.023 (0.025)\tLoss (Class) 0.5615 (0.6881)\tLoss (Regu) 0.1880 (0.1901)\tPrec@1 84.375 (84.360)\tPrec@5 100.000 (98.530)\n",
            "Epoch: [119][150/782]\tTime 0.022 (0.024)\tLoss (Class) 0.8528 (0.6865)\tLoss (Regu) 0.1871 (0.1902)\tPrec@1 75.000 (84.437)\tPrec@5 100.000 (98.479)\n",
            "Epoch: [119][200/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7226 (0.6898)\tLoss (Regu) 0.1862 (0.1897)\tPrec@1 85.938 (83.986)\tPrec@5 96.875 (98.484)\n",
            "Epoch: [119][250/782]\tTime 0.022 (0.024)\tLoss (Class) 0.7195 (0.6898)\tLoss (Regu) 0.1914 (0.1895)\tPrec@1 81.250 (84.033)\tPrec@5 100.000 (98.518)\n",
            "Epoch: [119][300/782]\tTime 0.022 (0.024)\tLoss (Class) 0.7458 (0.6906)\tLoss (Regu) 0.1905 (0.1895)\tPrec@1 78.125 (83.975)\tPrec@5 100.000 (98.552)\n",
            "Epoch: [119][350/782]\tTime 0.022 (0.024)\tLoss (Class) 0.6661 (0.6912)\tLoss (Regu) 0.1943 (0.1896)\tPrec@1 85.938 (83.930)\tPrec@5 96.875 (98.513)\n",
            "Epoch: [119][400/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6227 (0.6968)\tLoss (Regu) 0.1881 (0.1896)\tPrec@1 89.062 (83.865)\tPrec@5 100.000 (98.473)\n",
            "Epoch: [119][450/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8151 (0.6997)\tLoss (Regu) 0.1884 (0.1895)\tPrec@1 79.688 (83.769)\tPrec@5 95.312 (98.500)\n",
            "Epoch: [119][500/782]\tTime 0.022 (0.024)\tLoss (Class) 0.8017 (0.7013)\tLoss (Regu) 0.1932 (0.1896)\tPrec@1 79.688 (83.717)\tPrec@5 98.438 (98.500)\n",
            "Epoch: [119][550/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7212 (0.7017)\tLoss (Regu) 0.1886 (0.1897)\tPrec@1 79.688 (83.714)\tPrec@5 98.438 (98.506)\n",
            "Epoch: [119][600/782]\tTime 0.024 (0.024)\tLoss (Class) 0.6942 (0.7062)\tLoss (Regu) 0.1894 (0.1896)\tPrec@1 82.812 (83.561)\tPrec@5 100.000 (98.502)\n",
            "Epoch: [119][650/782]\tTime 0.022 (0.024)\tLoss (Class) 0.8195 (0.7099)\tLoss (Regu) 0.1884 (0.1897)\tPrec@1 78.125 (83.386)\tPrec@5 96.875 (98.471)\n",
            "Epoch: [119][700/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8012 (0.7138)\tLoss (Regu) 0.1859 (0.1896)\tPrec@1 78.125 (83.265)\tPrec@5 98.438 (98.417)\n",
            "Epoch: [119][750/782]\tTime 0.032 (0.024)\tLoss (Class) 0.8236 (0.7180)\tLoss (Regu) 0.1896 (0.1897)\tPrec@1 81.250 (83.139)\tPrec@5 95.312 (98.375)\n",
            "Test: [0/157]\tTime 0.113 (0.113)\tLoss (Class) 1.7662 (1.7662)\tLoss (Regu) 0.2183 (0.2183)\tPrec@1 60.938 (60.938)\tPrec@5 89.062 (89.062)\n",
            "Test: [50/157]\tTime 0.007 (0.010)\tLoss (Class) 1.5755 (1.7532)\tLoss (Regu) 0.2228 (0.2198)\tPrec@1 68.750 (63.725)\tPrec@5 87.500 (88.174)\n",
            "Test: [100/157]\tTime 0.009 (0.009)\tLoss (Class) 2.0091 (1.7247)\tLoss (Regu) 0.2207 (0.2200)\tPrec@1 60.938 (64.264)\tPrec@5 84.375 (88.258)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.6690 (1.7058)\tLoss (Regu) 0.2189 (0.2201)\tPrec@1 62.500 (64.808)\tPrec@5 90.625 (88.400)\n",
            " * Train[83.022 %, 98.334 %, 0.721 loss] Val [64.820 %, 88.390%, 1.709 loss] Best: 65.190 %\n",
            "Time for 119 / 150 20.020825147628784\n",
            "Learning rate:  0.03\n",
            "Epoch: [120][0/782]\tTime 0.157 (0.157)\tLoss (Class) 0.6810 (0.6810)\tLoss (Regu) 0.1897 (0.1897)\tPrec@1 87.500 (87.500)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [120][50/782]\tTime 0.023 (0.028)\tLoss (Class) 0.8649 (0.6936)\tLoss (Regu) 0.1907 (0.1922)\tPrec@1 82.812 (84.191)\tPrec@5 98.438 (98.529)\n",
            "Epoch: [120][100/782]\tTime 0.022 (0.027)\tLoss (Class) 0.5965 (0.6887)\tLoss (Regu) 0.1936 (0.1925)\tPrec@1 87.500 (84.329)\tPrec@5 100.000 (98.608)\n",
            "Epoch: [120][150/782]\tTime 0.023 (0.026)\tLoss (Class) 0.4618 (0.6880)\tLoss (Regu) 0.1900 (0.1926)\tPrec@1 93.750 (84.303)\tPrec@5 100.000 (98.675)\n",
            "Epoch: [120][200/782]\tTime 0.023 (0.026)\tLoss (Class) 0.7825 (0.6840)\tLoss (Regu) 0.1945 (0.1923)\tPrec@1 78.125 (84.484)\tPrec@5 100.000 (98.710)\n",
            "Epoch: [120][250/782]\tTime 0.023 (0.026)\tLoss (Class) 0.6949 (0.6851)\tLoss (Regu) 0.1886 (0.1923)\tPrec@1 84.375 (84.412)\tPrec@5 96.875 (98.649)\n",
            "Epoch: [120][300/782]\tTime 0.023 (0.025)\tLoss (Class) 0.6619 (0.6894)\tLoss (Regu) 0.1908 (0.1924)\tPrec@1 82.812 (84.152)\tPrec@5 96.875 (98.609)\n",
            "Epoch: [120][350/782]\tTime 0.022 (0.025)\tLoss (Class) 0.7556 (0.6947)\tLoss (Regu) 0.1879 (0.1922)\tPrec@1 79.688 (84.014)\tPrec@5 100.000 (98.580)\n",
            "Epoch: [120][400/782]\tTime 0.023 (0.025)\tLoss (Class) 0.7640 (0.6978)\tLoss (Regu) 0.1924 (0.1916)\tPrec@1 81.250 (83.978)\tPrec@5 100.000 (98.543)\n",
            "Epoch: [120][450/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8402 (0.7032)\tLoss (Regu) 0.1862 (0.1914)\tPrec@1 82.812 (83.789)\tPrec@5 98.438 (98.503)\n",
            "Epoch: [120][500/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7300 (0.7065)\tLoss (Regu) 0.1867 (0.1913)\tPrec@1 79.688 (83.645)\tPrec@5 100.000 (98.497)\n",
            "Epoch: [120][550/782]\tTime 0.027 (0.024)\tLoss (Class) 0.5840 (0.7087)\tLoss (Regu) 0.1890 (0.1910)\tPrec@1 84.375 (83.570)\tPrec@5 100.000 (98.480)\n",
            "Epoch: [120][600/782]\tTime 0.022 (0.024)\tLoss (Class) 0.6128 (0.7135)\tLoss (Regu) 0.1887 (0.1907)\tPrec@1 87.500 (83.436)\tPrec@5 96.875 (98.450)\n",
            "Epoch: [120][650/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7980 (0.7189)\tLoss (Regu) 0.1872 (0.1905)\tPrec@1 81.250 (83.285)\tPrec@5 95.312 (98.401)\n",
            "Epoch: [120][700/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8913 (0.7220)\tLoss (Regu) 0.1880 (0.1904)\tPrec@1 78.125 (83.189)\tPrec@5 92.188 (98.364)\n",
            "Epoch: [120][750/782]\tTime 0.022 (0.024)\tLoss (Class) 0.6932 (0.7253)\tLoss (Regu) 0.1879 (0.1904)\tPrec@1 84.375 (83.066)\tPrec@5 96.875 (98.342)\n",
            "Test: [0/157]\tTime 0.110 (0.110)\tLoss (Class) 1.6541 (1.6541)\tLoss (Regu) 0.2164 (0.2164)\tPrec@1 54.688 (54.688)\tPrec@5 93.750 (93.750)\n",
            "Test: [50/157]\tTime 0.009 (0.011)\tLoss (Class) 1.7977 (1.8527)\tLoss (Regu) 0.2223 (0.2198)\tPrec@1 62.500 (63.787)\tPrec@5 85.938 (87.868)\n",
            "Test: [100/157]\tTime 0.008 (0.009)\tLoss (Class) 1.9910 (1.8019)\tLoss (Regu) 0.2202 (0.2195)\tPrec@1 62.500 (63.908)\tPrec@5 87.500 (88.382)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.5663 (1.7932)\tLoss (Regu) 0.2170 (0.2195)\tPrec@1 71.875 (63.856)\tPrec@5 90.625 (88.193)\n",
            " * Train[83.024 %, 98.328 %, 0.726 loss] Val [63.920 %, 88.210%, 1.791 loss] Best: 65.190 %\n",
            "Time for 120 / 150 20.31905508041382\n",
            "Learning rate:  0.03\n",
            "Epoch: [121][0/782]\tTime 0.153 (0.153)\tLoss (Class) 0.5508 (0.5508)\tLoss (Regu) 0.1946 (0.1946)\tPrec@1 92.188 (92.188)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [121][50/782]\tTime 0.022 (0.027)\tLoss (Class) 0.6378 (0.7080)\tLoss (Regu) 0.1923 (0.1901)\tPrec@1 90.625 (84.344)\tPrec@5 100.000 (98.407)\n",
            "Epoch: [121][100/782]\tTime 0.023 (0.026)\tLoss (Class) 0.5124 (0.6693)\tLoss (Regu) 0.1933 (0.1912)\tPrec@1 89.062 (84.932)\tPrec@5 100.000 (98.778)\n",
            "Epoch: [121][150/782]\tTime 0.022 (0.025)\tLoss (Class) 0.6563 (0.6598)\tLoss (Regu) 0.1901 (0.1915)\tPrec@1 92.188 (85.182)\tPrec@5 98.438 (98.810)\n",
            "Epoch: [121][200/782]\tTime 0.022 (0.025)\tLoss (Class) 0.5955 (0.6654)\tLoss (Regu) 0.1913 (0.1915)\tPrec@1 85.938 (84.911)\tPrec@5 100.000 (98.795)\n",
            "Epoch: [121][250/782]\tTime 0.022 (0.025)\tLoss (Class) 0.6380 (0.6762)\tLoss (Regu) 0.1917 (0.1912)\tPrec@1 85.938 (84.568)\tPrec@5 98.438 (98.693)\n",
            "Epoch: [121][300/782]\tTime 0.030 (0.024)\tLoss (Class) 0.7107 (0.6820)\tLoss (Regu) 0.1944 (0.1912)\tPrec@1 81.250 (84.453)\tPrec@5 100.000 (98.656)\n",
            "Epoch: [121][350/782]\tTime 0.024 (0.024)\tLoss (Class) 0.7035 (0.6862)\tLoss (Regu) 0.1908 (0.1911)\tPrec@1 82.812 (84.299)\tPrec@5 98.438 (98.633)\n",
            "Epoch: [121][400/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6452 (0.6901)\tLoss (Regu) 0.1925 (0.1908)\tPrec@1 85.938 (84.192)\tPrec@5 98.438 (98.566)\n",
            "Epoch: [121][450/782]\tTime 0.023 (0.024)\tLoss (Class) 0.5960 (0.6925)\tLoss (Regu) 0.1906 (0.1909)\tPrec@1 85.938 (84.108)\tPrec@5 100.000 (98.566)\n",
            "Epoch: [121][500/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6714 (0.6940)\tLoss (Regu) 0.1955 (0.1908)\tPrec@1 81.250 (84.001)\tPrec@5 98.438 (98.553)\n",
            "Epoch: [121][550/782]\tTime 0.023 (0.024)\tLoss (Class) 0.9341 (0.7027)\tLoss (Regu) 0.1913 (0.1906)\tPrec@1 78.125 (83.768)\tPrec@5 98.438 (98.506)\n",
            "Epoch: [121][600/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6524 (0.7086)\tLoss (Regu) 0.1902 (0.1905)\tPrec@1 85.938 (83.553)\tPrec@5 100.000 (98.476)\n",
            "Epoch: [121][650/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8255 (0.7130)\tLoss (Regu) 0.1935 (0.1905)\tPrec@1 71.875 (83.367)\tPrec@5 98.438 (98.462)\n",
            "Epoch: [121][700/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8969 (0.7177)\tLoss (Regu) 0.1888 (0.1905)\tPrec@1 79.688 (83.209)\tPrec@5 95.312 (98.424)\n",
            "Epoch: [121][750/782]\tTime 0.022 (0.024)\tLoss (Class) 0.7354 (0.7195)\tLoss (Regu) 0.1869 (0.1903)\tPrec@1 84.375 (83.143)\tPrec@5 96.875 (98.404)\n",
            "Test: [0/157]\tTime 0.109 (0.109)\tLoss (Class) 1.6545 (1.6545)\tLoss (Regu) 0.2138 (0.2138)\tPrec@1 60.938 (60.938)\tPrec@5 87.500 (87.500)\n",
            "Test: [50/157]\tTime 0.008 (0.010)\tLoss (Class) 1.2251 (1.7472)\tLoss (Regu) 0.2116 (0.2151)\tPrec@1 71.875 (62.806)\tPrec@5 92.188 (88.205)\n",
            "Test: [100/157]\tTime 0.014 (0.010)\tLoss (Class) 1.4937 (1.7275)\tLoss (Regu) 0.2106 (0.2154)\tPrec@1 59.375 (63.196)\tPrec@5 93.750 (88.413)\n",
            "Test: [150/157]\tTime 0.007 (0.010)\tLoss (Class) 2.7446 (1.7457)\tLoss (Regu) 0.2155 (0.2155)\tPrec@1 46.875 (63.307)\tPrec@5 78.125 (88.214)\n",
            " * Train[83.030 %, 98.392 %, 0.723 loss] Val [63.540 %, 88.200%, 1.740 loss] Best: 65.190 %\n",
            "Time for 121 / 150 20.081851959228516\n",
            "Learning rate:  0.03\n",
            "Epoch: [122][0/782]\tTime 0.164 (0.164)\tLoss (Class) 0.5597 (0.5597)\tLoss (Regu) 0.1924 (0.1924)\tPrec@1 87.500 (87.500)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [122][50/782]\tTime 0.023 (0.026)\tLoss (Class) 0.9817 (0.6706)\tLoss (Regu) 0.1938 (0.1935)\tPrec@1 76.562 (84.559)\tPrec@5 96.875 (99.020)\n",
            "Epoch: [122][100/782]\tTime 0.023 (0.025)\tLoss (Class) 0.5480 (0.6719)\tLoss (Regu) 0.1942 (0.1925)\tPrec@1 90.625 (84.870)\tPrec@5 100.000 (98.871)\n",
            "Epoch: [122][150/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7929 (0.6661)\tLoss (Regu) 0.1911 (0.1924)\tPrec@1 82.812 (85.141)\tPrec@5 98.438 (98.800)\n",
            "Epoch: [122][200/782]\tTime 0.023 (0.024)\tLoss (Class) 0.5073 (0.6638)\tLoss (Regu) 0.1880 (0.1918)\tPrec@1 90.625 (85.207)\tPrec@5 100.000 (98.826)\n",
            "Epoch: [122][250/782]\tTime 0.022 (0.024)\tLoss (Class) 0.6178 (0.6665)\tLoss (Regu) 0.1893 (0.1914)\tPrec@1 90.625 (85.091)\tPrec@5 98.438 (98.792)\n",
            "Epoch: [122][300/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6413 (0.6715)\tLoss (Regu) 0.1901 (0.1910)\tPrec@1 82.812 (84.821)\tPrec@5 98.438 (98.733)\n",
            "Epoch: [122][350/782]\tTime 0.022 (0.024)\tLoss (Class) 0.8210 (0.6767)\tLoss (Regu) 0.1914 (0.1910)\tPrec@1 84.375 (84.629)\tPrec@5 95.312 (98.718)\n",
            "Epoch: [122][400/782]\tTime 0.022 (0.023)\tLoss (Class) 0.6869 (0.6814)\tLoss (Regu) 0.1911 (0.1908)\tPrec@1 82.812 (84.426)\tPrec@5 100.000 (98.648)\n",
            "Epoch: [122][450/782]\tTime 0.023 (0.023)\tLoss (Class) 0.9448 (0.6888)\tLoss (Regu) 0.1900 (0.1906)\tPrec@1 76.562 (84.181)\tPrec@5 98.438 (98.600)\n",
            "Epoch: [122][500/782]\tTime 0.025 (0.023)\tLoss (Class) 0.6032 (0.6950)\tLoss (Regu) 0.1889 (0.1905)\tPrec@1 85.938 (84.001)\tPrec@5 96.875 (98.534)\n",
            "Epoch: [122][550/782]\tTime 0.030 (0.024)\tLoss (Class) 0.6604 (0.6996)\tLoss (Regu) 0.1888 (0.1904)\tPrec@1 85.938 (83.836)\tPrec@5 98.438 (98.500)\n",
            "Epoch: [122][600/782]\tTime 0.023 (0.024)\tLoss (Class) 0.5138 (0.7036)\tLoss (Regu) 0.1893 (0.1902)\tPrec@1 93.750 (83.730)\tPrec@5 100.000 (98.471)\n",
            "Epoch: [122][650/782]\tTime 0.032 (0.024)\tLoss (Class) 0.8531 (0.7095)\tLoss (Regu) 0.1901 (0.1901)\tPrec@1 76.562 (83.473)\tPrec@5 98.438 (98.452)\n",
            "Epoch: [122][700/782]\tTime 0.032 (0.024)\tLoss (Class) 0.6644 (0.7115)\tLoss (Regu) 0.1847 (0.1899)\tPrec@1 81.250 (83.468)\tPrec@5 98.438 (98.409)\n",
            "Epoch: [122][750/782]\tTime 0.022 (0.024)\tLoss (Class) 0.6324 (0.7139)\tLoss (Regu) 0.1876 (0.1897)\tPrec@1 87.500 (83.351)\tPrec@5 98.438 (98.408)\n",
            "Test: [0/157]\tTime 0.119 (0.119)\tLoss (Class) 1.8951 (1.8951)\tLoss (Regu) 0.2230 (0.2230)\tPrec@1 67.188 (67.188)\tPrec@5 85.938 (85.938)\n",
            "Test: [50/157]\tTime 0.008 (0.010)\tLoss (Class) 2.2548 (1.7388)\tLoss (Regu) 0.2177 (0.2200)\tPrec@1 59.375 (64.154)\tPrec@5 82.812 (88.174)\n",
            "Test: [100/157]\tTime 0.008 (0.009)\tLoss (Class) 1.8593 (1.7396)\tLoss (Regu) 0.2230 (0.2201)\tPrec@1 70.312 (63.970)\tPrec@5 89.062 (88.583)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 2.2141 (1.7544)\tLoss (Regu) 0.2187 (0.2200)\tPrec@1 57.812 (63.649)\tPrec@5 85.938 (88.555)\n",
            " * Train[83.282 %, 98.392 %, 0.716 loss] Val [63.740 %, 88.490%, 1.757 loss] Best: 65.190 %\n",
            "Time for 122 / 150 20.329488515853882\n",
            "Learning rate:  0.03\n",
            "Epoch: [123][0/782]\tTime 0.147 (0.147)\tLoss (Class) 0.7167 (0.7167)\tLoss (Regu) 0.1876 (0.1876)\tPrec@1 81.250 (81.250)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [123][50/782]\tTime 0.023 (0.025)\tLoss (Class) 0.6559 (0.6298)\tLoss (Regu) 0.1886 (0.1901)\tPrec@1 87.500 (87.224)\tPrec@5 98.438 (99.050)\n",
            "Epoch: [123][100/782]\tTime 0.027 (0.024)\tLoss (Class) 0.5813 (0.6369)\tLoss (Regu) 0.1917 (0.1903)\tPrec@1 85.938 (86.309)\tPrec@5 100.000 (98.963)\n",
            "Epoch: [123][150/782]\tTime 0.022 (0.025)\tLoss (Class) 0.6200 (0.6595)\tLoss (Regu) 0.1869 (0.1905)\tPrec@1 85.938 (85.555)\tPrec@5 100.000 (98.810)\n",
            "Epoch: [123][200/782]\tTime 0.023 (0.025)\tLoss (Class) 0.6846 (0.6697)\tLoss (Regu) 0.1911 (0.1902)\tPrec@1 84.375 (85.090)\tPrec@5 100.000 (98.702)\n",
            "Epoch: [123][250/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6042 (0.6720)\tLoss (Regu) 0.1884 (0.1900)\tPrec@1 90.625 (84.892)\tPrec@5 96.875 (98.724)\n",
            "Epoch: [123][300/782]\tTime 0.023 (0.024)\tLoss (Class) 1.0139 (0.6776)\tLoss (Regu) 0.1913 (0.1899)\tPrec@1 76.562 (84.738)\tPrec@5 96.875 (98.635)\n",
            "Epoch: [123][350/782]\tTime 0.031 (0.024)\tLoss (Class) 0.5835 (0.6846)\tLoss (Regu) 0.1908 (0.1900)\tPrec@1 90.625 (84.477)\tPrec@5 98.438 (98.593)\n",
            "Epoch: [123][400/782]\tTime 0.023 (0.024)\tLoss (Class) 0.5571 (0.6877)\tLoss (Regu) 0.1872 (0.1899)\tPrec@1 87.500 (84.398)\tPrec@5 100.000 (98.570)\n",
            "Epoch: [123][450/782]\tTime 0.030 (0.024)\tLoss (Class) 0.7142 (0.6936)\tLoss (Regu) 0.1838 (0.1898)\tPrec@1 87.500 (84.205)\tPrec@5 96.875 (98.548)\n",
            "Epoch: [123][500/782]\tTime 0.022 (0.024)\tLoss (Class) 0.9153 (0.6934)\tLoss (Regu) 0.1901 (0.1896)\tPrec@1 81.250 (84.256)\tPrec@5 95.312 (98.537)\n",
            "Epoch: [123][550/782]\tTime 0.024 (0.024)\tLoss (Class) 0.7149 (0.6972)\tLoss (Regu) 0.1875 (0.1896)\tPrec@1 84.375 (84.060)\tPrec@5 98.438 (98.531)\n",
            "Epoch: [123][600/782]\tTime 0.023 (0.024)\tLoss (Class) 0.9111 (0.7005)\tLoss (Regu) 0.1895 (0.1896)\tPrec@1 76.562 (83.899)\tPrec@5 96.875 (98.523)\n",
            "Epoch: [123][650/782]\tTime 0.021 (0.024)\tLoss (Class) 0.7306 (0.7043)\tLoss (Regu) 0.1903 (0.1896)\tPrec@1 85.938 (83.821)\tPrec@5 100.000 (98.483)\n",
            "Epoch: [123][700/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6958 (0.7090)\tLoss (Regu) 0.1921 (0.1896)\tPrec@1 78.125 (83.664)\tPrec@5 96.875 (98.435)\n",
            "Epoch: [123][750/782]\tTime 0.021 (0.024)\tLoss (Class) 0.9246 (0.7118)\tLoss (Regu) 0.1839 (0.1897)\tPrec@1 73.438 (83.593)\tPrec@5 95.312 (98.406)\n",
            "Test: [0/157]\tTime 0.116 (0.116)\tLoss (Class) 1.2927 (1.2927)\tLoss (Regu) 0.2238 (0.2238)\tPrec@1 73.438 (73.438)\tPrec@5 90.625 (90.625)\n",
            "Test: [50/157]\tTime 0.009 (0.010)\tLoss (Class) 1.7402 (1.7241)\tLoss (Regu) 0.2266 (0.2235)\tPrec@1 62.500 (65.012)\tPrec@5 87.500 (88.940)\n",
            "Test: [100/157]\tTime 0.010 (0.010)\tLoss (Class) 1.5128 (1.7139)\tLoss (Regu) 0.2183 (0.2236)\tPrec@1 70.312 (64.929)\tPrec@5 95.312 (89.109)\n",
            "Test: [150/157]\tTime 0.007 (0.010)\tLoss (Class) 1.7093 (1.7165)\tLoss (Regu) 0.2242 (0.2234)\tPrec@1 59.375 (64.983)\tPrec@5 90.625 (89.145)\n",
            " * Train[83.500 %, 98.392 %, 0.714 loss] Val [65.010 %, 89.120%, 1.714 loss] Best: 65.190 %\n",
            "Time for 123 / 150 20.51655888557434\n",
            "Learning rate:  0.03\n",
            "Epoch: [124][0/782]\tTime 0.157 (0.157)\tLoss (Class) 0.5418 (0.5418)\tLoss (Regu) 0.1890 (0.1890)\tPrec@1 89.062 (89.062)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [124][50/782]\tTime 0.023 (0.026)\tLoss (Class) 0.7148 (0.6976)\tLoss (Regu) 0.1896 (0.1903)\tPrec@1 81.250 (84.375)\tPrec@5 98.438 (98.652)\n",
            "Epoch: [124][100/782]\tTime 0.023 (0.024)\tLoss (Class) 0.5672 (0.6893)\tLoss (Regu) 0.1951 (0.1905)\tPrec@1 85.938 (84.329)\tPrec@5 100.000 (98.731)\n",
            "Epoch: [124][150/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7450 (0.6929)\tLoss (Regu) 0.1931 (0.1906)\tPrec@1 82.812 (84.147)\tPrec@5 98.438 (98.603)\n",
            "Epoch: [124][200/782]\tTime 0.024 (0.024)\tLoss (Class) 0.7695 (0.6911)\tLoss (Regu) 0.1898 (0.1908)\tPrec@1 81.250 (84.251)\tPrec@5 100.000 (98.523)\n",
            "Epoch: [124][250/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6047 (0.6897)\tLoss (Regu) 0.1907 (0.1911)\tPrec@1 84.375 (84.238)\tPrec@5 100.000 (98.593)\n",
            "Epoch: [124][300/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6982 (0.6964)\tLoss (Regu) 0.1890 (0.1911)\tPrec@1 82.812 (84.084)\tPrec@5 98.438 (98.531)\n",
            "Epoch: [124][350/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7433 (0.6938)\tLoss (Regu) 0.1884 (0.1910)\tPrec@1 82.812 (84.210)\tPrec@5 98.438 (98.522)\n",
            "Epoch: [124][400/782]\tTime 0.023 (0.023)\tLoss (Class) 0.8563 (0.6930)\tLoss (Regu) 0.1896 (0.1911)\tPrec@1 73.438 (84.250)\tPrec@5 98.438 (98.535)\n",
            "Epoch: [124][450/782]\tTime 0.021 (0.023)\tLoss (Class) 0.7169 (0.6980)\tLoss (Regu) 0.1909 (0.1909)\tPrec@1 81.250 (84.122)\tPrec@5 100.000 (98.496)\n",
            "Epoch: [124][500/782]\tTime 0.022 (0.023)\tLoss (Class) 0.7652 (0.6998)\tLoss (Regu) 0.1942 (0.1909)\tPrec@1 84.375 (84.032)\tPrec@5 100.000 (98.497)\n",
            "Epoch: [124][550/782]\tTime 0.024 (0.023)\tLoss (Class) 0.6862 (0.7003)\tLoss (Regu) 0.1868 (0.1909)\tPrec@1 82.812 (83.967)\tPrec@5 100.000 (98.480)\n",
            "Epoch: [124][600/782]\tTime 0.022 (0.023)\tLoss (Class) 0.6387 (0.7015)\tLoss (Regu) 0.1950 (0.1910)\tPrec@1 82.812 (83.907)\tPrec@5 100.000 (98.497)\n",
            "Epoch: [124][650/782]\tTime 0.022 (0.023)\tLoss (Class) 0.8575 (0.7028)\tLoss (Regu) 0.1888 (0.1910)\tPrec@1 79.688 (83.818)\tPrec@5 93.750 (98.486)\n",
            "Epoch: [124][700/782]\tTime 0.032 (0.023)\tLoss (Class) 0.5675 (0.7064)\tLoss (Regu) 0.1864 (0.1908)\tPrec@1 89.062 (83.684)\tPrec@5 98.438 (98.478)\n",
            "Epoch: [124][750/782]\tTime 0.022 (0.023)\tLoss (Class) 0.8378 (0.7126)\tLoss (Regu) 0.1879 (0.1906)\tPrec@1 79.688 (83.478)\tPrec@5 100.000 (98.442)\n",
            "Test: [0/157]\tTime 0.110 (0.110)\tLoss (Class) 2.2537 (2.2537)\tLoss (Regu) 0.2123 (0.2123)\tPrec@1 53.125 (53.125)\tPrec@5 84.375 (84.375)\n",
            "Test: [50/157]\tTime 0.007 (0.010)\tLoss (Class) 1.8804 (1.7366)\tLoss (Regu) 0.2082 (0.2121)\tPrec@1 65.625 (64.951)\tPrec@5 85.938 (88.113)\n",
            "Test: [100/157]\tTime 0.007 (0.009)\tLoss (Class) 1.7210 (1.7613)\tLoss (Regu) 0.2105 (0.2122)\tPrec@1 56.250 (64.016)\tPrec@5 85.938 (87.887)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 2.2707 (1.7551)\tLoss (Regu) 0.2133 (0.2122)\tPrec@1 57.812 (63.742)\tPrec@5 84.375 (87.707)\n",
            " * Train[83.438 %, 98.434 %, 0.714 loss] Val [63.860 %, 87.770%, 1.752 loss] Best: 65.190 %\n",
            "Time for 124 / 150 19.83800745010376\n",
            "Learning rate:  0.03\n",
            "Epoch: [125][0/782]\tTime 0.156 (0.156)\tLoss (Class) 0.7324 (0.7324)\tLoss (Regu) 0.1849 (0.1849)\tPrec@1 84.375 (84.375)\tPrec@5 98.438 (98.438)\n",
            "Epoch: [125][50/782]\tTime 0.030 (0.028)\tLoss (Class) 0.6521 (0.6791)\tLoss (Regu) 0.1916 (0.1886)\tPrec@1 84.375 (84.957)\tPrec@5 98.438 (98.621)\n",
            "Epoch: [125][100/782]\tTime 0.022 (0.027)\tLoss (Class) 0.8143 (0.6859)\tLoss (Regu) 0.1897 (0.1885)\tPrec@1 81.250 (84.158)\tPrec@5 96.875 (98.577)\n",
            "Epoch: [125][150/782]\tTime 0.030 (0.026)\tLoss (Class) 0.6215 (0.6718)\tLoss (Regu) 0.1875 (0.1893)\tPrec@1 90.625 (84.727)\tPrec@5 100.000 (98.727)\n",
            "Epoch: [125][200/782]\tTime 0.024 (0.026)\tLoss (Class) 0.5989 (0.6676)\tLoss (Regu) 0.1889 (0.1897)\tPrec@1 85.938 (84.981)\tPrec@5 98.438 (98.733)\n",
            "Epoch: [125][250/782]\tTime 0.029 (0.025)\tLoss (Class) 0.9187 (0.6817)\tLoss (Regu) 0.1891 (0.1893)\tPrec@1 71.875 (84.680)\tPrec@5 95.312 (98.662)\n",
            "Epoch: [125][300/782]\tTime 0.022 (0.025)\tLoss (Class) 0.6006 (0.6855)\tLoss (Regu) 0.1899 (0.1891)\tPrec@1 87.500 (84.536)\tPrec@5 100.000 (98.687)\n",
            "Epoch: [125][350/782]\tTime 0.024 (0.025)\tLoss (Class) 0.5821 (0.6831)\tLoss (Regu) 0.1875 (0.1891)\tPrec@1 87.500 (84.526)\tPrec@5 98.438 (98.696)\n",
            "Epoch: [125][400/782]\tTime 0.022 (0.025)\tLoss (Class) 0.5870 (0.6887)\tLoss (Regu) 0.1898 (0.1889)\tPrec@1 87.500 (84.320)\tPrec@5 98.438 (98.652)\n",
            "Epoch: [125][450/782]\tTime 0.023 (0.025)\tLoss (Class) 1.0480 (0.6919)\tLoss (Regu) 0.1862 (0.1891)\tPrec@1 75.000 (84.101)\tPrec@5 93.750 (98.611)\n",
            "Epoch: [125][500/782]\tTime 0.032 (0.024)\tLoss (Class) 0.7873 (0.6967)\tLoss (Regu) 0.1880 (0.1892)\tPrec@1 81.250 (84.044)\tPrec@5 98.438 (98.537)\n",
            "Epoch: [125][550/782]\tTime 0.023 (0.024)\tLoss (Class) 0.9749 (0.7003)\tLoss (Regu) 0.1904 (0.1892)\tPrec@1 76.562 (83.969)\tPrec@5 98.438 (98.500)\n",
            "Epoch: [125][600/782]\tTime 0.031 (0.024)\tLoss (Class) 0.7439 (0.7028)\tLoss (Regu) 0.1909 (0.1894)\tPrec@1 81.250 (83.829)\tPrec@5 98.438 (98.476)\n",
            "Epoch: [125][650/782]\tTime 0.023 (0.025)\tLoss (Class) 0.7189 (0.7067)\tLoss (Regu) 0.1885 (0.1895)\tPrec@1 87.500 (83.725)\tPrec@5 96.875 (98.464)\n",
            "Epoch: [125][700/782]\tTime 0.033 (0.025)\tLoss (Class) 0.7302 (0.7112)\tLoss (Regu) 0.1893 (0.1893)\tPrec@1 79.688 (83.559)\tPrec@5 100.000 (98.440)\n",
            "Epoch: [125][750/782]\tTime 0.022 (0.025)\tLoss (Class) 0.6992 (0.7145)\tLoss (Regu) 0.1919 (0.1895)\tPrec@1 78.125 (83.472)\tPrec@5 96.875 (98.388)\n",
            "Test: [0/157]\tTime 0.121 (0.121)\tLoss (Class) 1.2654 (1.2654)\tLoss (Regu) 0.2125 (0.2125)\tPrec@1 64.062 (64.062)\tPrec@5 92.188 (92.188)\n",
            "Test: [50/157]\tTime 0.008 (0.011)\tLoss (Class) 1.6030 (1.6414)\tLoss (Regu) 0.2074 (0.2132)\tPrec@1 70.312 (65.319)\tPrec@5 85.938 (89.246)\n",
            "Test: [100/157]\tTime 0.008 (0.009)\tLoss (Class) 1.2105 (1.6684)\tLoss (Regu) 0.2107 (0.2130)\tPrec@1 75.000 (64.882)\tPrec@5 93.750 (88.753)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.7985 (1.6818)\tLoss (Regu) 0.2110 (0.2133)\tPrec@1 60.938 (64.808)\tPrec@5 82.812 (88.845)\n",
            " * Train[83.388 %, 98.358 %, 0.717 loss] Val [64.800 %, 88.780%, 1.683 loss] Best: 65.190 %\n",
            "Time for 125 / 150 20.78347373008728\n",
            "Learning rate:  0.03\n",
            "Epoch: [126][0/782]\tTime 0.152 (0.152)\tLoss (Class) 0.5239 (0.5239)\tLoss (Regu) 0.1897 (0.1897)\tPrec@1 89.062 (89.062)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [126][50/782]\tTime 0.023 (0.025)\tLoss (Class) 0.7323 (0.6817)\tLoss (Regu) 0.1941 (0.1935)\tPrec@1 79.688 (84.559)\tPrec@5 98.438 (98.897)\n",
            "Epoch: [126][100/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8695 (0.6662)\tLoss (Regu) 0.1923 (0.1932)\tPrec@1 76.562 (85.025)\tPrec@5 98.438 (98.917)\n",
            "Epoch: [126][150/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6480 (0.6642)\tLoss (Regu) 0.1890 (0.1928)\tPrec@1 87.500 (84.934)\tPrec@5 100.000 (98.945)\n",
            "Epoch: [126][200/782]\tTime 0.022 (0.023)\tLoss (Class) 0.8156 (0.6727)\tLoss (Regu) 0.1867 (0.1921)\tPrec@1 81.250 (84.600)\tPrec@5 100.000 (98.857)\n",
            "Epoch: [126][250/782]\tTime 0.022 (0.023)\tLoss (Class) 0.6617 (0.6779)\tLoss (Regu) 0.1868 (0.1917)\tPrec@1 84.375 (84.500)\tPrec@5 100.000 (98.743)\n",
            "Epoch: [126][300/782]\tTime 0.022 (0.023)\tLoss (Class) 0.7754 (0.6814)\tLoss (Regu) 0.1911 (0.1915)\tPrec@1 79.688 (84.282)\tPrec@5 98.438 (98.723)\n",
            "Epoch: [126][350/782]\tTime 0.026 (0.023)\tLoss (Class) 0.7236 (0.6887)\tLoss (Regu) 0.1907 (0.1913)\tPrec@1 81.250 (84.010)\tPrec@5 98.438 (98.687)\n",
            "Epoch: [126][400/782]\tTime 0.022 (0.023)\tLoss (Class) 0.6873 (0.6934)\tLoss (Regu) 0.1893 (0.1910)\tPrec@1 84.375 (83.915)\tPrec@5 100.000 (98.652)\n",
            "Epoch: [126][450/782]\tTime 0.024 (0.023)\tLoss (Class) 0.7538 (0.6940)\tLoss (Regu) 0.1897 (0.1910)\tPrec@1 84.375 (83.880)\tPrec@5 96.875 (98.666)\n",
            "Epoch: [126][500/782]\tTime 0.023 (0.023)\tLoss (Class) 0.5086 (0.6975)\tLoss (Regu) 0.1887 (0.1907)\tPrec@1 90.625 (83.807)\tPrec@5 100.000 (98.643)\n",
            "Epoch: [126][550/782]\tTime 0.021 (0.023)\tLoss (Class) 0.7647 (0.7017)\tLoss (Regu) 0.1886 (0.1904)\tPrec@1 81.250 (83.649)\tPrec@5 100.000 (98.627)\n",
            "Epoch: [126][600/782]\tTime 0.023 (0.023)\tLoss (Class) 0.6169 (0.7026)\tLoss (Regu) 0.1877 (0.1904)\tPrec@1 87.500 (83.564)\tPrec@5 98.438 (98.601)\n",
            "Epoch: [126][650/782]\tTime 0.022 (0.023)\tLoss (Class) 0.7862 (0.7057)\tLoss (Regu) 0.1855 (0.1902)\tPrec@1 76.562 (83.444)\tPrec@5 98.438 (98.579)\n",
            "Epoch: [126][700/782]\tTime 0.022 (0.023)\tLoss (Class) 0.5655 (0.7076)\tLoss (Regu) 0.1877 (0.1900)\tPrec@1 87.500 (83.341)\tPrec@5 98.438 (98.567)\n",
            "Epoch: [126][750/782]\tTime 0.023 (0.023)\tLoss (Class) 0.8084 (0.7115)\tLoss (Regu) 0.1856 (0.1898)\tPrec@1 76.562 (83.245)\tPrec@5 96.875 (98.531)\n",
            "Test: [0/157]\tTime 0.106 (0.106)\tLoss (Class) 1.6054 (1.6054)\tLoss (Regu) 0.2186 (0.2186)\tPrec@1 62.500 (62.500)\tPrec@5 85.938 (85.938)\n",
            "Test: [50/157]\tTime 0.008 (0.010)\tLoss (Class) 1.1932 (1.7269)\tLoss (Regu) 0.2163 (0.2180)\tPrec@1 75.000 (64.645)\tPrec@5 93.750 (88.542)\n",
            "Test: [100/157]\tTime 0.008 (0.009)\tLoss (Class) 1.5013 (1.7208)\tLoss (Regu) 0.2160 (0.2179)\tPrec@1 62.500 (64.325)\tPrec@5 87.500 (88.784)\n",
            "Test: [150/157]\tTime 0.007 (0.008)\tLoss (Class) 2.2940 (1.7205)\tLoss (Regu) 0.2204 (0.2178)\tPrec@1 56.250 (64.269)\tPrec@5 82.812 (88.721)\n",
            " * Train[83.198 %, 98.510 %, 0.713 loss] Val [64.320 %, 88.820%, 1.717 loss] Best: 65.190 %\n",
            "Time for 126 / 150 19.497427463531494\n",
            "Learning rate:  0.03\n",
            "Epoch: [127][0/782]\tTime 0.142 (0.142)\tLoss (Class) 0.6757 (0.6757)\tLoss (Regu) 0.1872 (0.1872)\tPrec@1 85.938 (85.938)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [127][50/782]\tTime 0.023 (0.025)\tLoss (Class) 0.6226 (0.6545)\tLoss (Regu) 0.1881 (0.1915)\tPrec@1 85.938 (85.018)\tPrec@5 100.000 (99.142)\n",
            "Epoch: [127][100/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7742 (0.6550)\tLoss (Regu) 0.1902 (0.1919)\tPrec@1 82.812 (85.040)\tPrec@5 95.312 (98.933)\n",
            "Epoch: [127][150/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7885 (0.6633)\tLoss (Regu) 0.1908 (0.1915)\tPrec@1 82.812 (85.141)\tPrec@5 98.438 (98.769)\n",
            "Epoch: [127][200/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8081 (0.6698)\tLoss (Regu) 0.1854 (0.1907)\tPrec@1 79.688 (85.036)\tPrec@5 96.875 (98.725)\n",
            "Epoch: [127][250/782]\tTime 0.023 (0.023)\tLoss (Class) 0.5945 (0.6670)\tLoss (Regu) 0.1905 (0.1904)\tPrec@1 90.625 (85.265)\tPrec@5 98.438 (98.736)\n",
            "Epoch: [127][300/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6819 (0.6741)\tLoss (Regu) 0.1870 (0.1902)\tPrec@1 84.375 (84.910)\tPrec@5 100.000 (98.718)\n",
            "Epoch: [127][350/782]\tTime 0.024 (0.024)\tLoss (Class) 0.8653 (0.6796)\tLoss (Regu) 0.1886 (0.1901)\tPrec@1 75.000 (84.696)\tPrec@5 98.438 (98.696)\n",
            "Epoch: [127][400/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7738 (0.6808)\tLoss (Regu) 0.1874 (0.1900)\tPrec@1 79.688 (84.624)\tPrec@5 96.875 (98.675)\n",
            "Epoch: [127][450/782]\tTime 0.022 (0.024)\tLoss (Class) 1.0711 (0.6855)\tLoss (Regu) 0.1877 (0.1897)\tPrec@1 70.312 (84.496)\tPrec@5 96.875 (98.621)\n",
            "Epoch: [127][500/782]\tTime 0.023 (0.024)\tLoss (Class) 0.5220 (0.6885)\tLoss (Regu) 0.1925 (0.1895)\tPrec@1 89.062 (84.319)\tPrec@5 98.438 (98.612)\n",
            "Epoch: [127][550/782]\tTime 0.021 (0.024)\tLoss (Class) 0.6645 (0.6931)\tLoss (Regu) 0.1846 (0.1895)\tPrec@1 90.625 (84.208)\tPrec@5 95.312 (98.576)\n",
            "Epoch: [127][600/782]\tTime 0.021 (0.025)\tLoss (Class) 0.7571 (0.6965)\tLoss (Regu) 0.1879 (0.1894)\tPrec@1 78.125 (84.084)\tPrec@5 98.438 (98.536)\n",
            "Epoch: [127][650/782]\tTime 0.023 (0.025)\tLoss (Class) 0.7531 (0.6963)\tLoss (Regu) 0.1896 (0.1893)\tPrec@1 79.688 (84.092)\tPrec@5 100.000 (98.531)\n",
            "Epoch: [127][700/782]\tTime 0.023 (0.025)\tLoss (Class) 0.6243 (0.6984)\tLoss (Regu) 0.1877 (0.1892)\tPrec@1 85.938 (84.003)\tPrec@5 100.000 (98.531)\n",
            "Epoch: [127][750/782]\tTime 0.032 (0.025)\tLoss (Class) 0.6625 (0.7021)\tLoss (Regu) 0.1877 (0.1891)\tPrec@1 85.938 (83.876)\tPrec@5 95.312 (98.502)\n",
            "Test: [0/157]\tTime 0.111 (0.111)\tLoss (Class) 2.1731 (2.1731)\tLoss (Regu) 0.2140 (0.2140)\tPrec@1 56.250 (56.250)\tPrec@5 85.938 (85.938)\n",
            "Test: [50/157]\tTime 0.008 (0.010)\tLoss (Class) 2.5718 (1.8694)\tLoss (Regu) 0.2183 (0.2152)\tPrec@1 53.125 (62.286)\tPrec@5 78.125 (86.734)\n",
            "Test: [100/157]\tTime 0.008 (0.009)\tLoss (Class) 1.5822 (1.8989)\tLoss (Regu) 0.2126 (0.2150)\tPrec@1 67.188 (61.680)\tPrec@5 84.375 (85.984)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.9915 (1.8731)\tLoss (Regu) 0.2153 (0.2150)\tPrec@1 68.750 (61.889)\tPrec@5 81.250 (86.527)\n",
            " * Train[83.818 %, 98.498 %, 0.704 loss] Val [61.960 %, 86.520%, 1.878 loss] Best: 65.190 %\n",
            "Time for 127 / 150 20.764002323150635\n",
            "Learning rate:  0.03\n",
            "Epoch: [128][0/782]\tTime 0.150 (0.150)\tLoss (Class) 0.8548 (0.8548)\tLoss (Regu) 0.1859 (0.1859)\tPrec@1 79.688 (79.688)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [128][50/782]\tTime 0.023 (0.027)\tLoss (Class) 0.6200 (0.6960)\tLoss (Regu) 0.1901 (0.1901)\tPrec@1 87.500 (84.191)\tPrec@5 98.438 (98.560)\n",
            "Epoch: [128][100/782]\tTime 0.023 (0.026)\tLoss (Class) 0.6001 (0.6666)\tLoss (Regu) 0.1914 (0.1905)\tPrec@1 87.500 (84.978)\tPrec@5 96.875 (98.685)\n",
            "Epoch: [128][150/782]\tTime 0.023 (0.025)\tLoss (Class) 0.5126 (0.6676)\tLoss (Regu) 0.1875 (0.1904)\tPrec@1 92.188 (84.654)\tPrec@5 100.000 (98.738)\n",
            "Epoch: [128][200/782]\tTime 0.023 (0.024)\tLoss (Class) 0.4534 (0.6695)\tLoss (Regu) 0.1933 (0.1903)\tPrec@1 92.188 (84.686)\tPrec@5 100.000 (98.733)\n",
            "Epoch: [128][250/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7657 (0.6688)\tLoss (Regu) 0.1898 (0.1907)\tPrec@1 81.250 (84.755)\tPrec@5 98.438 (98.755)\n",
            "Epoch: [128][300/782]\tTime 0.023 (0.024)\tLoss (Class) 0.9676 (0.6761)\tLoss (Regu) 0.1883 (0.1905)\tPrec@1 71.875 (84.442)\tPrec@5 96.875 (98.671)\n",
            "Epoch: [128][350/782]\tTime 0.022 (0.024)\tLoss (Class) 1.0220 (0.6802)\tLoss (Regu) 0.1893 (0.1906)\tPrec@1 71.875 (84.357)\tPrec@5 93.750 (98.629)\n",
            "Epoch: [128][400/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6800 (0.6821)\tLoss (Regu) 0.1878 (0.1906)\tPrec@1 89.062 (84.340)\tPrec@5 96.875 (98.621)\n",
            "Epoch: [128][450/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6833 (0.6857)\tLoss (Regu) 0.1915 (0.1907)\tPrec@1 84.375 (84.212)\tPrec@5 98.438 (98.597)\n",
            "Epoch: [128][500/782]\tTime 0.022 (0.024)\tLoss (Class) 0.7826 (0.6870)\tLoss (Regu) 0.1888 (0.1907)\tPrec@1 79.688 (84.213)\tPrec@5 96.875 (98.572)\n",
            "Epoch: [128][550/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8263 (0.6902)\tLoss (Regu) 0.1920 (0.1907)\tPrec@1 82.812 (84.162)\tPrec@5 98.438 (98.562)\n",
            "Epoch: [128][600/782]\tTime 0.023 (0.024)\tLoss (Class) 1.0248 (0.6946)\tLoss (Regu) 0.1907 (0.1908)\tPrec@1 68.750 (83.980)\tPrec@5 96.875 (98.528)\n",
            "Epoch: [128][650/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6602 (0.6972)\tLoss (Regu) 0.1931 (0.1908)\tPrec@1 85.938 (83.869)\tPrec@5 96.875 (98.514)\n",
            "Epoch: [128][700/782]\tTime 0.022 (0.024)\tLoss (Class) 0.8334 (0.6982)\tLoss (Regu) 0.1904 (0.1908)\tPrec@1 81.250 (83.898)\tPrec@5 95.312 (98.509)\n",
            "Epoch: [128][750/782]\tTime 0.032 (0.024)\tLoss (Class) 0.7824 (0.7014)\tLoss (Regu) 0.1887 (0.1907)\tPrec@1 82.812 (83.799)\tPrec@5 98.438 (98.483)\n",
            "Test: [0/157]\tTime 0.113 (0.113)\tLoss (Class) 2.2449 (2.2449)\tLoss (Regu) 0.2159 (0.2159)\tPrec@1 64.062 (64.062)\tPrec@5 78.125 (78.125)\n",
            "Test: [50/157]\tTime 0.007 (0.010)\tLoss (Class) 1.4261 (1.7495)\tLoss (Regu) 0.2169 (0.2164)\tPrec@1 70.312 (64.246)\tPrec@5 92.188 (88.879)\n",
            "Test: [100/157]\tTime 0.008 (0.009)\tLoss (Class) 1.4033 (1.7601)\tLoss (Regu) 0.2174 (0.2171)\tPrec@1 62.500 (63.861)\tPrec@5 92.188 (88.351)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.9343 (1.7725)\tLoss (Regu) 0.2176 (0.2174)\tPrec@1 64.062 (64.073)\tPrec@5 87.500 (88.069)\n",
            " * Train[83.784 %, 98.476 %, 0.703 loss] Val [64.060 %, 88.120%, 1.772 loss] Best: 65.190 %\n",
            "Time for 128 / 150 20.33711266517639\n",
            "Learning rate:  0.03\n",
            "Epoch: [129][0/782]\tTime 0.149 (0.149)\tLoss (Class) 0.7110 (0.7110)\tLoss (Regu) 0.1897 (0.1897)\tPrec@1 85.938 (85.938)\tPrec@5 96.875 (96.875)\n",
            "Epoch: [129][50/782]\tTime 0.023 (0.027)\tLoss (Class) 0.6905 (0.6967)\tLoss (Regu) 0.1920 (0.1899)\tPrec@1 84.375 (84.436)\tPrec@5 98.438 (98.621)\n",
            "Epoch: [129][100/782]\tTime 0.023 (0.026)\tLoss (Class) 0.7606 (0.6657)\tLoss (Regu) 0.1937 (0.1906)\tPrec@1 87.500 (85.210)\tPrec@5 98.438 (98.700)\n",
            "Epoch: [129][150/782]\tTime 0.022 (0.025)\tLoss (Class) 0.5878 (0.6626)\tLoss (Regu) 0.1866 (0.1910)\tPrec@1 87.500 (85.089)\tPrec@5 100.000 (98.696)\n",
            "Epoch: [129][200/782]\tTime 0.023 (0.025)\tLoss (Class) 0.6391 (0.6632)\tLoss (Regu) 0.1892 (0.1914)\tPrec@1 85.938 (85.082)\tPrec@5 96.875 (98.678)\n",
            "Epoch: [129][250/782]\tTime 0.022 (0.025)\tLoss (Class) 0.6868 (0.6671)\tLoss (Regu) 0.1929 (0.1911)\tPrec@1 82.812 (84.991)\tPrec@5 100.000 (98.655)\n",
            "Epoch: [129][300/782]\tTime 0.033 (0.025)\tLoss (Class) 0.5313 (0.6639)\tLoss (Regu) 0.1876 (0.1909)\tPrec@1 90.625 (85.045)\tPrec@5 98.438 (98.687)\n",
            "Epoch: [129][350/782]\tTime 0.030 (0.025)\tLoss (Class) 0.5678 (0.6659)\tLoss (Regu) 0.1853 (0.1905)\tPrec@1 87.500 (84.882)\tPrec@5 100.000 (98.731)\n",
            "Epoch: [129][400/782]\tTime 0.030 (0.025)\tLoss (Class) 0.6284 (0.6711)\tLoss (Regu) 0.1885 (0.1905)\tPrec@1 85.938 (84.644)\tPrec@5 98.438 (98.734)\n",
            "Epoch: [129][450/782]\tTime 0.023 (0.025)\tLoss (Class) 0.6441 (0.6783)\tLoss (Regu) 0.1883 (0.1902)\tPrec@1 89.062 (84.420)\tPrec@5 98.438 (98.656)\n",
            "Epoch: [129][500/782]\tTime 0.022 (0.025)\tLoss (Class) 0.7266 (0.6841)\tLoss (Regu) 0.1870 (0.1901)\tPrec@1 81.250 (84.228)\tPrec@5 98.438 (98.631)\n",
            "Epoch: [129][550/782]\tTime 0.030 (0.025)\tLoss (Class) 0.7162 (0.6907)\tLoss (Regu) 0.1907 (0.1900)\tPrec@1 78.125 (84.015)\tPrec@5 100.000 (98.591)\n",
            "Epoch: [129][600/782]\tTime 0.022 (0.025)\tLoss (Class) 0.6499 (0.6955)\tLoss (Regu) 0.1879 (0.1899)\tPrec@1 92.188 (83.850)\tPrec@5 96.875 (98.544)\n",
            "Epoch: [129][650/782]\tTime 0.030 (0.025)\tLoss (Class) 1.0352 (0.6981)\tLoss (Regu) 0.1914 (0.1901)\tPrec@1 71.875 (83.744)\tPrec@5 98.438 (98.536)\n",
            "Epoch: [129][700/782]\tTime 0.022 (0.025)\tLoss (Class) 0.7898 (0.7011)\tLoss (Regu) 0.1924 (0.1900)\tPrec@1 79.688 (83.702)\tPrec@5 96.875 (98.480)\n",
            "Epoch: [129][750/782]\tTime 0.030 (0.025)\tLoss (Class) 0.6402 (0.7033)\tLoss (Regu) 0.1938 (0.1900)\tPrec@1 84.375 (83.636)\tPrec@5 98.438 (98.446)\n",
            "Test: [0/157]\tTime 0.121 (0.121)\tLoss (Class) 1.8149 (1.8149)\tLoss (Regu) 0.2216 (0.2216)\tPrec@1 62.500 (62.500)\tPrec@5 89.062 (89.062)\n",
            "Test: [50/157]\tTime 0.007 (0.011)\tLoss (Class) 1.6927 (1.8018)\tLoss (Regu) 0.2115 (0.2224)\tPrec@1 70.312 (63.143)\tPrec@5 81.250 (88.419)\n",
            "Test: [100/157]\tTime 0.008 (0.010)\tLoss (Class) 2.2516 (1.8356)\tLoss (Regu) 0.2167 (0.2219)\tPrec@1 53.125 (62.995)\tPrec@5 87.500 (88.227)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 2.1744 (1.8403)\tLoss (Regu) 0.2264 (0.2220)\tPrec@1 65.625 (62.924)\tPrec@5 90.625 (88.255)\n",
            " * Train[83.562 %, 98.436 %, 0.706 loss] Val [63.000 %, 88.300%, 1.835 loss] Best: 65.190 %\n",
            "Time for 129 / 150 20.763742685317993\n",
            "Learning rate:  0.03\n",
            "Epoch: [130][0/782]\tTime 0.155 (0.155)\tLoss (Class) 0.8006 (0.8006)\tLoss (Regu) 0.1878 (0.1878)\tPrec@1 73.438 (73.438)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [130][50/782]\tTime 0.023 (0.026)\tLoss (Class) 0.5476 (0.7100)\tLoss (Regu) 0.1886 (0.1908)\tPrec@1 85.938 (82.935)\tPrec@5 100.000 (98.407)\n",
            "Epoch: [130][100/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7279 (0.6886)\tLoss (Regu) 0.1898 (0.1909)\tPrec@1 84.375 (84.127)\tPrec@5 96.875 (98.546)\n",
            "Epoch: [130][150/782]\tTime 0.022 (0.024)\tLoss (Class) 0.7385 (0.6777)\tLoss (Regu) 0.1897 (0.1908)\tPrec@1 78.125 (84.447)\tPrec@5 100.000 (98.644)\n",
            "Epoch: [130][200/782]\tTime 0.022 (0.024)\tLoss (Class) 0.7380 (0.6807)\tLoss (Regu) 0.1860 (0.1906)\tPrec@1 82.812 (84.414)\tPrec@5 96.875 (98.616)\n",
            "Epoch: [130][250/782]\tTime 0.023 (0.024)\tLoss (Class) 0.4990 (0.6773)\tLoss (Regu) 0.1972 (0.1910)\tPrec@1 92.188 (84.605)\tPrec@5 100.000 (98.649)\n",
            "Epoch: [130][300/782]\tTime 0.023 (0.024)\tLoss (Class) 0.5596 (0.6824)\tLoss (Regu) 0.1897 (0.1907)\tPrec@1 87.500 (84.432)\tPrec@5 100.000 (98.671)\n",
            "Epoch: [130][350/782]\tTime 0.022 (0.024)\tLoss (Class) 0.7049 (0.6859)\tLoss (Regu) 0.1870 (0.1903)\tPrec@1 81.250 (84.273)\tPrec@5 100.000 (98.633)\n",
            "Epoch: [130][400/782]\tTime 0.022 (0.024)\tLoss (Class) 0.6361 (0.6876)\tLoss (Regu) 0.1917 (0.1901)\tPrec@1 85.938 (84.258)\tPrec@5 96.875 (98.632)\n",
            "Epoch: [130][450/782]\tTime 0.021 (0.024)\tLoss (Class) 0.7019 (0.6932)\tLoss (Regu) 0.1856 (0.1900)\tPrec@1 84.375 (84.049)\tPrec@5 96.875 (98.580)\n",
            "Epoch: [130][500/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7574 (0.6945)\tLoss (Regu) 0.1952 (0.1899)\tPrec@1 81.250 (83.966)\tPrec@5 96.875 (98.547)\n",
            "Epoch: [130][550/782]\tTime 0.022 (0.024)\tLoss (Class) 0.6789 (0.6998)\tLoss (Regu) 0.1874 (0.1898)\tPrec@1 79.688 (83.819)\tPrec@5 100.000 (98.497)\n",
            "Epoch: [130][600/782]\tTime 0.022 (0.024)\tLoss (Class) 0.8307 (0.7025)\tLoss (Regu) 0.1871 (0.1895)\tPrec@1 81.250 (83.795)\tPrec@5 95.312 (98.474)\n",
            "Epoch: [130][650/782]\tTime 0.021 (0.024)\tLoss (Class) 0.6045 (0.7076)\tLoss (Regu) 0.1877 (0.1894)\tPrec@1 85.938 (83.585)\tPrec@5 100.000 (98.409)\n",
            "Epoch: [130][700/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7345 (0.7101)\tLoss (Regu) 0.1876 (0.1893)\tPrec@1 79.688 (83.508)\tPrec@5 100.000 (98.380)\n",
            "Epoch: [130][750/782]\tTime 0.025 (0.024)\tLoss (Class) 0.7884 (0.7126)\tLoss (Regu) 0.1875 (0.1892)\tPrec@1 79.688 (83.451)\tPrec@5 96.875 (98.356)\n",
            "Test: [0/157]\tTime 0.118 (0.118)\tLoss (Class) 1.7673 (1.7673)\tLoss (Regu) 0.2208 (0.2208)\tPrec@1 62.500 (62.500)\tPrec@5 90.625 (90.625)\n",
            "Test: [50/157]\tTime 0.007 (0.010)\tLoss (Class) 1.8990 (1.7827)\tLoss (Regu) 0.2202 (0.2181)\tPrec@1 57.812 (63.725)\tPrec@5 85.938 (87.837)\n",
            "Test: [100/157]\tTime 0.009 (0.009)\tLoss (Class) 2.1913 (1.8007)\tLoss (Regu) 0.2149 (0.2173)\tPrec@1 56.250 (63.274)\tPrec@5 82.812 (87.763)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.7326 (1.8047)\tLoss (Regu) 0.2196 (0.2173)\tPrec@1 60.938 (63.048)\tPrec@5 87.500 (87.666)\n",
            " * Train[83.418 %, 98.344 %, 0.714 loss] Val [62.910 %, 87.650%, 1.806 loss] Best: 65.190 %\n",
            "Time for 130 / 150 20.277782440185547\n",
            "Learning rate:  0.03\n",
            "Epoch: [131][0/782]\tTime 0.151 (0.151)\tLoss (Class) 0.7294 (0.7294)\tLoss (Regu) 0.1923 (0.1923)\tPrec@1 84.375 (84.375)\tPrec@5 98.438 (98.438)\n",
            "Epoch: [131][50/782]\tTime 0.022 (0.026)\tLoss (Class) 0.6450 (0.7096)\tLoss (Regu) 0.1899 (0.1877)\tPrec@1 82.812 (83.854)\tPrec@5 98.438 (98.407)\n",
            "Epoch: [131][100/782]\tTime 0.024 (0.025)\tLoss (Class) 0.6186 (0.6774)\tLoss (Regu) 0.1908 (0.1888)\tPrec@1 87.500 (84.576)\tPrec@5 98.438 (98.608)\n",
            "Epoch: [131][150/782]\tTime 0.030 (0.025)\tLoss (Class) 0.5640 (0.6683)\tLoss (Regu) 0.1934 (0.1894)\tPrec@1 87.500 (84.779)\tPrec@5 100.000 (98.696)\n",
            "Epoch: [131][200/782]\tTime 0.021 (0.025)\tLoss (Class) 0.5241 (0.6688)\tLoss (Regu) 0.1932 (0.1896)\tPrec@1 90.625 (84.935)\tPrec@5 100.000 (98.717)\n",
            "Epoch: [131][250/782]\tTime 0.022 (0.025)\tLoss (Class) 0.7279 (0.6707)\tLoss (Regu) 0.1950 (0.1900)\tPrec@1 82.812 (84.892)\tPrec@5 95.312 (98.618)\n",
            "Epoch: [131][300/782]\tTime 0.021 (0.025)\tLoss (Class) 0.6445 (0.6733)\tLoss (Regu) 0.1927 (0.1898)\tPrec@1 85.938 (84.858)\tPrec@5 98.438 (98.640)\n",
            "Epoch: [131][350/782]\tTime 0.022 (0.025)\tLoss (Class) 0.7542 (0.6783)\tLoss (Regu) 0.1864 (0.1898)\tPrec@1 84.375 (84.669)\tPrec@5 96.875 (98.647)\n",
            "Epoch: [131][400/782]\tTime 0.021 (0.025)\tLoss (Class) 0.7173 (0.6797)\tLoss (Regu) 0.1899 (0.1899)\tPrec@1 85.938 (84.706)\tPrec@5 95.312 (98.644)\n",
            "Epoch: [131][450/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8093 (0.6858)\tLoss (Regu) 0.1847 (0.1897)\tPrec@1 78.125 (84.437)\tPrec@5 98.438 (98.590)\n",
            "Epoch: [131][500/782]\tTime 0.022 (0.024)\tLoss (Class) 0.7657 (0.6875)\tLoss (Regu) 0.1872 (0.1897)\tPrec@1 82.812 (84.384)\tPrec@5 98.438 (98.578)\n",
            "Epoch: [131][550/782]\tTime 0.022 (0.024)\tLoss (Class) 0.6282 (0.6917)\tLoss (Regu) 0.1920 (0.1896)\tPrec@1 85.938 (84.199)\tPrec@5 98.438 (98.574)\n",
            "Epoch: [131][600/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6681 (0.6960)\tLoss (Regu) 0.1943 (0.1899)\tPrec@1 84.375 (84.063)\tPrec@5 100.000 (98.544)\n",
            "Epoch: [131][650/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8947 (0.7002)\tLoss (Regu) 0.1891 (0.1898)\tPrec@1 75.000 (83.969)\tPrec@5 93.750 (98.514)\n",
            "Epoch: [131][700/782]\tTime 0.022 (0.024)\tLoss (Class) 0.5185 (0.7046)\tLoss (Regu) 0.1868 (0.1896)\tPrec@1 93.750 (83.813)\tPrec@5 100.000 (98.484)\n",
            "Epoch: [131][750/782]\tTime 0.022 (0.024)\tLoss (Class) 0.6804 (0.7078)\tLoss (Regu) 0.1902 (0.1896)\tPrec@1 84.375 (83.672)\tPrec@5 98.438 (98.460)\n",
            "Test: [0/157]\tTime 0.120 (0.120)\tLoss (Class) 0.9295 (0.9295)\tLoss (Regu) 0.2160 (0.2160)\tPrec@1 79.688 (79.688)\tPrec@5 96.875 (96.875)\n",
            "Test: [50/157]\tTime 0.014 (0.013)\tLoss (Class) 1.8295 (1.8019)\tLoss (Regu) 0.2206 (0.2144)\tPrec@1 51.562 (62.745)\tPrec@5 90.625 (87.224)\n",
            "Test: [100/157]\tTime 0.009 (0.011)\tLoss (Class) 1.8405 (1.7980)\tLoss (Regu) 0.2178 (0.2148)\tPrec@1 64.062 (62.980)\tPrec@5 87.500 (87.438)\n",
            "Test: [150/157]\tTime 0.008 (0.011)\tLoss (Class) 1.2792 (1.8094)\tLoss (Regu) 0.2190 (0.2151)\tPrec@1 73.438 (62.821)\tPrec@5 92.188 (87.190)\n",
            " * Train[83.626 %, 98.436 %, 0.710 loss] Val [62.920 %, 87.280%, 1.802 loss] Best: 65.190 %\n",
            "Time for 131 / 150 20.613835334777832\n",
            "Learning rate:  0.03\n",
            "Epoch: [132][0/782]\tTime 0.148 (0.148)\tLoss (Class) 0.8874 (0.8874)\tLoss (Regu) 0.1861 (0.1861)\tPrec@1 78.125 (78.125)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [132][50/782]\tTime 0.023 (0.027)\tLoss (Class) 0.4658 (0.6727)\tLoss (Regu) 0.1892 (0.1917)\tPrec@1 90.625 (84.130)\tPrec@5 100.000 (98.744)\n",
            "Epoch: [132][100/782]\tTime 0.025 (0.025)\tLoss (Class) 0.5931 (0.6542)\tLoss (Regu) 0.1876 (0.1911)\tPrec@1 85.938 (85.303)\tPrec@5 100.000 (98.762)\n",
            "Epoch: [132][150/782]\tTime 0.022 (0.024)\tLoss (Class) 0.6791 (0.6491)\tLoss (Regu) 0.1892 (0.1914)\tPrec@1 84.375 (85.648)\tPrec@5 95.312 (98.758)\n",
            "Epoch: [132][200/782]\tTime 0.027 (0.024)\tLoss (Class) 0.7831 (0.6616)\tLoss (Regu) 0.1906 (0.1909)\tPrec@1 79.688 (85.152)\tPrec@5 100.000 (98.663)\n",
            "Epoch: [132][250/782]\tTime 0.023 (0.025)\tLoss (Class) 0.7707 (0.6689)\tLoss (Regu) 0.1886 (0.1907)\tPrec@1 82.812 (85.060)\tPrec@5 98.438 (98.680)\n",
            "Epoch: [132][300/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7334 (0.6687)\tLoss (Regu) 0.1870 (0.1907)\tPrec@1 84.375 (85.039)\tPrec@5 100.000 (98.671)\n",
            "Epoch: [132][350/782]\tTime 0.024 (0.024)\tLoss (Class) 0.8918 (0.6727)\tLoss (Regu) 0.1906 (0.1906)\tPrec@1 75.000 (84.878)\tPrec@5 100.000 (98.665)\n",
            "Epoch: [132][400/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6237 (0.6763)\tLoss (Regu) 0.1897 (0.1906)\tPrec@1 84.375 (84.741)\tPrec@5 98.438 (98.652)\n",
            "Epoch: [132][450/782]\tTime 0.025 (0.024)\tLoss (Class) 0.7111 (0.6788)\tLoss (Regu) 0.1866 (0.1904)\tPrec@1 78.125 (84.604)\tPrec@5 100.000 (98.638)\n",
            "Epoch: [132][500/782]\tTime 0.022 (0.024)\tLoss (Class) 0.7103 (0.6804)\tLoss (Regu) 0.1891 (0.1904)\tPrec@1 82.812 (84.556)\tPrec@5 98.438 (98.625)\n",
            "Epoch: [132][550/782]\tTime 0.022 (0.024)\tLoss (Class) 0.7990 (0.6841)\tLoss (Regu) 0.1890 (0.1902)\tPrec@1 76.562 (84.443)\tPrec@5 98.438 (98.593)\n",
            "Epoch: [132][600/782]\tTime 0.022 (0.024)\tLoss (Class) 0.6337 (0.6874)\tLoss (Regu) 0.1866 (0.1901)\tPrec@1 87.500 (84.297)\tPrec@5 98.438 (98.583)\n",
            "Epoch: [132][650/782]\tTime 0.024 (0.024)\tLoss (Class) 0.7802 (0.6919)\tLoss (Regu) 0.1919 (0.1900)\tPrec@1 82.812 (84.173)\tPrec@5 100.000 (98.548)\n",
            "Epoch: [132][700/782]\tTime 0.024 (0.024)\tLoss (Class) 0.6205 (0.6942)\tLoss (Regu) 0.1832 (0.1899)\tPrec@1 87.500 (84.103)\tPrec@5 100.000 (98.551)\n",
            "Epoch: [132][750/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8019 (0.6954)\tLoss (Regu) 0.1855 (0.1898)\tPrec@1 82.812 (84.042)\tPrec@5 96.875 (98.535)\n",
            "Test: [0/157]\tTime 0.109 (0.109)\tLoss (Class) 1.6205 (1.6205)\tLoss (Regu) 0.2187 (0.2187)\tPrec@1 65.625 (65.625)\tPrec@5 98.438 (98.438)\n",
            "Test: [50/157]\tTime 0.007 (0.010)\tLoss (Class) 1.2578 (1.7795)\tLoss (Regu) 0.2169 (0.2149)\tPrec@1 67.188 (62.316)\tPrec@5 95.312 (88.358)\n",
            "Test: [100/157]\tTime 0.008 (0.009)\tLoss (Class) 1.4021 (1.7412)\tLoss (Regu) 0.2084 (0.2155)\tPrec@1 71.875 (63.583)\tPrec@5 93.750 (88.769)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.2230 (1.7292)\tLoss (Regu) 0.2176 (0.2155)\tPrec@1 68.750 (64.000)\tPrec@5 93.750 (88.649)\n",
            " * Train[83.986 %, 98.526 %, 0.697 loss] Val [63.970 %, 88.640%, 1.730 loss] Best: 65.190 %\n",
            "Time for 132 / 150 20.065396547317505\n",
            "Learning rate:  0.03\n",
            "Epoch: [133][0/782]\tTime 0.166 (0.166)\tLoss (Class) 0.7181 (0.7181)\tLoss (Regu) 0.1896 (0.1896)\tPrec@1 84.375 (84.375)\tPrec@5 98.438 (98.438)\n",
            "Epoch: [133][50/782]\tTime 0.023 (0.028)\tLoss (Class) 0.6252 (0.6761)\tLoss (Regu) 0.1884 (0.1905)\tPrec@1 85.938 (84.835)\tPrec@5 100.000 (98.652)\n",
            "Epoch: [133][100/782]\tTime 0.023 (0.026)\tLoss (Class) 0.6658 (0.6528)\tLoss (Regu) 0.1904 (0.1914)\tPrec@1 87.500 (85.427)\tPrec@5 96.875 (98.917)\n",
            "Epoch: [133][150/782]\tTime 0.032 (0.026)\tLoss (Class) 0.5314 (0.6491)\tLoss (Regu) 0.1934 (0.1909)\tPrec@1 89.062 (85.503)\tPrec@5 100.000 (98.945)\n",
            "Epoch: [133][200/782]\tTime 0.023 (0.026)\tLoss (Class) 0.6369 (0.6600)\tLoss (Regu) 0.1884 (0.1908)\tPrec@1 89.062 (85.176)\tPrec@5 98.438 (98.857)\n",
            "Epoch: [133][250/782]\tTime 0.033 (0.026)\tLoss (Class) 0.4418 (0.6652)\tLoss (Regu) 0.1929 (0.1904)\tPrec@1 93.750 (85.004)\tPrec@5 100.000 (98.842)\n",
            "Epoch: [133][300/782]\tTime 0.023 (0.026)\tLoss (Class) 0.6657 (0.6690)\tLoss (Regu) 0.1906 (0.1906)\tPrec@1 82.812 (84.764)\tPrec@5 98.438 (98.775)\n",
            "Epoch: [133][350/782]\tTime 0.022 (0.025)\tLoss (Class) 0.6512 (0.6697)\tLoss (Regu) 0.1893 (0.1906)\tPrec@1 81.250 (84.762)\tPrec@5 100.000 (98.740)\n",
            "Epoch: [133][400/782]\tTime 0.027 (0.025)\tLoss (Class) 0.7349 (0.6694)\tLoss (Regu) 0.1896 (0.1906)\tPrec@1 82.812 (84.796)\tPrec@5 98.438 (98.765)\n",
            "Epoch: [133][450/782]\tTime 0.023 (0.025)\tLoss (Class) 0.6176 (0.6727)\tLoss (Regu) 0.1914 (0.1905)\tPrec@1 87.500 (84.649)\tPrec@5 100.000 (98.753)\n",
            "Epoch: [133][500/782]\tTime 0.022 (0.025)\tLoss (Class) 0.4924 (0.6806)\tLoss (Regu) 0.1886 (0.1903)\tPrec@1 89.062 (84.409)\tPrec@5 100.000 (98.662)\n",
            "Epoch: [133][550/782]\tTime 0.032 (0.025)\tLoss (Class) 0.8009 (0.6845)\tLoss (Regu) 0.1916 (0.1902)\tPrec@1 79.688 (84.236)\tPrec@5 98.438 (98.622)\n",
            "Epoch: [133][600/782]\tTime 0.022 (0.025)\tLoss (Class) 0.8598 (0.6896)\tLoss (Regu) 0.1885 (0.1901)\tPrec@1 73.438 (84.060)\tPrec@5 98.438 (98.604)\n",
            "Epoch: [133][650/782]\tTime 0.032 (0.025)\tLoss (Class) 0.7371 (0.6917)\tLoss (Regu) 0.1888 (0.1901)\tPrec@1 84.375 (83.986)\tPrec@5 96.875 (98.584)\n",
            "Epoch: [133][700/782]\tTime 0.022 (0.025)\tLoss (Class) 0.5466 (0.6939)\tLoss (Regu) 0.1907 (0.1901)\tPrec@1 87.500 (83.873)\tPrec@5 100.000 (98.589)\n",
            "Epoch: [133][750/782]\tTime 0.023 (0.025)\tLoss (Class) 0.6701 (0.6995)\tLoss (Regu) 0.1854 (0.1902)\tPrec@1 84.375 (83.722)\tPrec@5 100.000 (98.558)\n",
            "Test: [0/157]\tTime 0.121 (0.121)\tLoss (Class) 1.5368 (1.5368)\tLoss (Regu) 0.2167 (0.2167)\tPrec@1 64.062 (64.062)\tPrec@5 90.625 (90.625)\n",
            "Test: [50/157]\tTime 0.008 (0.011)\tLoss (Class) 1.6018 (1.7821)\tLoss (Regu) 0.2199 (0.2181)\tPrec@1 67.188 (62.561)\tPrec@5 90.625 (87.714)\n",
            "Test: [100/157]\tTime 0.008 (0.011)\tLoss (Class) 1.8202 (1.7782)\tLoss (Regu) 0.2180 (0.2176)\tPrec@1 75.000 (62.748)\tPrec@5 87.500 (88.057)\n",
            "Test: [150/157]\tTime 0.008 (0.011)\tLoss (Class) 2.0332 (1.7777)\tLoss (Regu) 0.2156 (0.2176)\tPrec@1 57.812 (62.924)\tPrec@5 85.938 (88.059)\n",
            " * Train[83.666 %, 98.542 %, 0.702 loss] Val [62.840 %, 87.970%, 1.779 loss] Best: 65.190 %\n",
            "Time for 133 / 150 21.09011936187744\n",
            "Learning rate:  0.03\n",
            "Epoch: [134][0/782]\tTime 0.145 (0.145)\tLoss (Class) 0.6077 (0.6077)\tLoss (Regu) 0.1909 (0.1909)\tPrec@1 85.938 (85.938)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [134][50/782]\tTime 0.022 (0.025)\tLoss (Class) 0.6196 (0.6797)\tLoss (Regu) 0.1911 (0.1928)\tPrec@1 85.938 (84.559)\tPrec@5 100.000 (98.744)\n",
            "Epoch: [134][100/782]\tTime 0.022 (0.024)\tLoss (Class) 0.7828 (0.6771)\tLoss (Regu) 0.1907 (0.1923)\tPrec@1 81.250 (84.406)\tPrec@5 100.000 (98.793)\n",
            "Epoch: [134][150/782]\tTime 0.022 (0.024)\tLoss (Class) 0.7856 (0.6694)\tLoss (Regu) 0.1857 (0.1920)\tPrec@1 78.125 (84.799)\tPrec@5 98.438 (98.696)\n",
            "Epoch: [134][200/782]\tTime 0.024 (0.024)\tLoss (Class) 0.6788 (0.6656)\tLoss (Regu) 0.1920 (0.1919)\tPrec@1 84.375 (84.865)\tPrec@5 100.000 (98.733)\n",
            "Epoch: [134][250/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6078 (0.6676)\tLoss (Regu) 0.1918 (0.1918)\tPrec@1 82.812 (84.823)\tPrec@5 98.438 (98.724)\n",
            "Epoch: [134][300/782]\tTime 0.022 (0.024)\tLoss (Class) 0.5673 (0.6634)\tLoss (Regu) 0.1900 (0.1919)\tPrec@1 89.062 (84.967)\tPrec@5 98.438 (98.770)\n",
            "Epoch: [134][350/782]\tTime 0.031 (0.024)\tLoss (Class) 0.5281 (0.6653)\tLoss (Regu) 0.1907 (0.1914)\tPrec@1 92.188 (84.834)\tPrec@5 98.438 (98.736)\n",
            "Epoch: [134][400/782]\tTime 0.022 (0.024)\tLoss (Class) 0.7215 (0.6706)\tLoss (Regu) 0.1857 (0.1910)\tPrec@1 81.250 (84.585)\tPrec@5 98.438 (98.706)\n",
            "Epoch: [134][450/782]\tTime 0.035 (0.024)\tLoss (Class) 0.6747 (0.6729)\tLoss (Regu) 0.1912 (0.1906)\tPrec@1 89.062 (84.590)\tPrec@5 98.438 (98.677)\n",
            "Epoch: [134][500/782]\tTime 0.022 (0.024)\tLoss (Class) 0.6774 (0.6753)\tLoss (Regu) 0.1846 (0.1904)\tPrec@1 87.500 (84.465)\tPrec@5 96.875 (98.650)\n",
            "Epoch: [134][550/782]\tTime 0.032 (0.024)\tLoss (Class) 0.7929 (0.6793)\tLoss (Regu) 0.1921 (0.1902)\tPrec@1 84.375 (84.327)\tPrec@5 95.312 (98.625)\n",
            "Epoch: [134][600/782]\tTime 0.024 (0.024)\tLoss (Class) 0.6818 (0.6847)\tLoss (Regu) 0.1885 (0.1901)\tPrec@1 87.500 (84.138)\tPrec@5 98.438 (98.573)\n",
            "Epoch: [134][650/782]\tTime 0.031 (0.024)\tLoss (Class) 0.6651 (0.6885)\tLoss (Regu) 0.1875 (0.1899)\tPrec@1 85.938 (84.053)\tPrec@5 98.438 (98.558)\n",
            "Epoch: [134][700/782]\tTime 0.022 (0.024)\tLoss (Class) 0.6629 (0.6908)\tLoss (Regu) 0.1884 (0.1897)\tPrec@1 81.250 (83.996)\tPrec@5 100.000 (98.551)\n",
            "Epoch: [134][750/782]\tTime 0.030 (0.024)\tLoss (Class) 0.7108 (0.6947)\tLoss (Regu) 0.1909 (0.1896)\tPrec@1 85.938 (83.899)\tPrec@5 95.312 (98.508)\n",
            "Test: [0/157]\tTime 0.107 (0.107)\tLoss (Class) 1.6454 (1.6454)\tLoss (Regu) 0.2179 (0.2179)\tPrec@1 62.500 (62.500)\tPrec@5 92.188 (92.188)\n",
            "Test: [50/157]\tTime 0.008 (0.010)\tLoss (Class) 1.8503 (1.8031)\tLoss (Regu) 0.2137 (0.2148)\tPrec@1 65.625 (62.623)\tPrec@5 87.500 (87.806)\n",
            "Test: [100/157]\tTime 0.008 (0.010)\tLoss (Class) 1.6405 (1.8383)\tLoss (Regu) 0.2161 (0.2149)\tPrec@1 62.500 (62.299)\tPrec@5 93.750 (87.314)\n",
            "Test: [150/157]\tTime 0.007 (0.010)\tLoss (Class) 1.4323 (1.8204)\tLoss (Regu) 0.2196 (0.2151)\tPrec@1 75.000 (62.873)\tPrec@5 93.750 (87.666)\n",
            " * Train[83.820 %, 98.510 %, 0.697 loss] Val [62.760 %, 87.640%, 1.826 loss] Best: 65.190 %\n",
            "Time for 134 / 150 20.610677242279053\n",
            "Learning rate:  0.03\n",
            "Epoch: [135][0/782]\tTime 0.147 (0.147)\tLoss (Class) 0.8063 (0.8063)\tLoss (Regu) 0.1872 (0.1872)\tPrec@1 78.125 (78.125)\tPrec@5 98.438 (98.438)\n",
            "Epoch: [135][50/782]\tTime 0.023 (0.027)\tLoss (Class) 0.6770 (0.6700)\tLoss (Regu) 0.1873 (0.1886)\tPrec@1 87.500 (84.559)\tPrec@5 98.438 (98.744)\n",
            "Epoch: [135][100/782]\tTime 0.022 (0.025)\tLoss (Class) 0.6210 (0.6639)\tLoss (Regu) 0.1915 (0.1902)\tPrec@1 90.625 (85.133)\tPrec@5 98.438 (98.716)\n",
            "Epoch: [135][150/782]\tTime 0.023 (0.024)\tLoss (Class) 0.5134 (0.6546)\tLoss (Regu) 0.1907 (0.1912)\tPrec@1 90.625 (85.296)\tPrec@5 98.438 (98.800)\n",
            "Epoch: [135][200/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6913 (0.6536)\tLoss (Regu) 0.1893 (0.1907)\tPrec@1 84.375 (85.362)\tPrec@5 96.875 (98.842)\n",
            "Epoch: [135][250/782]\tTime 0.022 (0.024)\tLoss (Class) 0.6017 (0.6499)\tLoss (Regu) 0.1907 (0.1906)\tPrec@1 87.500 (85.433)\tPrec@5 100.000 (98.911)\n",
            "Epoch: [135][300/782]\tTime 0.030 (0.024)\tLoss (Class) 0.5139 (0.6541)\tLoss (Regu) 0.1900 (0.1903)\tPrec@1 90.625 (85.320)\tPrec@5 96.875 (98.879)\n",
            "Epoch: [135][350/782]\tTime 0.030 (0.025)\tLoss (Class) 0.5745 (0.6608)\tLoss (Regu) 0.1912 (0.1902)\tPrec@1 85.938 (85.101)\tPrec@5 100.000 (98.798)\n",
            "Epoch: [135][400/782]\tTime 0.022 (0.025)\tLoss (Class) 0.5820 (0.6634)\tLoss (Regu) 0.1847 (0.1903)\tPrec@1 85.938 (85.002)\tPrec@5 98.438 (98.757)\n",
            "Epoch: [135][450/782]\tTime 0.024 (0.025)\tLoss (Class) 0.6906 (0.6690)\tLoss (Regu) 0.1924 (0.1901)\tPrec@1 84.375 (84.860)\tPrec@5 100.000 (98.715)\n",
            "Epoch: [135][500/782]\tTime 0.023 (0.025)\tLoss (Class) 0.7578 (0.6747)\tLoss (Regu) 0.1872 (0.1900)\tPrec@1 78.125 (84.665)\tPrec@5 98.438 (98.693)\n",
            "Epoch: [135][550/782]\tTime 0.022 (0.025)\tLoss (Class) 0.6254 (0.6787)\tLoss (Regu) 0.1901 (0.1900)\tPrec@1 87.500 (84.562)\tPrec@5 98.438 (98.693)\n",
            "Epoch: [135][600/782]\tTime 0.030 (0.025)\tLoss (Class) 0.5642 (0.6808)\tLoss (Regu) 0.1929 (0.1901)\tPrec@1 87.500 (84.422)\tPrec@5 98.438 (98.669)\n",
            "Epoch: [135][650/782]\tTime 0.022 (0.025)\tLoss (Class) 0.5501 (0.6845)\tLoss (Regu) 0.1929 (0.1900)\tPrec@1 87.500 (84.277)\tPrec@5 100.000 (98.663)\n",
            "Epoch: [135][700/782]\tTime 0.030 (0.025)\tLoss (Class) 0.6519 (0.6891)\tLoss (Regu) 0.1912 (0.1900)\tPrec@1 85.938 (84.154)\tPrec@5 100.000 (98.602)\n",
            "Epoch: [135][750/782]\tTime 0.022 (0.024)\tLoss (Class) 0.8420 (0.6910)\tLoss (Regu) 0.1890 (0.1900)\tPrec@1 81.250 (84.048)\tPrec@5 98.438 (98.577)\n",
            "Test: [0/157]\tTime 0.119 (0.119)\tLoss (Class) 0.9115 (0.9115)\tLoss (Regu) 0.2161 (0.2161)\tPrec@1 75.000 (75.000)\tPrec@5 96.875 (96.875)\n",
            "Test: [50/157]\tTime 0.008 (0.011)\tLoss (Class) 1.6952 (1.6345)\tLoss (Regu) 0.2115 (0.2162)\tPrec@1 68.750 (65.962)\tPrec@5 89.062 (89.706)\n",
            "Test: [100/157]\tTime 0.008 (0.009)\tLoss (Class) 1.2446 (1.6571)\tLoss (Regu) 0.2127 (0.2163)\tPrec@1 75.000 (65.733)\tPrec@5 92.188 (89.480)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.8691 (1.6373)\tLoss (Regu) 0.2151 (0.2165)\tPrec@1 70.312 (66.132)\tPrec@5 89.062 (89.694)\n",
            " * Train[84.016 %, 98.564 %, 0.692 loss] Val [66.040 %, 89.680%, 1.641 loss] Best: 66.040 %\n",
            "Time for 135 / 150 20.59165382385254\n",
            "Learning rate:  0.03\n",
            "Epoch: [136][0/782]\tTime 0.147 (0.147)\tLoss (Class) 0.6101 (0.6101)\tLoss (Regu) 0.1896 (0.1896)\tPrec@1 92.188 (92.188)\tPrec@5 98.438 (98.438)\n",
            "Epoch: [136][50/782]\tTime 0.023 (0.027)\tLoss (Class) 0.8005 (0.6841)\tLoss (Regu) 0.1867 (0.1903)\tPrec@1 79.688 (84.130)\tPrec@5 98.438 (98.775)\n",
            "Epoch: [136][100/782]\tTime 0.023 (0.026)\tLoss (Class) 0.5245 (0.6680)\tLoss (Regu) 0.1888 (0.1904)\tPrec@1 92.188 (84.901)\tPrec@5 100.000 (98.762)\n",
            "Epoch: [136][150/782]\tTime 0.024 (0.025)\tLoss (Class) 0.4890 (0.6536)\tLoss (Regu) 0.1875 (0.1905)\tPrec@1 89.062 (85.306)\tPrec@5 100.000 (98.851)\n",
            "Epoch: [136][200/782]\tTime 0.023 (0.025)\tLoss (Class) 0.6163 (0.6581)\tLoss (Regu) 0.1904 (0.1902)\tPrec@1 85.938 (85.106)\tPrec@5 100.000 (98.818)\n",
            "Epoch: [136][250/782]\tTime 0.022 (0.024)\tLoss (Class) 0.7956 (0.6652)\tLoss (Regu) 0.1841 (0.1901)\tPrec@1 85.938 (84.966)\tPrec@5 98.438 (98.805)\n",
            "Epoch: [136][300/782]\tTime 0.022 (0.024)\tLoss (Class) 0.4807 (0.6727)\tLoss (Regu) 0.1885 (0.1898)\tPrec@1 92.188 (84.692)\tPrec@5 100.000 (98.765)\n",
            "Epoch: [136][350/782]\tTime 0.037 (0.024)\tLoss (Class) 0.7142 (0.6776)\tLoss (Regu) 0.1880 (0.1899)\tPrec@1 84.375 (84.513)\tPrec@5 98.438 (98.740)\n",
            "Epoch: [136][400/782]\tTime 0.022 (0.024)\tLoss (Class) 0.7875 (0.6804)\tLoss (Regu) 0.1846 (0.1897)\tPrec@1 81.250 (84.324)\tPrec@5 96.875 (98.741)\n",
            "Epoch: [136][450/782]\tTime 0.032 (0.024)\tLoss (Class) 0.5305 (0.6849)\tLoss (Regu) 0.1922 (0.1896)\tPrec@1 89.062 (84.202)\tPrec@5 98.438 (98.704)\n",
            "Epoch: [136][500/782]\tTime 0.032 (0.025)\tLoss (Class) 0.6627 (0.6884)\tLoss (Regu) 0.1889 (0.1897)\tPrec@1 82.812 (84.104)\tPrec@5 96.875 (98.653)\n",
            "Epoch: [136][550/782]\tTime 0.033 (0.025)\tLoss (Class) 0.9436 (0.6914)\tLoss (Regu) 0.1907 (0.1894)\tPrec@1 76.562 (84.015)\tPrec@5 100.000 (98.613)\n",
            "Epoch: [136][600/782]\tTime 0.032 (0.025)\tLoss (Class) 0.7034 (0.6916)\tLoss (Regu) 0.1920 (0.1895)\tPrec@1 81.250 (84.019)\tPrec@5 100.000 (98.617)\n",
            "Epoch: [136][650/782]\tTime 0.023 (0.025)\tLoss (Class) 0.6866 (0.6938)\tLoss (Regu) 0.1863 (0.1895)\tPrec@1 89.062 (83.938)\tPrec@5 96.875 (98.608)\n",
            "Epoch: [136][700/782]\tTime 0.021 (0.025)\tLoss (Class) 0.8942 (0.6974)\tLoss (Regu) 0.1883 (0.1895)\tPrec@1 76.562 (83.804)\tPrec@5 95.312 (98.585)\n",
            "Epoch: [136][750/782]\tTime 0.025 (0.025)\tLoss (Class) 0.7792 (0.7001)\tLoss (Regu) 0.1868 (0.1894)\tPrec@1 81.250 (83.715)\tPrec@5 100.000 (98.558)\n",
            "Test: [0/157]\tTime 0.117 (0.117)\tLoss (Class) 1.8171 (1.8171)\tLoss (Regu) 0.2169 (0.2169)\tPrec@1 57.812 (57.812)\tPrec@5 93.750 (93.750)\n",
            "Test: [50/157]\tTime 0.009 (0.012)\tLoss (Class) 2.1446 (1.8442)\tLoss (Regu) 0.2190 (0.2189)\tPrec@1 54.688 (62.776)\tPrec@5 84.375 (87.898)\n",
            "Test: [100/157]\tTime 0.007 (0.010)\tLoss (Class) 1.2971 (1.8305)\tLoss (Regu) 0.2171 (0.2191)\tPrec@1 67.188 (63.243)\tPrec@5 96.875 (87.732)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.5409 (1.8233)\tLoss (Regu) 0.2205 (0.2190)\tPrec@1 67.188 (63.297)\tPrec@5 95.312 (87.779)\n",
            " * Train[83.680 %, 98.536 %, 0.702 loss] Val [63.450 %, 87.850%, 1.817 loss] Best: 66.040 %\n",
            "Time for 136 / 150 21.090732097625732\n",
            "Learning rate:  0.03\n",
            "Epoch: [137][0/782]\tTime 0.142 (0.142)\tLoss (Class) 0.6562 (0.6562)\tLoss (Regu) 0.1884 (0.1884)\tPrec@1 76.562 (76.562)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [137][50/782]\tTime 0.023 (0.026)\tLoss (Class) 0.5210 (0.6621)\tLoss (Regu) 0.1885 (0.1891)\tPrec@1 90.625 (84.498)\tPrec@5 100.000 (99.020)\n",
            "Epoch: [137][100/782]\tTime 0.032 (0.025)\tLoss (Class) 0.7796 (0.6506)\tLoss (Regu) 0.1899 (0.1901)\tPrec@1 79.688 (85.272)\tPrec@5 98.438 (98.979)\n",
            "Epoch: [137][150/782]\tTime 0.023 (0.025)\tLoss (Class) 0.5704 (0.6447)\tLoss (Regu) 0.1881 (0.1898)\tPrec@1 85.938 (85.410)\tPrec@5 100.000 (98.955)\n",
            "Epoch: [137][200/782]\tTime 0.022 (0.025)\tLoss (Class) 0.9418 (0.6431)\tLoss (Regu) 0.1897 (0.1901)\tPrec@1 75.000 (85.238)\tPrec@5 98.438 (99.028)\n",
            "Epoch: [137][250/782]\tTime 0.030 (0.024)\tLoss (Class) 0.7820 (0.6426)\tLoss (Regu) 0.1930 (0.1899)\tPrec@1 82.812 (85.222)\tPrec@5 98.438 (98.979)\n",
            "Epoch: [137][300/782]\tTime 0.030 (0.025)\tLoss (Class) 0.6459 (0.6427)\tLoss (Regu) 0.1911 (0.1898)\tPrec@1 87.500 (85.200)\tPrec@5 98.438 (99.003)\n",
            "Epoch: [137][350/782]\tTime 0.023 (0.025)\tLoss (Class) 0.5892 (0.6489)\tLoss (Regu) 0.1867 (0.1897)\tPrec@1 87.500 (85.020)\tPrec@5 100.000 (98.923)\n",
            "Epoch: [137][400/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6331 (0.6555)\tLoss (Regu) 0.1899 (0.1897)\tPrec@1 81.250 (84.761)\tPrec@5 100.000 (98.886)\n",
            "Epoch: [137][450/782]\tTime 0.026 (0.024)\tLoss (Class) 0.7055 (0.6631)\tLoss (Regu) 0.1866 (0.1895)\tPrec@1 79.688 (84.583)\tPrec@5 100.000 (98.846)\n",
            "Epoch: [137][500/782]\tTime 0.022 (0.024)\tLoss (Class) 0.5637 (0.6684)\tLoss (Regu) 0.1904 (0.1895)\tPrec@1 87.500 (84.425)\tPrec@5 100.000 (98.771)\n",
            "Epoch: [137][550/782]\tTime 0.022 (0.024)\tLoss (Class) 0.6470 (0.6737)\tLoss (Regu) 0.1895 (0.1895)\tPrec@1 89.062 (84.315)\tPrec@5 100.000 (98.724)\n",
            "Epoch: [137][600/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7983 (0.6758)\tLoss (Regu) 0.1911 (0.1896)\tPrec@1 84.375 (84.281)\tPrec@5 96.875 (98.718)\n",
            "Epoch: [137][650/782]\tTime 0.032 (0.024)\tLoss (Class) 0.8271 (0.6821)\tLoss (Regu) 0.1865 (0.1895)\tPrec@1 82.812 (84.077)\tPrec@5 96.875 (98.637)\n",
            "Epoch: [137][700/782]\tTime 0.030 (0.024)\tLoss (Class) 0.8368 (0.6871)\tLoss (Regu) 0.1826 (0.1893)\tPrec@1 81.250 (83.965)\tPrec@5 98.438 (98.611)\n",
            "Epoch: [137][750/782]\tTime 0.022 (0.024)\tLoss (Class) 0.6339 (0.6899)\tLoss (Regu) 0.1888 (0.1892)\tPrec@1 84.375 (83.924)\tPrec@5 100.000 (98.575)\n",
            "Test: [0/157]\tTime 0.117 (0.117)\tLoss (Class) 1.7121 (1.7121)\tLoss (Regu) 0.2151 (0.2151)\tPrec@1 64.062 (64.062)\tPrec@5 87.500 (87.500)\n",
            "Test: [50/157]\tTime 0.008 (0.010)\tLoss (Class) 1.7362 (1.7037)\tLoss (Regu) 0.2144 (0.2132)\tPrec@1 68.750 (64.430)\tPrec@5 84.375 (88.450)\n",
            "Test: [100/157]\tTime 0.008 (0.009)\tLoss (Class) 1.4589 (1.7165)\tLoss (Regu) 0.2118 (0.2134)\tPrec@1 65.625 (64.202)\tPrec@5 87.500 (88.304)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.6624 (1.7275)\tLoss (Regu) 0.2186 (0.2133)\tPrec@1 71.875 (63.887)\tPrec@5 82.812 (88.245)\n",
            " * Train[83.908 %, 98.556 %, 0.692 loss] Val [63.810 %, 88.250%, 1.729 loss] Best: 66.040 %\n",
            "Time for 137 / 150 20.24038863182068\n",
            "Learning rate:  0.03\n",
            "Epoch: [138][0/782]\tTime 0.151 (0.151)\tLoss (Class) 0.6617 (0.6617)\tLoss (Regu) 0.1850 (0.1850)\tPrec@1 84.375 (84.375)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [138][50/782]\tTime 0.037 (0.028)\tLoss (Class) 0.8437 (0.6512)\tLoss (Regu) 0.1856 (0.1888)\tPrec@1 78.125 (85.631)\tPrec@5 100.000 (98.928)\n",
            "Epoch: [138][100/782]\tTime 0.022 (0.026)\tLoss (Class) 0.5287 (0.6406)\tLoss (Regu) 0.1895 (0.1896)\tPrec@1 90.625 (86.061)\tPrec@5 100.000 (98.840)\n",
            "Epoch: [138][150/782]\tTime 0.023 (0.025)\tLoss (Class) 0.5243 (0.6472)\tLoss (Regu) 0.1890 (0.1895)\tPrec@1 89.062 (85.606)\tPrec@5 100.000 (98.924)\n",
            "Epoch: [138][200/782]\tTime 0.022 (0.025)\tLoss (Class) 0.6085 (0.6477)\tLoss (Regu) 0.1905 (0.1899)\tPrec@1 87.500 (85.580)\tPrec@5 100.000 (98.904)\n",
            "Epoch: [138][250/782]\tTime 0.026 (0.025)\tLoss (Class) 0.7009 (0.6508)\tLoss (Regu) 0.1868 (0.1897)\tPrec@1 79.688 (85.713)\tPrec@5 100.000 (98.817)\n",
            "Epoch: [138][300/782]\tTime 0.023 (0.025)\tLoss (Class) 0.6741 (0.6485)\tLoss (Regu) 0.1921 (0.1899)\tPrec@1 87.500 (85.782)\tPrec@5 96.875 (98.858)\n",
            "Epoch: [138][350/782]\tTime 0.022 (0.025)\tLoss (Class) 0.6058 (0.6488)\tLoss (Regu) 0.1882 (0.1899)\tPrec@1 82.812 (85.728)\tPrec@5 98.438 (98.860)\n",
            "Epoch: [138][400/782]\tTime 0.022 (0.025)\tLoss (Class) 0.8798 (0.6531)\tLoss (Regu) 0.1865 (0.1900)\tPrec@1 79.688 (85.528)\tPrec@5 95.312 (98.827)\n",
            "Epoch: [138][450/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7190 (0.6560)\tLoss (Regu) 0.1884 (0.1899)\tPrec@1 79.688 (85.411)\tPrec@5 100.000 (98.819)\n",
            "Epoch: [138][500/782]\tTime 0.023 (0.024)\tLoss (Class) 0.4632 (0.6597)\tLoss (Regu) 0.1925 (0.1900)\tPrec@1 90.625 (85.239)\tPrec@5 100.000 (98.784)\n",
            "Epoch: [138][550/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7820 (0.6651)\tLoss (Regu) 0.1874 (0.1901)\tPrec@1 79.688 (85.041)\tPrec@5 96.875 (98.738)\n",
            "Epoch: [138][600/782]\tTime 0.022 (0.024)\tLoss (Class) 0.6651 (0.6679)\tLoss (Regu) 0.1913 (0.1902)\tPrec@1 81.250 (84.950)\tPrec@5 98.438 (98.736)\n",
            "Epoch: [138][650/782]\tTime 0.021 (0.024)\tLoss (Class) 0.6579 (0.6734)\tLoss (Regu) 0.1879 (0.1902)\tPrec@1 84.375 (84.752)\tPrec@5 98.438 (98.699)\n",
            "Epoch: [138][700/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6922 (0.6801)\tLoss (Regu) 0.1868 (0.1900)\tPrec@1 87.500 (84.513)\tPrec@5 95.312 (98.647)\n",
            "Epoch: [138][750/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6672 (0.6837)\tLoss (Regu) 0.1918 (0.1898)\tPrec@1 82.812 (84.400)\tPrec@5 98.438 (98.608)\n",
            "Test: [0/157]\tTime 0.115 (0.115)\tLoss (Class) 1.8207 (1.8207)\tLoss (Regu) 0.2262 (0.2262)\tPrec@1 64.062 (64.062)\tPrec@5 85.938 (85.938)\n",
            "Test: [50/157]\tTime 0.008 (0.011)\tLoss (Class) 1.4872 (1.7628)\tLoss (Regu) 0.2177 (0.2208)\tPrec@1 67.188 (63.725)\tPrec@5 95.312 (88.266)\n",
            "Test: [100/157]\tTime 0.008 (0.010)\tLoss (Class) 1.4258 (1.7656)\tLoss (Regu) 0.2167 (0.2209)\tPrec@1 68.750 (64.248)\tPrec@5 90.625 (88.552)\n",
            "Test: [150/157]\tTime 0.007 (0.010)\tLoss (Class) 1.3644 (1.7773)\tLoss (Regu) 0.2259 (0.2209)\tPrec@1 71.875 (63.659)\tPrec@5 92.188 (88.535)\n",
            " * Train[84.334 %, 98.590 %, 0.685 loss] Val [63.620 %, 88.540%, 1.779 loss] Best: 66.040 %\n",
            "Time for 138 / 150 20.62553906440735\n",
            "Learning rate:  0.03\n",
            "Epoch: [139][0/782]\tTime 0.158 (0.158)\tLoss (Class) 0.7061 (0.7061)\tLoss (Regu) 0.1874 (0.1874)\tPrec@1 82.812 (82.812)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [139][50/782]\tTime 0.023 (0.026)\tLoss (Class) 0.5983 (0.6783)\tLoss (Regu) 0.1897 (0.1896)\tPrec@1 82.812 (84.130)\tPrec@5 100.000 (98.621)\n",
            "Epoch: [139][100/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6754 (0.6600)\tLoss (Regu) 0.1938 (0.1897)\tPrec@1 85.938 (84.932)\tPrec@5 100.000 (98.762)\n",
            "Epoch: [139][150/782]\tTime 0.021 (0.024)\tLoss (Class) 0.5508 (0.6545)\tLoss (Regu) 0.1926 (0.1903)\tPrec@1 87.500 (85.213)\tPrec@5 98.438 (98.800)\n",
            "Epoch: [139][200/782]\tTime 0.022 (0.024)\tLoss (Class) 0.6445 (0.6482)\tLoss (Regu) 0.1904 (0.1899)\tPrec@1 87.500 (85.533)\tPrec@5 100.000 (98.780)\n",
            "Epoch: [139][250/782]\tTime 0.023 (0.024)\tLoss (Class) 0.5295 (0.6424)\tLoss (Regu) 0.1913 (0.1897)\tPrec@1 85.938 (85.850)\tPrec@5 100.000 (98.823)\n",
            "Epoch: [139][300/782]\tTime 0.036 (0.024)\tLoss (Class) 0.5542 (0.6485)\tLoss (Regu) 0.1954 (0.1895)\tPrec@1 90.625 (85.668)\tPrec@5 100.000 (98.806)\n",
            "Epoch: [139][350/782]\tTime 0.024 (0.024)\tLoss (Class) 0.7026 (0.6491)\tLoss (Regu) 0.1907 (0.1898)\tPrec@1 85.938 (85.684)\tPrec@5 98.438 (98.843)\n",
            "Epoch: [139][400/782]\tTime 0.023 (0.025)\tLoss (Class) 0.5725 (0.6488)\tLoss (Regu) 0.1908 (0.1897)\tPrec@1 87.500 (85.712)\tPrec@5 100.000 (98.827)\n",
            "Epoch: [139][450/782]\tTime 0.024 (0.025)\tLoss (Class) 0.7772 (0.6520)\tLoss (Regu) 0.1854 (0.1896)\tPrec@1 79.688 (85.577)\tPrec@5 96.875 (98.794)\n",
            "Epoch: [139][500/782]\tTime 0.032 (0.025)\tLoss (Class) 0.6406 (0.6571)\tLoss (Regu) 0.1844 (0.1893)\tPrec@1 87.500 (85.379)\tPrec@5 100.000 (98.784)\n",
            "Epoch: [139][550/782]\tTime 0.023 (0.025)\tLoss (Class) 0.6219 (0.6597)\tLoss (Regu) 0.1868 (0.1892)\tPrec@1 81.250 (85.254)\tPrec@5 100.000 (98.775)\n",
            "Epoch: [139][600/782]\tTime 0.021 (0.025)\tLoss (Class) 0.6591 (0.6629)\tLoss (Regu) 0.1876 (0.1892)\tPrec@1 90.625 (85.165)\tPrec@5 100.000 (98.744)\n",
            "Epoch: [139][650/782]\tTime 0.024 (0.024)\tLoss (Class) 0.7228 (0.6646)\tLoss (Regu) 0.1885 (0.1891)\tPrec@1 84.375 (85.095)\tPrec@5 98.438 (98.730)\n",
            "Epoch: [139][700/782]\tTime 0.022 (0.024)\tLoss (Class) 0.8490 (0.6656)\tLoss (Regu) 0.1920 (0.1892)\tPrec@1 76.562 (85.055)\tPrec@5 98.438 (98.727)\n",
            "Epoch: [139][750/782]\tTime 0.022 (0.024)\tLoss (Class) 0.5776 (0.6703)\tLoss (Regu) 0.1943 (0.1893)\tPrec@1 90.625 (84.910)\tPrec@5 98.438 (98.698)\n",
            "Test: [0/157]\tTime 0.117 (0.117)\tLoss (Class) 1.7582 (1.7582)\tLoss (Regu) 0.2296 (0.2296)\tPrec@1 70.312 (70.312)\tPrec@5 90.625 (90.625)\n",
            "Test: [50/157]\tTime 0.008 (0.010)\tLoss (Class) 1.8336 (1.8168)\tLoss (Regu) 0.2236 (0.2273)\tPrec@1 62.500 (63.695)\tPrec@5 84.375 (88.388)\n",
            "Test: [100/157]\tTime 0.009 (0.009)\tLoss (Class) 1.7231 (1.8002)\tLoss (Regu) 0.2294 (0.2270)\tPrec@1 67.188 (63.800)\tPrec@5 92.188 (88.753)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.5471 (1.8083)\tLoss (Regu) 0.2274 (0.2268)\tPrec@1 70.312 (63.545)\tPrec@5 93.750 (88.483)\n",
            " * Train[84.772 %, 98.686 %, 0.673 loss] Val [63.520 %, 88.450%, 1.808 loss] Best: 66.040 %\n",
            "Time for 139 / 150 20.374903678894043\n",
            "Learning rate:  0.03\n",
            "Epoch: [140][0/782]\tTime 0.153 (0.153)\tLoss (Class) 0.6116 (0.6116)\tLoss (Regu) 0.1950 (0.1950)\tPrec@1 90.625 (90.625)\tPrec@5 98.438 (98.438)\n",
            "Epoch: [140][50/782]\tTime 0.023 (0.027)\tLoss (Class) 0.6384 (0.6350)\tLoss (Regu) 0.1914 (0.1921)\tPrec@1 84.375 (85.754)\tPrec@5 100.000 (98.836)\n",
            "Epoch: [140][100/782]\tTime 0.023 (0.025)\tLoss (Class) 0.5084 (0.6274)\tLoss (Regu) 0.1873 (0.1912)\tPrec@1 85.938 (85.968)\tPrec@5 100.000 (98.886)\n",
            "Epoch: [140][150/782]\tTime 0.022 (0.025)\tLoss (Class) 0.7548 (0.6333)\tLoss (Regu) 0.1889 (0.1914)\tPrec@1 84.375 (85.896)\tPrec@5 98.438 (98.789)\n",
            "Epoch: [140][200/782]\tTime 0.022 (0.024)\tLoss (Class) 0.5058 (0.6354)\tLoss (Regu) 0.1924 (0.1908)\tPrec@1 89.062 (85.844)\tPrec@5 100.000 (98.857)\n",
            "Epoch: [140][250/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8259 (0.6385)\tLoss (Regu) 0.1920 (0.1906)\tPrec@1 82.812 (85.838)\tPrec@5 96.875 (98.767)\n",
            "Epoch: [140][300/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7474 (0.6408)\tLoss (Regu) 0.1874 (0.1898)\tPrec@1 85.938 (85.745)\tPrec@5 98.438 (98.739)\n",
            "Epoch: [140][350/782]\tTime 0.026 (0.024)\tLoss (Class) 0.6112 (0.6503)\tLoss (Regu) 0.1876 (0.1897)\tPrec@1 87.500 (85.408)\tPrec@5 98.438 (98.682)\n",
            "Epoch: [140][400/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6775 (0.6544)\tLoss (Regu) 0.1894 (0.1898)\tPrec@1 82.812 (85.201)\tPrec@5 98.438 (98.718)\n",
            "Epoch: [140][450/782]\tTime 0.024 (0.024)\tLoss (Class) 0.7990 (0.6622)\tLoss (Regu) 0.1941 (0.1898)\tPrec@1 76.562 (84.936)\tPrec@5 95.312 (98.663)\n",
            "Epoch: [140][500/782]\tTime 0.022 (0.024)\tLoss (Class) 0.5582 (0.6647)\tLoss (Regu) 0.1912 (0.1900)\tPrec@1 87.500 (84.893)\tPrec@5 98.438 (98.662)\n",
            "Epoch: [140][550/782]\tTime 0.022 (0.024)\tLoss (Class) 0.5328 (0.6698)\tLoss (Regu) 0.1878 (0.1898)\tPrec@1 89.062 (84.690)\tPrec@5 98.438 (98.656)\n",
            "Epoch: [140][600/782]\tTime 0.030 (0.024)\tLoss (Class) 0.5468 (0.6713)\tLoss (Regu) 0.1898 (0.1898)\tPrec@1 89.062 (84.682)\tPrec@5 100.000 (98.635)\n",
            "Epoch: [140][650/782]\tTime 0.022 (0.024)\tLoss (Class) 0.8502 (0.6730)\tLoss (Regu) 0.1912 (0.1897)\tPrec@1 79.688 (84.620)\tPrec@5 95.312 (98.615)\n",
            "Epoch: [140][700/782]\tTime 0.022 (0.024)\tLoss (Class) 0.8544 (0.6788)\tLoss (Regu) 0.1914 (0.1896)\tPrec@1 79.688 (84.457)\tPrec@5 93.750 (98.569)\n",
            "Epoch: [140][750/782]\tTime 0.023 (0.024)\tLoss (Class) 0.5973 (0.6814)\tLoss (Regu) 0.1910 (0.1896)\tPrec@1 87.500 (84.329)\tPrec@5 98.438 (98.562)\n",
            "Test: [0/157]\tTime 0.113 (0.113)\tLoss (Class) 1.1008 (1.1008)\tLoss (Regu) 0.2180 (0.2180)\tPrec@1 70.312 (70.312)\tPrec@5 93.750 (93.750)\n",
            "Test: [50/157]\tTime 0.009 (0.010)\tLoss (Class) 2.6636 (1.8304)\tLoss (Regu) 0.2174 (0.2179)\tPrec@1 48.438 (62.960)\tPrec@5 78.125 (87.623)\n",
            "Test: [100/157]\tTime 0.008 (0.009)\tLoss (Class) 1.2527 (1.8201)\tLoss (Regu) 0.2202 (0.2181)\tPrec@1 70.312 (62.825)\tPrec@5 92.188 (87.840)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 2.2778 (1.8052)\tLoss (Regu) 0.2149 (0.2181)\tPrec@1 50.000 (62.955)\tPrec@5 85.938 (88.017)\n",
            " * Train[84.250 %, 98.550 %, 0.683 loss] Val [62.970 %, 87.930%, 1.807 loss] Best: 66.040 %\n",
            "Time for 140 / 150 20.018927097320557\n",
            "Learning rate:  0.03\n",
            "Epoch: [141][0/782]\tTime 0.156 (0.156)\tLoss (Class) 0.7159 (0.7159)\tLoss (Regu) 0.1914 (0.1914)\tPrec@1 82.812 (82.812)\tPrec@5 96.875 (96.875)\n",
            "Epoch: [141][50/782]\tTime 0.023 (0.025)\tLoss (Class) 0.5188 (0.6512)\tLoss (Regu) 0.1941 (0.1892)\tPrec@1 92.188 (85.110)\tPrec@5 100.000 (99.081)\n",
            "Epoch: [141][100/782]\tTime 0.022 (0.024)\tLoss (Class) 0.4750 (0.6378)\tLoss (Regu) 0.1876 (0.1901)\tPrec@1 92.188 (85.504)\tPrec@5 100.000 (99.165)\n",
            "Epoch: [141][150/782]\tTime 0.022 (0.024)\tLoss (Class) 0.7361 (0.6359)\tLoss (Regu) 0.1863 (0.1898)\tPrec@1 84.375 (85.751)\tPrec@5 96.875 (99.089)\n",
            "Epoch: [141][200/782]\tTime 0.022 (0.023)\tLoss (Class) 0.8084 (0.6413)\tLoss (Regu) 0.1887 (0.1894)\tPrec@1 82.812 (85.588)\tPrec@5 98.438 (99.005)\n",
            "Epoch: [141][250/782]\tTime 0.023 (0.024)\tLoss (Class) 0.5831 (0.6489)\tLoss (Regu) 0.1868 (0.1887)\tPrec@1 89.062 (85.483)\tPrec@5 100.000 (98.954)\n",
            "Epoch: [141][300/782]\tTime 0.024 (0.024)\tLoss (Class) 0.6999 (0.6533)\tLoss (Regu) 0.1851 (0.1887)\tPrec@1 81.250 (85.247)\tPrec@5 96.875 (98.946)\n",
            "Epoch: [141][350/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7858 (0.6546)\tLoss (Regu) 0.1891 (0.1887)\tPrec@1 82.812 (85.279)\tPrec@5 98.438 (98.945)\n",
            "Epoch: [141][400/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7473 (0.6579)\tLoss (Regu) 0.1864 (0.1888)\tPrec@1 81.250 (85.154)\tPrec@5 95.312 (98.886)\n",
            "Epoch: [141][450/782]\tTime 0.023 (0.024)\tLoss (Class) 0.5770 (0.6651)\tLoss (Regu) 0.1858 (0.1887)\tPrec@1 85.938 (84.860)\tPrec@5 98.438 (98.839)\n",
            "Epoch: [141][500/782]\tTime 0.030 (0.024)\tLoss (Class) 0.6434 (0.6650)\tLoss (Regu) 0.1903 (0.1888)\tPrec@1 84.375 (84.883)\tPrec@5 100.000 (98.834)\n",
            "Epoch: [141][550/782]\tTime 0.023 (0.024)\tLoss (Class) 0.4858 (0.6667)\tLoss (Regu) 0.1894 (0.1888)\tPrec@1 93.750 (84.820)\tPrec@5 100.000 (98.803)\n",
            "Epoch: [141][600/782]\tTime 0.030 (0.024)\tLoss (Class) 0.7982 (0.6709)\tLoss (Regu) 0.1912 (0.1889)\tPrec@1 79.688 (84.674)\tPrec@5 98.438 (98.768)\n",
            "Epoch: [141][650/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8191 (0.6737)\tLoss (Regu) 0.1854 (0.1888)\tPrec@1 82.812 (84.598)\tPrec@5 96.875 (98.740)\n",
            "Epoch: [141][700/782]\tTime 0.032 (0.024)\tLoss (Class) 0.7248 (0.6781)\tLoss (Regu) 0.1895 (0.1888)\tPrec@1 79.688 (84.440)\tPrec@5 100.000 (98.712)\n",
            "Epoch: [141][750/782]\tTime 0.023 (0.024)\tLoss (Class) 0.5676 (0.6820)\tLoss (Regu) 0.1884 (0.1887)\tPrec@1 81.250 (84.304)\tPrec@5 100.000 (98.679)\n",
            "Test: [0/157]\tTime 0.113 (0.113)\tLoss (Class) 1.4267 (1.4267)\tLoss (Regu) 0.2152 (0.2152)\tPrec@1 70.312 (70.312)\tPrec@5 92.188 (92.188)\n",
            "Test: [50/157]\tTime 0.009 (0.011)\tLoss (Class) 2.2842 (1.7169)\tLoss (Regu) 0.2072 (0.2146)\tPrec@1 59.375 (64.246)\tPrec@5 79.688 (88.572)\n",
            "Test: [100/157]\tTime 0.008 (0.010)\tLoss (Class) 1.6719 (1.7050)\tLoss (Regu) 0.2137 (0.2150)\tPrec@1 60.938 (64.712)\tPrec@5 90.625 (88.722)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 2.1295 (1.7232)\tLoss (Regu) 0.2147 (0.2147)\tPrec@1 60.938 (64.414)\tPrec@5 84.375 (88.555)\n",
            " * Train[84.248 %, 98.660 %, 0.685 loss] Val [64.430 %, 88.590%, 1.721 loss] Best: 66.040 %\n",
            "Time for 141 / 150 20.343270540237427\n",
            "Learning rate:  0.03\n",
            "Epoch: [142][0/782]\tTime 0.148 (0.148)\tLoss (Class) 0.5605 (0.5605)\tLoss (Regu) 0.1894 (0.1894)\tPrec@1 85.938 (85.938)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [142][50/782]\tTime 0.023 (0.026)\tLoss (Class) 0.6152 (0.6476)\tLoss (Regu) 0.1907 (0.1905)\tPrec@1 87.500 (85.294)\tPrec@5 100.000 (98.713)\n",
            "Epoch: [142][100/782]\tTime 0.022 (0.024)\tLoss (Class) 0.4034 (0.6406)\tLoss (Regu) 0.1913 (0.1902)\tPrec@1 90.625 (85.675)\tPrec@5 100.000 (98.902)\n",
            "Epoch: [142][150/782]\tTime 0.022 (0.024)\tLoss (Class) 0.7429 (0.6431)\tLoss (Regu) 0.1874 (0.1902)\tPrec@1 87.500 (85.731)\tPrec@5 98.438 (98.851)\n",
            "Epoch: [142][200/782]\tTime 0.023 (0.024)\tLoss (Class) 0.5410 (0.6486)\tLoss (Regu) 0.1902 (0.1902)\tPrec@1 85.938 (85.448)\tPrec@5 100.000 (98.795)\n",
            "Epoch: [142][250/782]\tTime 0.023 (0.023)\tLoss (Class) 0.5411 (0.6525)\tLoss (Regu) 0.1860 (0.1902)\tPrec@1 87.500 (85.303)\tPrec@5 100.000 (98.805)\n",
            "Epoch: [142][300/782]\tTime 0.023 (0.023)\tLoss (Class) 0.6378 (0.6550)\tLoss (Regu) 0.1877 (0.1897)\tPrec@1 85.938 (85.237)\tPrec@5 98.438 (98.744)\n",
            "Epoch: [142][350/782]\tTime 0.022 (0.023)\tLoss (Class) 0.4735 (0.6556)\tLoss (Regu) 0.1899 (0.1897)\tPrec@1 93.750 (85.163)\tPrec@5 100.000 (98.736)\n",
            "Epoch: [142][400/782]\tTime 0.023 (0.024)\tLoss (Class) 0.4482 (0.6532)\tLoss (Regu) 0.1860 (0.1898)\tPrec@1 95.312 (85.248)\tPrec@5 98.438 (98.788)\n",
            "Epoch: [142][450/782]\tTime 0.023 (0.024)\tLoss (Class) 0.8112 (0.6558)\tLoss (Regu) 0.1849 (0.1896)\tPrec@1 79.688 (85.179)\tPrec@5 98.438 (98.777)\n",
            "Epoch: [142][500/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6108 (0.6576)\tLoss (Regu) 0.1876 (0.1895)\tPrec@1 84.375 (85.092)\tPrec@5 100.000 (98.765)\n",
            "Epoch: [142][550/782]\tTime 0.030 (0.024)\tLoss (Class) 0.6938 (0.6604)\tLoss (Regu) 0.1925 (0.1893)\tPrec@1 81.250 (84.936)\tPrec@5 95.312 (98.752)\n",
            "Epoch: [142][600/782]\tTime 0.023 (0.024)\tLoss (Class) 0.5086 (0.6608)\tLoss (Regu) 0.1908 (0.1893)\tPrec@1 89.062 (84.957)\tPrec@5 100.000 (98.757)\n",
            "Epoch: [142][650/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6908 (0.6655)\tLoss (Regu) 0.1877 (0.1892)\tPrec@1 79.688 (84.817)\tPrec@5 98.438 (98.723)\n",
            "Epoch: [142][700/782]\tTime 0.033 (0.024)\tLoss (Class) 0.6853 (0.6695)\tLoss (Regu) 0.1886 (0.1891)\tPrec@1 87.500 (84.736)\tPrec@5 98.438 (98.660)\n",
            "Epoch: [142][750/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6600 (0.6733)\tLoss (Regu) 0.1904 (0.1890)\tPrec@1 87.500 (84.621)\tPrec@5 96.875 (98.641)\n",
            "Test: [0/157]\tTime 0.120 (0.120)\tLoss (Class) 1.4403 (1.4403)\tLoss (Regu) 0.2250 (0.2250)\tPrec@1 71.875 (71.875)\tPrec@5 90.625 (90.625)\n",
            "Test: [50/157]\tTime 0.008 (0.013)\tLoss (Class) 1.9454 (1.8159)\tLoss (Regu) 0.2253 (0.2240)\tPrec@1 70.312 (63.909)\tPrec@5 89.062 (87.990)\n",
            "Test: [100/157]\tTime 0.008 (0.012)\tLoss (Class) 1.7221 (1.8557)\tLoss (Regu) 0.2216 (0.2233)\tPrec@1 62.500 (63.011)\tPrec@5 90.625 (87.639)\n",
            "Test: [150/157]\tTime 0.007 (0.011)\tLoss (Class) 1.9992 (1.8739)\tLoss (Regu) 0.2272 (0.2233)\tPrec@1 53.125 (62.376)\tPrec@5 89.062 (87.572)\n",
            " * Train[84.638 %, 98.626 %, 0.674 loss] Val [62.480 %, 87.580%, 1.871 loss] Best: 66.040 %\n",
            "Time for 142 / 150 20.389852285385132\n",
            "Learning rate:  0.03\n",
            "Epoch: [143][0/782]\tTime 0.142 (0.142)\tLoss (Class) 0.5210 (0.5210)\tLoss (Regu) 0.1907 (0.1907)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [143][50/782]\tTime 0.023 (0.025)\tLoss (Class) 0.5347 (0.6349)\tLoss (Regu) 0.1884 (0.1891)\tPrec@1 93.750 (85.846)\tPrec@5 100.000 (99.112)\n",
            "Epoch: [143][100/782]\tTime 0.022 (0.024)\tLoss (Class) 0.8811 (0.6328)\tLoss (Regu) 0.1941 (0.1908)\tPrec@1 78.125 (86.154)\tPrec@5 96.875 (99.134)\n",
            "Epoch: [143][150/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6801 (0.6481)\tLoss (Regu) 0.1892 (0.1901)\tPrec@1 85.938 (85.668)\tPrec@5 98.438 (99.017)\n",
            "Epoch: [143][200/782]\tTime 0.022 (0.023)\tLoss (Class) 0.6035 (0.6569)\tLoss (Regu) 0.1898 (0.1902)\tPrec@1 87.500 (85.176)\tPrec@5 98.438 (98.873)\n",
            "Epoch: [143][250/782]\tTime 0.032 (0.024)\tLoss (Class) 0.6811 (0.6618)\tLoss (Regu) 0.1900 (0.1898)\tPrec@1 84.375 (84.991)\tPrec@5 96.875 (98.867)\n",
            "Epoch: [143][300/782]\tTime 0.022 (0.024)\tLoss (Class) 0.5207 (0.6587)\tLoss (Regu) 0.1954 (0.1899)\tPrec@1 93.750 (85.081)\tPrec@5 100.000 (98.874)\n",
            "Epoch: [143][350/782]\tTime 0.022 (0.024)\tLoss (Class) 0.8063 (0.6609)\tLoss (Regu) 0.1929 (0.1903)\tPrec@1 84.375 (85.007)\tPrec@5 96.875 (98.838)\n",
            "Epoch: [143][400/782]\tTime 0.023 (0.024)\tLoss (Class) 0.5457 (0.6587)\tLoss (Regu) 0.1964 (0.1902)\tPrec@1 89.062 (85.076)\tPrec@5 98.438 (98.835)\n",
            "Epoch: [143][450/782]\tTime 0.023 (0.024)\tLoss (Class) 0.5909 (0.6629)\tLoss (Regu) 0.1951 (0.1902)\tPrec@1 89.062 (84.957)\tPrec@5 98.438 (98.812)\n",
            "Epoch: [143][500/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7031 (0.6654)\tLoss (Regu) 0.1889 (0.1902)\tPrec@1 84.375 (84.971)\tPrec@5 100.000 (98.806)\n",
            "Epoch: [143][550/782]\tTime 0.022 (0.024)\tLoss (Class) 0.4995 (0.6671)\tLoss (Regu) 0.1878 (0.1899)\tPrec@1 92.188 (84.897)\tPrec@5 100.000 (98.800)\n",
            "Epoch: [143][600/782]\tTime 0.022 (0.024)\tLoss (Class) 0.6585 (0.6703)\tLoss (Regu) 0.1851 (0.1898)\tPrec@1 85.938 (84.747)\tPrec@5 96.875 (98.783)\n",
            "Epoch: [143][650/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6597 (0.6730)\tLoss (Regu) 0.1868 (0.1896)\tPrec@1 85.938 (84.670)\tPrec@5 98.438 (98.747)\n",
            "Epoch: [143][700/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7482 (0.6754)\tLoss (Regu) 0.1878 (0.1895)\tPrec@1 76.562 (84.571)\tPrec@5 100.000 (98.745)\n",
            "Epoch: [143][750/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6887 (0.6786)\tLoss (Regu) 0.1868 (0.1895)\tPrec@1 84.375 (84.435)\tPrec@5 100.000 (98.754)\n",
            "Test: [0/157]\tTime 0.112 (0.112)\tLoss (Class) 1.6929 (1.6929)\tLoss (Regu) 0.2113 (0.2113)\tPrec@1 60.938 (60.938)\tPrec@5 89.062 (89.062)\n",
            "Test: [50/157]\tTime 0.008 (0.011)\tLoss (Class) 1.5136 (1.7245)\tLoss (Regu) 0.2178 (0.2188)\tPrec@1 71.875 (63.542)\tPrec@5 89.062 (89.308)\n",
            "Test: [100/157]\tTime 0.009 (0.010)\tLoss (Class) 1.2160 (1.7489)\tLoss (Regu) 0.2183 (0.2185)\tPrec@1 71.875 (63.413)\tPrec@5 92.188 (89.140)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 2.3472 (1.7608)\tLoss (Regu) 0.2172 (0.2185)\tPrec@1 64.062 (63.618)\tPrec@5 85.938 (88.835)\n",
            " * Train[84.350 %, 98.740 %, 0.681 loss] Val [63.690 %, 88.780%, 1.763 loss] Best: 66.040 %\n",
            "Time for 143 / 150 19.990421533584595\n",
            "Learning rate:  0.03\n",
            "Epoch: [144][0/782]\tTime 0.154 (0.154)\tLoss (Class) 0.6695 (0.6695)\tLoss (Regu) 0.1892 (0.1892)\tPrec@1 84.375 (84.375)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [144][50/782]\tTime 0.022 (0.027)\tLoss (Class) 0.6952 (0.6641)\tLoss (Regu) 0.1916 (0.1894)\tPrec@1 89.062 (85.294)\tPrec@5 96.875 (99.020)\n",
            "Epoch: [144][100/782]\tTime 0.023 (0.026)\tLoss (Class) 0.6048 (0.6421)\tLoss (Regu) 0.1923 (0.1913)\tPrec@1 82.812 (85.659)\tPrec@5 100.000 (99.056)\n",
            "Epoch: [144][150/782]\tTime 0.023 (0.026)\tLoss (Class) 0.6490 (0.6400)\tLoss (Regu) 0.1904 (0.1911)\tPrec@1 85.938 (85.989)\tPrec@5 100.000 (99.038)\n",
            "Epoch: [144][200/782]\tTime 0.023 (0.025)\tLoss (Class) 0.6118 (0.6348)\tLoss (Regu) 0.1889 (0.1906)\tPrec@1 85.938 (86.062)\tPrec@5 98.438 (99.028)\n",
            "Epoch: [144][250/782]\tTime 0.033 (0.025)\tLoss (Class) 0.5912 (0.6351)\tLoss (Regu) 0.1903 (0.1905)\tPrec@1 92.188 (85.969)\tPrec@5 98.438 (99.035)\n",
            "Epoch: [144][300/782]\tTime 0.023 (0.025)\tLoss (Class) 0.5659 (0.6382)\tLoss (Regu) 0.1910 (0.1903)\tPrec@1 89.062 (85.948)\tPrec@5 98.438 (98.957)\n",
            "Epoch: [144][350/782]\tTime 0.023 (0.025)\tLoss (Class) 0.7721 (0.6444)\tLoss (Regu) 0.1884 (0.1899)\tPrec@1 79.688 (85.702)\tPrec@5 96.875 (98.909)\n",
            "Epoch: [144][400/782]\tTime 0.021 (0.025)\tLoss (Class) 0.6780 (0.6482)\tLoss (Regu) 0.1866 (0.1896)\tPrec@1 82.812 (85.524)\tPrec@5 100.000 (98.862)\n",
            "Epoch: [144][450/782]\tTime 0.023 (0.025)\tLoss (Class) 0.6292 (0.6508)\tLoss (Regu) 0.1884 (0.1893)\tPrec@1 87.500 (85.449)\tPrec@5 100.000 (98.791)\n",
            "Epoch: [144][500/782]\tTime 0.022 (0.025)\tLoss (Class) 0.6690 (0.6578)\tLoss (Regu) 0.1847 (0.1892)\tPrec@1 84.375 (85.230)\tPrec@5 95.312 (98.746)\n",
            "Epoch: [144][550/782]\tTime 0.022 (0.025)\tLoss (Class) 0.6757 (0.6599)\tLoss (Regu) 0.1876 (0.1892)\tPrec@1 87.500 (85.135)\tPrec@5 100.000 (98.744)\n",
            "Epoch: [144][600/782]\tTime 0.032 (0.025)\tLoss (Class) 0.5684 (0.6621)\tLoss (Regu) 0.1925 (0.1892)\tPrec@1 89.062 (85.056)\tPrec@5 98.438 (98.742)\n",
            "Epoch: [144][650/782]\tTime 0.023 (0.025)\tLoss (Class) 0.8886 (0.6693)\tLoss (Regu) 0.1881 (0.1891)\tPrec@1 76.562 (84.821)\tPrec@5 98.438 (98.687)\n",
            "Epoch: [144][700/782]\tTime 0.032 (0.025)\tLoss (Class) 0.7337 (0.6713)\tLoss (Regu) 0.1901 (0.1892)\tPrec@1 84.375 (84.734)\tPrec@5 98.438 (98.678)\n",
            "Epoch: [144][750/782]\tTime 0.022 (0.025)\tLoss (Class) 0.6845 (0.6736)\tLoss (Regu) 0.1901 (0.1892)\tPrec@1 84.375 (84.616)\tPrec@5 98.438 (98.671)\n",
            "Test: [0/157]\tTime 0.116 (0.116)\tLoss (Class) 1.9756 (1.9756)\tLoss (Regu) 0.2224 (0.2224)\tPrec@1 65.625 (65.625)\tPrec@5 84.375 (84.375)\n",
            "Test: [50/157]\tTime 0.009 (0.013)\tLoss (Class) 1.6195 (1.8839)\tLoss (Regu) 0.2243 (0.2203)\tPrec@1 65.625 (61.949)\tPrec@5 90.625 (87.469)\n",
            "Test: [100/157]\tTime 0.009 (0.012)\tLoss (Class) 2.4153 (1.8470)\tLoss (Regu) 0.2226 (0.2203)\tPrec@1 59.375 (62.717)\tPrec@5 79.688 (87.809)\n",
            "Test: [150/157]\tTime 0.007 (0.011)\tLoss (Class) 1.8867 (1.8719)\tLoss (Regu) 0.2209 (0.2202)\tPrec@1 59.375 (62.479)\tPrec@5 87.500 (87.697)\n",
            " * Train[84.518 %, 98.662 %, 0.676 loss] Val [62.400 %, 87.610%, 1.877 loss] Best: 66.040 %\n",
            "Time for 144 / 150 21.316814422607422\n",
            "Learning rate:  0.03\n",
            "Epoch: [145][0/782]\tTime 0.155 (0.155)\tLoss (Class) 0.7087 (0.7087)\tLoss (Regu) 0.1901 (0.1901)\tPrec@1 84.375 (84.375)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [145][50/782]\tTime 0.022 (0.027)\tLoss (Class) 0.6917 (0.6507)\tLoss (Regu) 0.1866 (0.1905)\tPrec@1 81.250 (85.447)\tPrec@5 96.875 (99.020)\n",
            "Epoch: [145][100/782]\tTime 0.030 (0.025)\tLoss (Class) 0.7021 (0.6485)\tLoss (Regu) 0.1907 (0.1899)\tPrec@1 85.938 (85.365)\tPrec@5 98.438 (98.902)\n",
            "Epoch: [145][150/782]\tTime 0.023 (0.025)\tLoss (Class) 0.6535 (0.6528)\tLoss (Regu) 0.1897 (0.1900)\tPrec@1 87.500 (85.410)\tPrec@5 100.000 (98.924)\n",
            "Epoch: [145][200/782]\tTime 0.030 (0.025)\tLoss (Class) 0.5732 (0.6530)\tLoss (Regu) 0.1901 (0.1901)\tPrec@1 92.188 (85.557)\tPrec@5 100.000 (98.881)\n",
            "Epoch: [145][250/782]\tTime 0.022 (0.025)\tLoss (Class) 0.6918 (0.6516)\tLoss (Regu) 0.1918 (0.1904)\tPrec@1 87.500 (85.483)\tPrec@5 100.000 (98.942)\n",
            "Epoch: [145][300/782]\tTime 0.030 (0.025)\tLoss (Class) 0.8866 (0.6516)\tLoss (Regu) 0.1865 (0.1904)\tPrec@1 73.438 (85.304)\tPrec@5 96.875 (98.936)\n",
            "Epoch: [145][350/782]\tTime 0.023 (0.025)\tLoss (Class) 0.6377 (0.6553)\tLoss (Regu) 0.1908 (0.1904)\tPrec@1 85.938 (85.105)\tPrec@5 100.000 (98.918)\n",
            "Epoch: [145][400/782]\tTime 0.022 (0.025)\tLoss (Class) 0.6359 (0.6564)\tLoss (Regu) 0.1882 (0.1904)\tPrec@1 79.688 (84.987)\tPrec@5 98.438 (98.925)\n",
            "Epoch: [145][450/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7125 (0.6563)\tLoss (Regu) 0.1920 (0.1905)\tPrec@1 76.562 (84.971)\tPrec@5 98.438 (98.888)\n",
            "Epoch: [145][500/782]\tTime 0.023 (0.024)\tLoss (Class) 0.5909 (0.6602)\tLoss (Regu) 0.1872 (0.1903)\tPrec@1 87.500 (84.849)\tPrec@5 100.000 (98.874)\n",
            "Epoch: [145][550/782]\tTime 0.022 (0.024)\tLoss (Class) 0.9624 (0.6648)\tLoss (Regu) 0.1906 (0.1902)\tPrec@1 82.812 (84.701)\tPrec@5 98.438 (98.835)\n",
            "Epoch: [145][600/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6669 (0.6686)\tLoss (Regu) 0.1924 (0.1901)\tPrec@1 85.938 (84.591)\tPrec@5 100.000 (98.817)\n",
            "Epoch: [145][650/782]\tTime 0.022 (0.024)\tLoss (Class) 0.7976 (0.6720)\tLoss (Regu) 0.1904 (0.1900)\tPrec@1 85.938 (84.469)\tPrec@5 98.438 (98.786)\n",
            "Epoch: [145][700/782]\tTime 0.022 (0.024)\tLoss (Class) 0.6195 (0.6750)\tLoss (Regu) 0.1892 (0.1900)\tPrec@1 87.500 (84.355)\tPrec@5 95.312 (98.763)\n",
            "Epoch: [145][750/782]\tTime 0.023 (0.024)\tLoss (Class) 0.5843 (0.6788)\tLoss (Regu) 0.1880 (0.1899)\tPrec@1 85.938 (84.211)\tPrec@5 100.000 (98.758)\n",
            "Test: [0/157]\tTime 0.113 (0.113)\tLoss (Class) 1.4118 (1.4118)\tLoss (Regu) 0.2148 (0.2148)\tPrec@1 71.875 (71.875)\tPrec@5 92.188 (92.188)\n",
            "Test: [50/157]\tTime 0.008 (0.010)\tLoss (Class) 1.7589 (1.6871)\tLoss (Regu) 0.2157 (0.2094)\tPrec@1 64.062 (63.143)\tPrec@5 87.500 (88.817)\n",
            "Test: [100/157]\tTime 0.009 (0.009)\tLoss (Class) 1.4342 (1.7255)\tLoss (Regu) 0.2075 (0.2096)\tPrec@1 68.750 (63.320)\tPrec@5 92.188 (88.490)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.5325 (1.7332)\tLoss (Regu) 0.2113 (0.2096)\tPrec@1 68.750 (63.380)\tPrec@5 95.312 (88.649)\n",
            " * Train[84.116 %, 98.722 %, 0.681 loss] Val [63.390 %, 88.650%, 1.730 loss] Best: 66.040 %\n",
            "Time for 145 / 150 20.47984743118286\n",
            "Learning rate:  0.03\n",
            "Epoch: [146][0/782]\tTime 0.153 (0.153)\tLoss (Class) 0.6403 (0.6403)\tLoss (Regu) 0.1826 (0.1826)\tPrec@1 87.500 (87.500)\tPrec@5 98.438 (98.438)\n",
            "Epoch: [146][50/782]\tTime 0.023 (0.028)\tLoss (Class) 0.5190 (0.6624)\tLoss (Regu) 0.1874 (0.1888)\tPrec@1 89.062 (85.172)\tPrec@5 100.000 (98.805)\n",
            "Epoch: [146][100/782]\tTime 0.037 (0.026)\tLoss (Class) 0.7209 (0.6575)\tLoss (Regu) 0.1901 (0.1885)\tPrec@1 79.688 (85.195)\tPrec@5 100.000 (98.933)\n",
            "Epoch: [146][150/782]\tTime 0.023 (0.026)\tLoss (Class) 0.7032 (0.6593)\tLoss (Regu) 0.1904 (0.1892)\tPrec@1 84.375 (85.099)\tPrec@5 98.438 (98.851)\n",
            "Epoch: [146][200/782]\tTime 0.022 (0.025)\tLoss (Class) 0.6690 (0.6616)\tLoss (Regu) 0.1864 (0.1890)\tPrec@1 87.500 (85.044)\tPrec@5 98.438 (98.787)\n",
            "Epoch: [146][250/782]\tTime 0.024 (0.025)\tLoss (Class) 0.6875 (0.6677)\tLoss (Regu) 0.1875 (0.1884)\tPrec@1 85.938 (84.767)\tPrec@5 100.000 (98.687)\n",
            "Epoch: [146][300/782]\tTime 0.023 (0.025)\tLoss (Class) 0.6989 (0.6705)\tLoss (Regu) 0.1854 (0.1883)\tPrec@1 82.812 (84.676)\tPrec@5 100.000 (98.666)\n",
            "Epoch: [146][350/782]\tTime 0.022 (0.024)\tLoss (Class) 0.8174 (0.6689)\tLoss (Regu) 0.1888 (0.1886)\tPrec@1 76.562 (84.749)\tPrec@5 100.000 (98.656)\n",
            "Epoch: [146][400/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6953 (0.6692)\tLoss (Regu) 0.1901 (0.1884)\tPrec@1 81.250 (84.753)\tPrec@5 98.438 (98.644)\n",
            "Epoch: [146][450/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7741 (0.6701)\tLoss (Regu) 0.1868 (0.1884)\tPrec@1 82.812 (84.760)\tPrec@5 100.000 (98.635)\n",
            "Epoch: [146][500/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7832 (0.6685)\tLoss (Regu) 0.1898 (0.1883)\tPrec@1 79.688 (84.824)\tPrec@5 98.438 (98.675)\n",
            "Epoch: [146][550/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6830 (0.6702)\tLoss (Regu) 0.1887 (0.1885)\tPrec@1 85.938 (84.752)\tPrec@5 98.438 (98.662)\n",
            "Epoch: [146][600/782]\tTime 0.022 (0.024)\tLoss (Class) 0.5829 (0.6709)\tLoss (Regu) 0.1879 (0.1884)\tPrec@1 89.062 (84.757)\tPrec@5 100.000 (98.677)\n",
            "Epoch: [146][650/782]\tTime 0.022 (0.024)\tLoss (Class) 0.7610 (0.6723)\tLoss (Regu) 0.1855 (0.1883)\tPrec@1 82.812 (84.725)\tPrec@5 96.875 (98.685)\n",
            "Epoch: [146][700/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7295 (0.6758)\tLoss (Regu) 0.1927 (0.1884)\tPrec@1 81.250 (84.636)\tPrec@5 98.438 (98.672)\n",
            "Epoch: [146][750/782]\tTime 0.021 (0.024)\tLoss (Class) 0.5159 (0.6780)\tLoss (Regu) 0.1919 (0.1885)\tPrec@1 90.625 (84.546)\tPrec@5 98.438 (98.643)\n",
            "Test: [0/157]\tTime 0.117 (0.117)\tLoss (Class) 1.9046 (1.9046)\tLoss (Regu) 0.2228 (0.2228)\tPrec@1 60.938 (60.938)\tPrec@5 84.375 (84.375)\n",
            "Test: [50/157]\tTime 0.008 (0.011)\tLoss (Class) 1.3532 (1.6858)\tLoss (Regu) 0.2187 (0.2202)\tPrec@1 70.312 (66.422)\tPrec@5 92.188 (89.001)\n",
            "Test: [100/157]\tTime 0.007 (0.009)\tLoss (Class) 1.3722 (1.6974)\tLoss (Regu) 0.2197 (0.2197)\tPrec@1 70.312 (65.733)\tPrec@5 92.188 (88.830)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.8778 (1.6862)\tLoss (Regu) 0.2225 (0.2197)\tPrec@1 64.062 (65.604)\tPrec@5 90.625 (89.114)\n",
            " * Train[84.508 %, 98.628 %, 0.679 loss] Val [65.590 %, 89.080%, 1.688 loss] Best: 66.040 %\n",
            "Time for 146 / 150 20.309285402297974\n",
            "Learning rate:  0.03\n",
            "Epoch: [147][0/782]\tTime 0.149 (0.149)\tLoss (Class) 0.5559 (0.5559)\tLoss (Regu) 0.1910 (0.1910)\tPrec@1 84.375 (84.375)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [147][50/782]\tTime 0.022 (0.026)\tLoss (Class) 0.6376 (0.6620)\tLoss (Regu) 0.1914 (0.1908)\tPrec@1 87.500 (85.263)\tPrec@5 96.875 (98.438)\n",
            "Epoch: [147][100/782]\tTime 0.023 (0.025)\tLoss (Class) 0.5891 (0.6586)\tLoss (Regu) 0.1874 (0.1907)\tPrec@1 92.188 (85.319)\tPrec@5 100.000 (98.639)\n",
            "Epoch: [147][150/782]\tTime 0.023 (0.025)\tLoss (Class) 0.4584 (0.6449)\tLoss (Regu) 0.1924 (0.1903)\tPrec@1 92.188 (85.668)\tPrec@5 100.000 (98.831)\n",
            "Epoch: [147][200/782]\tTime 0.023 (0.024)\tLoss (Class) 0.4529 (0.6399)\tLoss (Regu) 0.1894 (0.1904)\tPrec@1 92.188 (85.821)\tPrec@5 100.000 (98.904)\n",
            "Epoch: [147][250/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6630 (0.6452)\tLoss (Regu) 0.1893 (0.1899)\tPrec@1 85.938 (85.701)\tPrec@5 96.875 (98.886)\n",
            "Epoch: [147][300/782]\tTime 0.022 (0.024)\tLoss (Class) 0.5745 (0.6443)\tLoss (Regu) 0.1870 (0.1899)\tPrec@1 87.500 (85.699)\tPrec@5 100.000 (98.858)\n",
            "Epoch: [147][350/782]\tTime 0.024 (0.024)\tLoss (Class) 0.6539 (0.6491)\tLoss (Regu) 0.1890 (0.1901)\tPrec@1 85.938 (85.475)\tPrec@5 100.000 (98.838)\n",
            "Epoch: [147][400/782]\tTime 0.030 (0.024)\tLoss (Class) 0.6314 (0.6504)\tLoss (Regu) 0.1897 (0.1900)\tPrec@1 87.500 (85.497)\tPrec@5 96.875 (98.804)\n",
            "Epoch: [147][450/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6739 (0.6536)\tLoss (Regu) 0.1885 (0.1899)\tPrec@1 85.938 (85.394)\tPrec@5 100.000 (98.770)\n",
            "Epoch: [147][500/782]\tTime 0.023 (0.024)\tLoss (Class) 0.5769 (0.6577)\tLoss (Regu) 0.1889 (0.1898)\tPrec@1 87.500 (85.251)\tPrec@5 98.438 (98.734)\n",
            "Epoch: [147][550/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6814 (0.6626)\tLoss (Regu) 0.1899 (0.1896)\tPrec@1 84.375 (85.027)\tPrec@5 100.000 (98.730)\n",
            "Epoch: [147][600/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7316 (0.6653)\tLoss (Regu) 0.1799 (0.1896)\tPrec@1 84.375 (84.882)\tPrec@5 100.000 (98.716)\n",
            "Epoch: [147][650/782]\tTime 0.022 (0.024)\tLoss (Class) 0.8127 (0.6700)\tLoss (Regu) 0.1881 (0.1894)\tPrec@1 82.812 (84.771)\tPrec@5 96.875 (98.668)\n",
            "Epoch: [147][700/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7412 (0.6710)\tLoss (Regu) 0.1868 (0.1893)\tPrec@1 81.250 (84.705)\tPrec@5 96.875 (98.694)\n",
            "Epoch: [147][750/782]\tTime 0.023 (0.024)\tLoss (Class) 0.6537 (0.6705)\tLoss (Regu) 0.1912 (0.1892)\tPrec@1 82.812 (84.714)\tPrec@5 100.000 (98.689)\n",
            "Test: [0/157]\tTime 0.116 (0.116)\tLoss (Class) 1.8413 (1.8413)\tLoss (Regu) 0.2150 (0.2150)\tPrec@1 57.812 (57.812)\tPrec@5 90.625 (90.625)\n",
            "Test: [50/157]\tTime 0.007 (0.012)\tLoss (Class) 2.0935 (1.7715)\tLoss (Regu) 0.2146 (0.2160)\tPrec@1 64.062 (63.756)\tPrec@5 85.938 (89.154)\n",
            "Test: [100/157]\tTime 0.008 (0.011)\tLoss (Class) 2.6120 (1.7961)\tLoss (Regu) 0.2154 (0.2164)\tPrec@1 54.688 (63.103)\tPrec@5 81.250 (88.861)\n",
            "Test: [150/157]\tTime 0.007 (0.010)\tLoss (Class) 1.7391 (1.7944)\tLoss (Regu) 0.2225 (0.2165)\tPrec@1 71.875 (63.328)\tPrec@5 85.938 (88.711)\n",
            " * Train[84.654 %, 98.658 %, 0.673 loss] Val [63.320 %, 88.670%, 1.793 loss] Best: 66.040 %\n",
            "Time for 147 / 150 20.565752506256104\n",
            "Learning rate:  0.03\n",
            "Epoch: [148][0/782]\tTime 0.156 (0.156)\tLoss (Class) 0.6394 (0.6394)\tLoss (Regu) 0.1878 (0.1878)\tPrec@1 84.375 (84.375)\tPrec@5 98.438 (98.438)\n",
            "Epoch: [148][50/782]\tTime 0.023 (0.027)\tLoss (Class) 0.6585 (0.6277)\tLoss (Regu) 0.1868 (0.1898)\tPrec@1 84.375 (86.121)\tPrec@5 98.438 (98.989)\n",
            "Epoch: [148][100/782]\tTime 0.021 (0.025)\tLoss (Class) 0.6280 (0.6240)\tLoss (Regu) 0.1912 (0.1888)\tPrec@1 87.500 (86.170)\tPrec@5 100.000 (99.010)\n",
            "Epoch: [148][150/782]\tTime 0.023 (0.025)\tLoss (Class) 0.8501 (0.6118)\tLoss (Regu) 0.1876 (0.1890)\tPrec@1 78.125 (86.589)\tPrec@5 95.312 (99.048)\n",
            "Epoch: [148][200/782]\tTime 0.022 (0.025)\tLoss (Class) 0.6363 (0.6160)\tLoss (Regu) 0.1892 (0.1888)\tPrec@1 85.938 (86.427)\tPrec@5 96.875 (99.005)\n",
            "Epoch: [148][250/782]\tTime 0.023 (0.025)\tLoss (Class) 0.5039 (0.6247)\tLoss (Regu) 0.1844 (0.1886)\tPrec@1 90.625 (86.112)\tPrec@5 100.000 (98.929)\n",
            "Epoch: [148][300/782]\tTime 0.022 (0.025)\tLoss (Class) 0.7397 (0.6316)\tLoss (Regu) 0.1918 (0.1885)\tPrec@1 78.125 (85.870)\tPrec@5 100.000 (98.848)\n",
            "Epoch: [148][350/782]\tTime 0.023 (0.025)\tLoss (Class) 0.5511 (0.6375)\tLoss (Regu) 0.1898 (0.1886)\tPrec@1 89.062 (85.733)\tPrec@5 98.438 (98.798)\n",
            "Epoch: [148][400/782]\tTime 0.025 (0.025)\tLoss (Class) 0.5672 (0.6441)\tLoss (Regu) 0.1835 (0.1886)\tPrec@1 89.062 (85.466)\tPrec@5 98.438 (98.745)\n",
            "Epoch: [148][450/782]\tTime 0.022 (0.025)\tLoss (Class) 0.9765 (0.6492)\tLoss (Regu) 0.1883 (0.1885)\tPrec@1 76.562 (85.258)\tPrec@5 98.438 (98.760)\n",
            "Epoch: [148][500/782]\tTime 0.023 (0.025)\tLoss (Class) 0.8163 (0.6543)\tLoss (Regu) 0.1890 (0.1882)\tPrec@1 84.375 (85.105)\tPrec@5 96.875 (98.762)\n",
            "Epoch: [148][550/782]\tTime 0.021 (0.025)\tLoss (Class) 0.8848 (0.6559)\tLoss (Regu) 0.1876 (0.1882)\tPrec@1 81.250 (85.067)\tPrec@5 93.750 (98.738)\n",
            "Epoch: [148][600/782]\tTime 0.022 (0.025)\tLoss (Class) 0.5889 (0.6610)\tLoss (Regu) 0.1873 (0.1881)\tPrec@1 90.625 (84.944)\tPrec@5 100.000 (98.684)\n",
            "Epoch: [148][650/782]\tTime 0.022 (0.025)\tLoss (Class) 0.8567 (0.6650)\tLoss (Regu) 0.1862 (0.1881)\tPrec@1 82.812 (84.759)\tPrec@5 98.438 (98.663)\n",
            "Epoch: [148][700/782]\tTime 0.024 (0.025)\tLoss (Class) 0.6037 (0.6676)\tLoss (Regu) 0.1912 (0.1880)\tPrec@1 82.812 (84.694)\tPrec@5 100.000 (98.651)\n",
            "Epoch: [148][750/782]\tTime 0.030 (0.025)\tLoss (Class) 0.8665 (0.6719)\tLoss (Regu) 0.1865 (0.1880)\tPrec@1 78.125 (84.568)\tPrec@5 98.438 (98.650)\n",
            "Test: [0/157]\tTime 0.110 (0.110)\tLoss (Class) 0.9466 (0.9466)\tLoss (Regu) 0.2230 (0.2230)\tPrec@1 79.688 (79.688)\tPrec@5 95.312 (95.312)\n",
            "Test: [50/157]\tTime 0.008 (0.010)\tLoss (Class) 1.9819 (1.7283)\tLoss (Regu) 0.2211 (0.2184)\tPrec@1 59.375 (65.104)\tPrec@5 85.938 (88.634)\n",
            "Test: [100/157]\tTime 0.007 (0.009)\tLoss (Class) 1.7555 (1.7684)\tLoss (Regu) 0.2188 (0.2185)\tPrec@1 68.750 (63.861)\tPrec@5 90.625 (88.614)\n",
            "Test: [150/157]\tTime 0.008 (0.009)\tLoss (Class) 1.6933 (1.7830)\tLoss (Regu) 0.2175 (0.2186)\tPrec@1 65.625 (63.452)\tPrec@5 89.062 (88.514)\n",
            " * Train[84.510 %, 98.628 %, 0.674 loss] Val [63.460 %, 88.510%, 1.784 loss] Best: 66.040 %\n",
            "Time for 148 / 150 20.811513900756836\n",
            "Learning rate:  0.03\n",
            "Epoch: [149][0/782]\tTime 0.153 (0.153)\tLoss (Class) 0.4493 (0.4493)\tLoss (Regu) 0.1914 (0.1914)\tPrec@1 92.188 (92.188)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [149][50/782]\tTime 0.023 (0.027)\tLoss (Class) 0.7583 (0.6227)\tLoss (Regu) 0.1911 (0.1916)\tPrec@1 81.250 (86.520)\tPrec@5 98.438 (98.958)\n",
            "Epoch: [149][100/782]\tTime 0.023 (0.026)\tLoss (Class) 0.7771 (0.6124)\tLoss (Regu) 0.1870 (0.1912)\tPrec@1 84.375 (87.191)\tPrec@5 93.750 (98.963)\n",
            "Epoch: [149][150/782]\tTime 0.030 (0.026)\tLoss (Class) 0.5513 (0.6204)\tLoss (Regu) 0.1893 (0.1906)\tPrec@1 89.062 (86.724)\tPrec@5 100.000 (99.048)\n",
            "Epoch: [149][200/782]\tTime 0.030 (0.026)\tLoss (Class) 0.7993 (0.6253)\tLoss (Regu) 0.1852 (0.1900)\tPrec@1 78.125 (86.404)\tPrec@5 98.438 (98.982)\n",
            "Epoch: [149][250/782]\tTime 0.023 (0.026)\tLoss (Class) 0.6570 (0.6274)\tLoss (Regu) 0.1951 (0.1899)\tPrec@1 85.938 (86.261)\tPrec@5 100.000 (98.979)\n",
            "Epoch: [149][300/782]\tTime 0.030 (0.026)\tLoss (Class) 0.6911 (0.6343)\tLoss (Regu) 0.1907 (0.1899)\tPrec@1 87.500 (86.104)\tPrec@5 96.875 (98.910)\n",
            "Epoch: [149][350/782]\tTime 0.022 (0.025)\tLoss (Class) 0.7991 (0.6352)\tLoss (Regu) 0.1891 (0.1900)\tPrec@1 85.938 (86.075)\tPrec@5 96.875 (98.914)\n",
            "Epoch: [149][400/782]\tTime 0.022 (0.025)\tLoss (Class) 0.8062 (0.6432)\tLoss (Regu) 0.1876 (0.1898)\tPrec@1 81.250 (85.821)\tPrec@5 95.312 (98.819)\n",
            "Epoch: [149][450/782]\tTime 0.022 (0.025)\tLoss (Class) 0.6582 (0.6490)\tLoss (Regu) 0.1911 (0.1897)\tPrec@1 84.375 (85.692)\tPrec@5 100.000 (98.787)\n",
            "Epoch: [149][500/782]\tTime 0.023 (0.025)\tLoss (Class) 0.6188 (0.6546)\tLoss (Regu) 0.1903 (0.1897)\tPrec@1 87.500 (85.501)\tPrec@5 100.000 (98.743)\n",
            "Epoch: [149][550/782]\tTime 0.023 (0.025)\tLoss (Class) 0.7527 (0.6542)\tLoss (Regu) 0.1893 (0.1897)\tPrec@1 81.250 (85.538)\tPrec@5 98.438 (98.744)\n",
            "Epoch: [149][600/782]\tTime 0.023 (0.024)\tLoss (Class) 0.5803 (0.6570)\tLoss (Regu) 0.1894 (0.1896)\tPrec@1 87.500 (85.418)\tPrec@5 100.000 (98.723)\n",
            "Epoch: [149][650/782]\tTime 0.023 (0.024)\tLoss (Class) 0.5270 (0.6588)\tLoss (Regu) 0.1868 (0.1896)\tPrec@1 89.062 (85.354)\tPrec@5 98.438 (98.680)\n",
            "Epoch: [149][700/782]\tTime 0.023 (0.024)\tLoss (Class) 0.7177 (0.6604)\tLoss (Regu) 0.1867 (0.1895)\tPrec@1 82.812 (85.320)\tPrec@5 100.000 (98.680)\n",
            "Epoch: [149][750/782]\tTime 0.022 (0.025)\tLoss (Class) 0.7244 (0.6640)\tLoss (Regu) 0.1852 (0.1895)\tPrec@1 81.250 (85.209)\tPrec@5 98.438 (98.673)\n",
            "Test: [0/157]\tTime 0.116 (0.116)\tLoss (Class) 1.3061 (1.3061)\tLoss (Regu) 0.2158 (0.2158)\tPrec@1 70.312 (70.312)\tPrec@5 92.188 (92.188)\n",
            "Test: [50/157]\tTime 0.007 (0.011)\tLoss (Class) 1.0618 (1.6430)\tLoss (Regu) 0.2183 (0.2155)\tPrec@1 78.125 (65.227)\tPrec@5 95.312 (90.196)\n",
            "Test: [100/157]\tTime 0.009 (0.009)\tLoss (Class) 1.6420 (1.6974)\tLoss (Regu) 0.2131 (0.2152)\tPrec@1 70.312 (64.851)\tPrec@5 92.188 (89.171)\n",
            "Test: [150/157]\tTime 0.007 (0.009)\tLoss (Class) 1.1831 (1.7337)\tLoss (Regu) 0.2164 (0.2154)\tPrec@1 76.562 (63.990)\tPrec@5 92.188 (88.804)\n",
            " * Train[85.118 %, 98.672 %, 0.666 loss] Val [63.930 %, 88.720%, 1.740 loss] Best: 66.040 %\n",
            "Time for 149 / 150 20.654072999954224\n",
            "Best accuracy:  66.04\n"
          ]
        }
      ],
      "source": [
        "!python train.py --epochs 150 -b 64 --lr 0.03 --name dwnn_cifar-100_32 \\\n",
        "--lrdecay 150 225 --database cifar-100 --tempdir ~/tmp dawn \\\n",
        "--regu_detail 0.1 --regu_approx 0.1 --levels 3 --first_conv 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzTEW5WpDh1U",
        "outputId": "bd4cb662-99f8-4809-c977-273bb32b3b4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloaded to /content/DAWN_WACV2020/data/kth-tips2-b_col_200x200.tar\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://www.csc.kth.se/cvap/databases/kth-tips/kth-tips2-b_col_200x200.tar\"\n",
        "out_path = \"/content/DAWN_WACV2020/data/kth-tips2-b_col_200x200.tar\"\n",
        "\n",
        "with requests.get(url, stream=True) as r:\n",
        "    r.raise_for_status()\n",
        "    with open(out_path, \"wb\") as f:\n",
        "        for chunk in r.iter_content(chunk_size=8192):\n",
        "            if chunk:  # filter out keep-alive chunks\n",
        "                f.write(chunk)\n",
        "\n",
        "print(\"Downloaded to\", out_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtHR5PhSN62r",
        "outputId": "7d7e5d8f-82be-475e-905b-9b35bfe13a22"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-856776700.py:6: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tar.extractall(path=extract_root)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted to /content/DAWN_WACV2020/data/KTH-TIPS2-b\n"
          ]
        }
      ],
      "source": [
        "import tarfile\n",
        "tar_path = \"/content/DAWN_WACV2020/data/kth-tips2-b_col_200x200.tar\"\n",
        "extract_root = \"/content/DAWN_WACV2020/data/KTH-TIPS2-b\"  # choose where to extract\n",
        "\n",
        "with tarfile.open(tar_path, \"r:\") as tar:\n",
        "    tar.extractall(path=extract_root)\n",
        "\n",
        "print(\"Extracted to\", extract_root)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHU3mvS8O2E0",
        "outputId": "df3e483a-313f-4df9-d770-2a37f0dbefcb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 3801 Test: 951\n"
          ]
        }
      ],
      "source": [
        "import os, shutil, random\n",
        "\n",
        "src_root = extract_root  # where images currently are\n",
        "kth_data_dir = \"/content/DAWN_WACV2020/data\"\n",
        "target_root = os.path.join(kth_data_dir, \"KTH-TIPS2-b3\")\n",
        "\n",
        "train_dir = os.path.join(target_root, \"Train\")\n",
        "test_dir  = os.path.join(target_root, \"Test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Example: random 80/20 split over all images\n",
        "all_imgs = []\n",
        "for root, _, files in os.walk(src_root):\n",
        "    for f in files:\n",
        "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".tif\", \".bmp\")):\n",
        "            all_imgs.append(os.path.join(root, f))\n",
        "\n",
        "random.shuffle(all_imgs)\n",
        "split = int(0.8 * len(all_imgs))\n",
        "train_imgs = all_imgs[:split]\n",
        "test_imgs  = all_imgs[split:]\n",
        "\n",
        "for p in train_imgs:\n",
        "    rel = os.path.relpath(p, src_root)\n",
        "    dst = os.path.join(train_dir, rel)\n",
        "    os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
        "    shutil.copy2(p, dst)\n",
        "\n",
        "for p in test_imgs:\n",
        "    rel = os.path.relpath(p, src_root)\n",
        "    dst = os.path.join(test_dir, rel)\n",
        "    os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
        "    shutil.copy2(p, dst)\n",
        "\n",
        "print(\"Train:\", len(train_imgs), \"Test:\", len(test_imgs))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "koaIYWGONno8",
        "outputId": "07933fa6-c379-4cd5-80e2-4ff9943589e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ignore tensorboard logger\n",
            "Launch...\n",
            "['train.py', '--epochs', '90', '-b', '16', '--lr', '0.03', '--name', 'dwnn_kth_3_dwnn_l5_16', '--lrdecay', '30', '60', '--database', 'kth', '--traindir', '/content/DAWN_WACV2020/data/KTH-TIPS2-b3/Test/', '--valdir', '/content/DAWN_WACV2020/data/KTH-TIPS2-b3/Train/', 'dawn', '--levels', '5', '--first_conv', '16', '--regu_details', '0.1', '--regu_approx', '0.1']\n",
            "DAWN:\n",
            "- first conv: 16\n",
            "- image size: 224\n",
            "- nb levels : 5\n",
            "- levels U/P: [2, 1]\n",
            "- channels:  3\n",
            "Final channel: 256\n",
            "Final size   : 7\n",
            "Number of model parameters            : 68,667\n",
            "Number of *trainable* model parameters: 68,667\n",
            "Learning rate:  0.03\n",
            "Epoch: [0][0/60]\tTime 1.144 (1.144)\tLoss (Class) 4.5820 (4.5820)\tLoss (Regu) 1.4635 (1.4635)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.000)\n",
            "Epoch: [0][50/60]\tTime 0.049 (0.062)\tLoss (Class) 0.5269 (0.9610)\tLoss (Regu) 0.5201 (0.8965)\tPrec@1 100.000 (98.039)\tPrec@5 100.000 (98.039)\n",
            "Test: [0/238]\tTime 0.179 (0.179)\tLoss (Class) 0.6388 (0.6388)\tLoss (Regu) 0.6309 (0.6309)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.013 (0.019)\tLoss (Class) 0.6496 (0.6255)\tLoss (Regu) 0.6382 (0.6162)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.035 (0.019)\tLoss (Class) 0.2729 (0.5872)\tLoss (Regu) 0.2619 (0.5777)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.011 (0.018)\tLoss (Class) 0.3608 (0.5566)\tLoss (Regu) 0.3496 (0.5471)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.041 (0.019)\tLoss (Class) 0.2950 (0.5439)\tLoss (Regu) 0.2843 (0.5342)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[98.318 %, 98.318 %, 0.895 loss] Val [100.000 %, 100.000%, 0.541 loss] Best: 100.000 %\n",
            "Time for 0 / 90 8.291475772857666\n",
            "Learning rate:  0.03\n",
            "Epoch: [1][0/60]\tTime 0.221 (0.221)\tLoss (Class) 0.4066 (0.4066)\tLoss (Regu) 0.3943 (0.3943)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [1][50/60]\tTime 0.040 (0.044)\tLoss (Class) 0.2942 (0.3601)\tLoss (Regu) 0.2874 (0.3526)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.180 (0.180)\tLoss (Class) 0.5796 (0.5796)\tLoss (Regu) 0.5724 (0.5724)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.013 (0.019)\tLoss (Class) 0.7070 (0.4053)\tLoss (Regu) 0.7000 (0.3985)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.013 (0.017)\tLoss (Class) 0.1212 (0.3501)\tLoss (Regu) 0.1142 (0.3433)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.015 (0.017)\tLoss (Class) 0.1377 (0.3434)\tLoss (Regu) 0.1313 (0.3366)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.015 (0.017)\tLoss (Class) 0.5569 (0.3458)\tLoss (Regu) 0.5502 (0.3389)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.348 loss] Val [100.000 %, 100.000%, 0.357 loss] Best: 100.000 %\n",
            "Time for 1 / 90 6.644812107086182\n",
            "Learning rate:  0.03\n",
            "Epoch: [2][0/60]\tTime 0.220 (0.220)\tLoss (Class) 0.2452 (0.2452)\tLoss (Regu) 0.2376 (0.2376)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [2][50/60]\tTime 0.038 (0.043)\tLoss (Class) 0.2365 (0.2349)\tLoss (Regu) 0.2331 (0.2300)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.174 (0.174)\tLoss (Class) 0.3788 (0.3788)\tLoss (Regu) 0.3753 (0.3753)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.014 (0.020)\tLoss (Class) 0.0658 (0.1963)\tLoss (Regu) 0.0619 (0.1928)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.028 (0.019)\tLoss (Class) 0.0543 (0.1985)\tLoss (Regu) 0.0502 (0.1951)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.016 (0.018)\tLoss (Class) 0.0784 (0.1872)\tLoss (Regu) 0.0752 (0.1838)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.029 (0.018)\tLoss (Class) 0.1069 (0.1998)\tLoss (Regu) 0.1033 (0.1963)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.228 loss] Val [100.000 %, 100.000%, 0.205 loss] Best: 100.000 %\n",
            "Time for 2 / 90 6.8029093742370605\n",
            "Learning rate:  0.03\n",
            "Epoch: [3][0/60]\tTime 0.222 (0.222)\tLoss (Class) 0.2162 (0.2162)\tLoss (Regu) 0.2137 (0.2137)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [3][50/60]\tTime 0.040 (0.045)\tLoss (Class) 0.1221 (0.1800)\tLoss (Regu) 0.1197 (0.1773)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.175 (0.175)\tLoss (Class) 0.2101 (0.2101)\tLoss (Regu) 0.2080 (0.2080)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.018 (0.020)\tLoss (Class) 0.3979 (0.2322)\tLoss (Regu) 0.3960 (0.2301)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.013 (0.018)\tLoss (Class) 0.0510 (0.2059)\tLoss (Regu) 0.0487 (0.2037)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.012 (0.018)\tLoss (Class) 0.0588 (0.2141)\tLoss (Regu) 0.0566 (0.2120)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.021 (0.018)\tLoss (Class) 0.5047 (0.2111)\tLoss (Regu) 0.5027 (0.2089)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.178 loss] Val [100.000 %, 100.000%, 0.201 loss] Best: 100.000 %\n",
            "Time for 3 / 90 6.796775579452515\n",
            "Learning rate:  0.03\n",
            "Epoch: [4][0/60]\tTime 0.206 (0.206)\tLoss (Class) 0.1137 (0.1137)\tLoss (Regu) 0.1113 (0.1113)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [4][50/60]\tTime 0.038 (0.042)\tLoss (Class) 0.0846 (0.1331)\tLoss (Regu) 0.0830 (0.1313)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.171 (0.171)\tLoss (Class) 0.0703 (0.0703)\tLoss (Regu) 0.0687 (0.0687)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.013 (0.019)\tLoss (Class) 0.3314 (0.1611)\tLoss (Regu) 0.3301 (0.1596)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.023 (0.017)\tLoss (Class) 0.0469 (0.1725)\tLoss (Regu) 0.0453 (0.1710)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.012 (0.017)\tLoss (Class) 0.3365 (0.1660)\tLoss (Regu) 0.3351 (0.1645)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.013 (0.017)\tLoss (Class) 0.0707 (0.1630)\tLoss (Regu) 0.0691 (0.1615)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.129 loss] Val [100.000 %, 100.000%, 0.168 loss] Best: 100.000 %\n",
            "Time for 4 / 90 6.482654094696045\n",
            "Learning rate:  0.03\n",
            "Epoch: [5][0/60]\tTime 0.207 (0.207)\tLoss (Class) 0.0871 (0.0871)\tLoss (Regu) 0.0856 (0.0856)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [5][50/60]\tTime 0.039 (0.042)\tLoss (Class) 0.0970 (0.1037)\tLoss (Regu) 0.0959 (0.1025)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.173 (0.173)\tLoss (Class) 0.3749 (0.3749)\tLoss (Regu) 0.3738 (0.3738)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.012 (0.019)\tLoss (Class) 0.1284 (0.2577)\tLoss (Regu) 0.1272 (0.2566)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.011 (0.018)\tLoss (Class) 0.0969 (0.2252)\tLoss (Regu) 0.0958 (0.2241)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.024 (0.017)\tLoss (Class) 0.0587 (0.2048)\tLoss (Regu) 0.0576 (0.2037)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.018 (0.016)\tLoss (Class) 0.0437 (0.2078)\tLoss (Regu) 0.0424 (0.2067)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.103 loss] Val [100.000 %, 100.000%, 0.199 loss] Best: 100.000 %\n",
            "Time for 5 / 90 6.436446189880371\n",
            "Learning rate:  0.03\n",
            "Epoch: [6][0/60]\tTime 0.221 (0.221)\tLoss (Class) 0.0783 (0.0783)\tLoss (Regu) 0.0772 (0.0772)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [6][50/60]\tTime 0.037 (0.042)\tLoss (Class) 0.0576 (0.0858)\tLoss (Regu) 0.0567 (0.0849)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.172 (0.172)\tLoss (Class) 0.0504 (0.0504)\tLoss (Regu) 0.0496 (0.0496)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.018 (0.019)\tLoss (Class) 0.0363 (0.0797)\tLoss (Regu) 0.0354 (0.0789)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.011 (0.018)\tLoss (Class) 0.0354 (0.0992)\tLoss (Regu) 0.0344 (0.0983)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.012 (0.018)\tLoss (Class) 0.3955 (0.1071)\tLoss (Regu) 0.3947 (0.1062)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.018 (0.018)\tLoss (Class) 0.1899 (0.1033)\tLoss (Regu) 0.1891 (0.1025)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.083 loss] Val [100.000 %, 100.000%, 0.105 loss] Best: 100.000 %\n",
            "Time for 6 / 90 6.745247840881348\n",
            "Learning rate:  0.03\n",
            "Epoch: [7][0/60]\tTime 0.211 (0.211)\tLoss (Class) 0.0854 (0.0854)\tLoss (Regu) 0.0846 (0.0846)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [7][50/60]\tTime 0.046 (0.044)\tLoss (Class) 0.0417 (0.0633)\tLoss (Regu) 0.0410 (0.0626)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.179 (0.179)\tLoss (Class) 0.0307 (0.0307)\tLoss (Regu) 0.0300 (0.0300)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.012 (0.020)\tLoss (Class) 0.4844 (0.1268)\tLoss (Regu) 0.4839 (0.1262)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.032 (0.019)\tLoss (Class) 0.2648 (0.1168)\tLoss (Regu) 0.2642 (0.1161)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.013 (0.018)\tLoss (Class) 0.0252 (0.1249)\tLoss (Regu) 0.0245 (0.1242)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.033 (0.018)\tLoss (Class) 0.0240 (0.1232)\tLoss (Regu) 0.0233 (0.1226)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.063 loss] Val [100.000 %, 100.000%, 0.124 loss] Best: 100.000 %\n",
            "Time for 7 / 90 6.83128023147583\n",
            "Learning rate:  0.03\n",
            "Epoch: [8][0/60]\tTime 0.218 (0.218)\tLoss (Class) 0.0433 (0.0433)\tLoss (Regu) 0.0427 (0.0427)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [8][50/60]\tTime 0.037 (0.041)\tLoss (Class) 0.0363 (0.0554)\tLoss (Regu) 0.0358 (0.0549)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.179 (0.179)\tLoss (Class) 0.0563 (0.0563)\tLoss (Regu) 0.0558 (0.0558)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.013 (0.018)\tLoss (Class) 0.0322 (0.0848)\tLoss (Regu) 0.0317 (0.0843)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.012 (0.017)\tLoss (Class) 0.0316 (0.0994)\tLoss (Regu) 0.0311 (0.0989)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.013 (0.017)\tLoss (Class) 0.0343 (0.0979)\tLoss (Regu) 0.0338 (0.0974)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.035 (0.017)\tLoss (Class) 0.3567 (0.1002)\tLoss (Regu) 0.3563 (0.0997)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.054 loss] Val [100.000 %, 100.000%, 0.100 loss] Best: 100.000 %\n",
            "Time for 8 / 90 6.517253160476685\n",
            "Learning rate:  0.03\n",
            "Epoch: [9][0/60]\tTime 0.212 (0.212)\tLoss (Class) 0.0478 (0.0478)\tLoss (Regu) 0.0473 (0.0473)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [9][50/60]\tTime 0.039 (0.042)\tLoss (Class) 0.0276 (0.0479)\tLoss (Regu) 0.0272 (0.0474)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.186 (0.186)\tLoss (Class) 0.0278 (0.0278)\tLoss (Regu) 0.0274 (0.0274)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.036 (0.019)\tLoss (Class) 0.1128 (0.0661)\tLoss (Regu) 0.1124 (0.0657)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.011 (0.018)\tLoss (Class) 0.0327 (0.0636)\tLoss (Regu) 0.0322 (0.0632)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.018 (0.017)\tLoss (Class) 0.0305 (0.0574)\tLoss (Regu) 0.0301 (0.0570)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.011 (0.017)\tLoss (Class) 0.0283 (0.0577)\tLoss (Regu) 0.0279 (0.0573)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.047 loss] Val [100.000 %, 100.000%, 0.056 loss] Best: 100.000 %\n",
            "Time for 9 / 90 6.45502781867981\n",
            "Learning rate:  0.03\n",
            "Epoch: [10][0/60]\tTime 0.217 (0.217)\tLoss (Class) 0.0770 (0.0770)\tLoss (Regu) 0.0766 (0.0766)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [10][50/60]\tTime 0.037 (0.042)\tLoss (Class) 0.0369 (0.0441)\tLoss (Regu) 0.0366 (0.0438)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.183 (0.183)\tLoss (Class) 0.0305 (0.0305)\tLoss (Regu) 0.0301 (0.0301)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.011 (0.021)\tLoss (Class) 0.1764 (0.0559)\tLoss (Regu) 0.1761 (0.0556)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.011 (0.019)\tLoss (Class) 0.0300 (0.0496)\tLoss (Regu) 0.0297 (0.0493)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.037 (0.018)\tLoss (Class) 0.0305 (0.0490)\tLoss (Regu) 0.0302 (0.0486)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.011 (0.018)\tLoss (Class) 0.0300 (0.0488)\tLoss (Regu) 0.0297 (0.0485)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.042 loss] Val [100.000 %, 100.000%, 0.052 loss] Best: 100.000 %\n",
            "Time for 10 / 90 6.614840030670166\n",
            "Learning rate:  0.03\n",
            "Epoch: [11][0/60]\tTime 0.211 (0.211)\tLoss (Class) 0.0263 (0.0263)\tLoss (Regu) 0.0260 (0.0260)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [11][50/60]\tTime 0.037 (0.041)\tLoss (Class) 0.0193 (0.0341)\tLoss (Regu) 0.0190 (0.0338)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.182 (0.182)\tLoss (Class) 0.1419 (0.1419)\tLoss (Regu) 0.1417 (0.1417)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.012 (0.020)\tLoss (Class) 0.1281 (0.0674)\tLoss (Regu) 0.1279 (0.0671)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.017 (0.018)\tLoss (Class) 0.0310 (0.0624)\tLoss (Regu) 0.0307 (0.0622)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.013 (0.017)\tLoss (Class) 0.0302 (0.0587)\tLoss (Regu) 0.0299 (0.0584)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.013 (0.017)\tLoss (Class) 0.2405 (0.0596)\tLoss (Regu) 0.2402 (0.0593)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.034 loss] Val [100.000 %, 100.000%, 0.061 loss] Best: 100.000 %\n",
            "Time for 11 / 90 6.489093065261841\n",
            "Learning rate:  0.03\n",
            "Epoch: [12][0/60]\tTime 0.208 (0.208)\tLoss (Class) 0.0284 (0.0284)\tLoss (Regu) 0.0281 (0.0281)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [12][50/60]\tTime 0.038 (0.043)\tLoss (Class) 0.0217 (0.0290)\tLoss (Regu) 0.0214 (0.0287)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.183 (0.183)\tLoss (Class) 0.0250 (0.0250)\tLoss (Regu) 0.0247 (0.0247)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.011 (0.019)\tLoss (Class) 0.0246 (0.0404)\tLoss (Regu) 0.0244 (0.0401)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.022 (0.017)\tLoss (Class) 0.1473 (0.0444)\tLoss (Regu) 0.1470 (0.0442)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.034 (0.017)\tLoss (Class) 0.0243 (0.0445)\tLoss (Regu) 0.0241 (0.0442)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.012 (0.017)\tLoss (Class) 0.0251 (0.0433)\tLoss (Regu) 0.0249 (0.0431)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.028 loss] Val [100.000 %, 100.000%, 0.042 loss] Best: 100.000 %\n",
            "Time for 12 / 90 6.629868268966675\n",
            "Learning rate:  0.03\n",
            "Epoch: [13][0/60]\tTime 0.226 (0.226)\tLoss (Class) 0.0206 (0.0206)\tLoss (Regu) 0.0203 (0.0203)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [13][50/60]\tTime 0.041 (0.044)\tLoss (Class) 0.0183 (0.0254)\tLoss (Regu) 0.0181 (0.0252)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.179 (0.179)\tLoss (Class) 0.0278 (0.0278)\tLoss (Regu) 0.0276 (0.0276)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.012 (0.018)\tLoss (Class) 0.0275 (0.0503)\tLoss (Regu) 0.0272 (0.0500)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.015 (0.017)\tLoss (Class) 0.0281 (0.0460)\tLoss (Regu) 0.0278 (0.0457)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.012 (0.017)\tLoss (Class) 0.0170 (0.0501)\tLoss (Regu) 0.0168 (0.0499)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.012 (0.017)\tLoss (Class) 0.0277 (0.0479)\tLoss (Regu) 0.0274 (0.0477)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.025 loss] Val [100.000 %, 100.000%, 0.048 loss] Best: 100.000 %\n",
            "Time for 13 / 90 6.661984205245972\n",
            "Learning rate:  0.03\n",
            "Epoch: [14][0/60]\tTime 0.213 (0.213)\tLoss (Class) 0.0695 (0.0695)\tLoss (Regu) 0.0693 (0.0693)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [14][50/60]\tTime 0.038 (0.044)\tLoss (Class) 0.0137 (0.0261)\tLoss (Regu) 0.0135 (0.0259)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.178 (0.178)\tLoss (Class) 0.1032 (0.1032)\tLoss (Regu) 0.1030 (0.1030)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.026 (0.019)\tLoss (Class) 0.0270 (0.0417)\tLoss (Regu) 0.0267 (0.0414)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.017 (0.017)\tLoss (Class) 0.0269 (0.0408)\tLoss (Regu) 0.0267 (0.0406)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.034 (0.017)\tLoss (Class) 0.1080 (0.0448)\tLoss (Regu) 0.1077 (0.0445)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.011 (0.017)\tLoss (Class) 0.0204 (0.0489)\tLoss (Regu) 0.0202 (0.0486)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.027 loss] Val [100.000 %, 100.000%, 0.048 loss] Best: 100.000 %\n",
            "Time for 14 / 90 6.584917306900024\n",
            "Learning rate:  0.03\n",
            "Epoch: [15][0/60]\tTime 0.231 (0.231)\tLoss (Class) 0.0160 (0.0160)\tLoss (Regu) 0.0158 (0.0158)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [15][50/60]\tTime 0.038 (0.042)\tLoss (Class) 0.0171 (0.0231)\tLoss (Regu) 0.0169 (0.0229)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.183 (0.183)\tLoss (Class) 0.0259 (0.0259)\tLoss (Regu) 0.0257 (0.0257)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.011 (0.018)\tLoss (Class) 0.0262 (0.0399)\tLoss (Regu) 0.0260 (0.0397)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.011 (0.017)\tLoss (Class) 0.0242 (0.0402)\tLoss (Regu) 0.0240 (0.0399)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.011 (0.017)\tLoss (Class) 0.0261 (0.0390)\tLoss (Regu) 0.0258 (0.0388)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.012 (0.017)\tLoss (Class) 0.0262 (0.0396)\tLoss (Regu) 0.0260 (0.0394)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.023 loss] Val [100.000 %, 100.000%, 0.040 loss] Best: 100.000 %\n",
            "Time for 15 / 90 6.5670325756073\n",
            "Learning rate:  0.03\n",
            "Epoch: [16][0/60]\tTime 0.218 (0.218)\tLoss (Class) 0.0141 (0.0141)\tLoss (Regu) 0.0139 (0.0139)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [16][50/60]\tTime 0.044 (0.043)\tLoss (Class) 0.0220 (0.0204)\tLoss (Regu) 0.0218 (0.0202)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.177 (0.177)\tLoss (Class) 0.0226 (0.0226)\tLoss (Regu) 0.0223 (0.0223)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.013 (0.020)\tLoss (Class) 0.0224 (0.0418)\tLoss (Regu) 0.0222 (0.0416)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.028 (0.018)\tLoss (Class) 0.0167 (0.0403)\tLoss (Regu) 0.0165 (0.0401)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.012 (0.017)\tLoss (Class) 0.0222 (0.0372)\tLoss (Regu) 0.0220 (0.0370)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.012 (0.017)\tLoss (Class) 0.0592 (0.0386)\tLoss (Regu) 0.0590 (0.0384)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.020 loss] Val [100.000 %, 100.000%, 0.039 loss] Best: 100.000 %\n",
            "Time for 16 / 90 6.647686958312988\n",
            "Learning rate:  0.03\n",
            "Epoch: [17][0/60]\tTime 0.209 (0.209)\tLoss (Class) 0.0117 (0.0117)\tLoss (Regu) 0.0115 (0.0115)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [17][50/60]\tTime 0.038 (0.042)\tLoss (Class) 0.0219 (0.0172)\tLoss (Regu) 0.0217 (0.0170)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.173 (0.173)\tLoss (Class) 0.0210 (0.0210)\tLoss (Regu) 0.0208 (0.0208)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.026 (0.019)\tLoss (Class) 0.0207 (0.0416)\tLoss (Regu) 0.0205 (0.0414)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.013 (0.019)\tLoss (Class) 0.0209 (0.0424)\tLoss (Regu) 0.0206 (0.0422)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.025 (0.018)\tLoss (Class) 0.0199 (0.0414)\tLoss (Regu) 0.0197 (0.0412)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.011 (0.017)\tLoss (Class) 0.0137 (0.0398)\tLoss (Regu) 0.0135 (0.0396)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.017 loss] Val [100.000 %, 100.000%, 0.040 loss] Best: 100.000 %\n",
            "Time for 17 / 90 6.549922943115234\n",
            "Learning rate:  0.03\n",
            "Epoch: [18][0/60]\tTime 0.218 (0.218)\tLoss (Class) 0.0134 (0.0134)\tLoss (Regu) 0.0132 (0.0132)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [18][50/60]\tTime 0.037 (0.042)\tLoss (Class) 0.0149 (0.0171)\tLoss (Regu) 0.0147 (0.0169)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.174 (0.174)\tLoss (Class) 0.0219 (0.0219)\tLoss (Regu) 0.0217 (0.0217)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.012 (0.020)\tLoss (Class) 0.0177 (0.0345)\tLoss (Regu) 0.0175 (0.0343)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.018 (0.018)\tLoss (Class) 0.0222 (0.0377)\tLoss (Regu) 0.0220 (0.0375)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.011 (0.018)\tLoss (Class) 0.0655 (0.0363)\tLoss (Regu) 0.0653 (0.0361)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.031 (0.018)\tLoss (Class) 0.0544 (0.0366)\tLoss (Regu) 0.0542 (0.0364)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.017 loss] Val [100.000 %, 100.000%, 0.036 loss] Best: 100.000 %\n",
            "Time for 18 / 90 6.856090545654297\n",
            "Learning rate:  0.03\n",
            "Epoch: [19][0/60]\tTime 0.221 (0.221)\tLoss (Class) 0.0261 (0.0261)\tLoss (Regu) 0.0259 (0.0259)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [19][50/60]\tTime 0.037 (0.042)\tLoss (Class) 0.0102 (0.0154)\tLoss (Regu) 0.0100 (0.0152)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.195 (0.195)\tLoss (Class) 0.0363 (0.0363)\tLoss (Regu) 0.0361 (0.0361)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.012 (0.022)\tLoss (Class) 0.0130 (0.0315)\tLoss (Regu) 0.0128 (0.0312)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.014 (0.019)\tLoss (Class) 0.0199 (0.0298)\tLoss (Regu) 0.0197 (0.0296)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.015 (0.018)\tLoss (Class) 0.0420 (0.0306)\tLoss (Regu) 0.0418 (0.0304)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.012 (0.018)\tLoss (Class) 0.0970 (0.0312)\tLoss (Regu) 0.0968 (0.0309)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.015 loss] Val [100.000 %, 100.000%, 0.030 loss] Best: 100.000 %\n",
            "Time for 19 / 90 6.767466068267822\n",
            "Learning rate:  0.03\n",
            "Epoch: [20][0/60]\tTime 0.217 (0.217)\tLoss (Class) 0.0111 (0.0111)\tLoss (Regu) 0.0109 (0.0109)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [20][50/60]\tTime 0.037 (0.045)\tLoss (Class) 0.0121 (0.0130)\tLoss (Regu) 0.0119 (0.0128)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.171 (0.171)\tLoss (Class) 0.0170 (0.0170)\tLoss (Regu) 0.0168 (0.0168)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.012 (0.019)\tLoss (Class) 0.0169 (0.0304)\tLoss (Regu) 0.0167 (0.0302)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.026 (0.017)\tLoss (Class) 0.0393 (0.0319)\tLoss (Regu) 0.0391 (0.0317)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.011 (0.017)\tLoss (Class) 0.0536 (0.0325)\tLoss (Regu) 0.0534 (0.0323)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.020 (0.016)\tLoss (Class) 0.0170 (0.0311)\tLoss (Regu) 0.0168 (0.0309)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.013 loss] Val [100.000 %, 100.000%, 0.032 loss] Best: 100.000 %\n",
            "Time for 20 / 90 6.57141375541687\n",
            "Learning rate:  0.03\n",
            "Epoch: [21][0/60]\tTime 0.227 (0.227)\tLoss (Class) 0.0095 (0.0095)\tLoss (Regu) 0.0093 (0.0093)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [21][50/60]\tTime 0.038 (0.043)\tLoss (Class) 0.0102 (0.0129)\tLoss (Regu) 0.0100 (0.0127)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.184 (0.184)\tLoss (Class) 0.0111 (0.0111)\tLoss (Regu) 0.0109 (0.0109)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.013 (0.019)\tLoss (Class) 0.0100 (0.0320)\tLoss (Regu) 0.0098 (0.0318)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.011 (0.018)\tLoss (Class) 0.0155 (0.0321)\tLoss (Regu) 0.0153 (0.0320)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.013 (0.017)\tLoss (Class) 0.0462 (0.0310)\tLoss (Regu) 0.0460 (0.0308)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.012 (0.017)\tLoss (Class) 0.0112 (0.0312)\tLoss (Regu) 0.0110 (0.0311)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.013 loss] Val [100.000 %, 100.000%, 0.031 loss] Best: 100.000 %\n",
            "Time for 21 / 90 6.543099403381348\n",
            "Learning rate:  0.03\n",
            "Epoch: [22][0/60]\tTime 0.221 (0.221)\tLoss (Class) 0.0104 (0.0104)\tLoss (Regu) 0.0103 (0.0103)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [22][50/60]\tTime 0.040 (0.042)\tLoss (Class) 0.0084 (0.0110)\tLoss (Regu) 0.0082 (0.0108)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.177 (0.177)\tLoss (Class) 0.1303 (0.1303)\tLoss (Regu) 0.1302 (0.1302)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.012 (0.019)\tLoss (Class) 0.0221 (0.0306)\tLoss (Regu) 0.0219 (0.0304)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.011 (0.018)\tLoss (Class) 0.0131 (0.0290)\tLoss (Regu) 0.0129 (0.0289)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.012 (0.018)\tLoss (Class) 0.0131 (0.0306)\tLoss (Regu) 0.0130 (0.0304)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.013 (0.018)\tLoss (Class) 0.0618 (0.0315)\tLoss (Regu) 0.0616 (0.0313)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.011 loss] Val [100.000 %, 100.000%, 0.032 loss] Best: 100.000 %\n",
            "Time for 22 / 90 6.715352296829224\n",
            "Learning rate:  0.03\n",
            "Epoch: [23][0/60]\tTime 0.219 (0.219)\tLoss (Class) 0.0097 (0.0097)\tLoss (Regu) 0.0096 (0.0096)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [23][50/60]\tTime 0.040 (0.044)\tLoss (Class) 0.0113 (0.0111)\tLoss (Regu) 0.0111 (0.0109)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.175 (0.175)\tLoss (Class) 0.0599 (0.0599)\tLoss (Regu) 0.0598 (0.0598)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.012 (0.020)\tLoss (Class) 0.0243 (0.0285)\tLoss (Regu) 0.0242 (0.0283)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.013 (0.018)\tLoss (Class) 0.0138 (0.0330)\tLoss (Regu) 0.0136 (0.0329)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.011 (0.017)\tLoss (Class) 0.0280 (0.0340)\tLoss (Regu) 0.0278 (0.0338)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.018 (0.017)\tLoss (Class) 0.0785 (0.0361)\tLoss (Regu) 0.0784 (0.0360)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.011 loss] Val [100.000 %, 100.000%, 0.036 loss] Best: 100.000 %\n",
            "Time for 23 / 90 6.722112655639648\n",
            "Learning rate:  0.03\n",
            "Epoch: [24][0/60]\tTime 0.207 (0.207)\tLoss (Class) 0.0153 (0.0153)\tLoss (Regu) 0.0152 (0.0152)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [24][50/60]\tTime 0.038 (0.041)\tLoss (Class) 0.0091 (0.0111)\tLoss (Regu) 0.0089 (0.0109)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.179 (0.179)\tLoss (Class) 0.0417 (0.0417)\tLoss (Regu) 0.0415 (0.0415)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.028 (0.020)\tLoss (Class) 0.0123 (0.0207)\tLoss (Regu) 0.0121 (0.0206)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.014 (0.019)\tLoss (Class) 0.0122 (0.0212)\tLoss (Regu) 0.0121 (0.0211)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.026 (0.018)\tLoss (Class) 0.0436 (0.0222)\tLoss (Regu) 0.0435 (0.0220)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.014 (0.018)\tLoss (Class) 0.0447 (0.0222)\tLoss (Regu) 0.0446 (0.0221)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.011 loss] Val [100.000 %, 100.000%, 0.023 loss] Best: 100.000 %\n",
            "Time for 24 / 90 6.608973979949951\n",
            "Learning rate:  0.03\n",
            "Epoch: [25][0/60]\tTime 0.209 (0.209)\tLoss (Class) 0.0158 (0.0158)\tLoss (Regu) 0.0157 (0.0157)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [25][50/60]\tTime 0.039 (0.045)\tLoss (Class) 0.0094 (0.0097)\tLoss (Regu) 0.0093 (0.0095)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.183 (0.183)\tLoss (Class) 0.0082 (0.0082)\tLoss (Regu) 0.0080 (0.0080)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.033 (0.020)\tLoss (Class) 0.0622 (0.0275)\tLoss (Regu) 0.0620 (0.0273)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.014 (0.018)\tLoss (Class) 0.0335 (0.0279)\tLoss (Regu) 0.0334 (0.0278)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.025 (0.017)\tLoss (Class) 0.0594 (0.0308)\tLoss (Regu) 0.0592 (0.0306)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.012 (0.017)\tLoss (Class) 0.0606 (0.0314)\tLoss (Regu) 0.0605 (0.0312)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.010 loss] Val [100.000 %, 100.000%, 0.031 loss] Best: 100.000 %\n",
            "Time for 25 / 90 6.612618684768677\n",
            "Learning rate:  0.03\n",
            "Epoch: [26][0/60]\tTime 0.229 (0.229)\tLoss (Class) 0.0084 (0.0084)\tLoss (Regu) 0.0082 (0.0082)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [26][50/60]\tTime 0.038 (0.045)\tLoss (Class) 0.0071 (0.0089)\tLoss (Regu) 0.0069 (0.0087)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.177 (0.177)\tLoss (Class) 0.0091 (0.0091)\tLoss (Regu) 0.0090 (0.0090)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.040 (0.021)\tLoss (Class) 0.0624 (0.0354)\tLoss (Regu) 0.0622 (0.0352)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.012 (0.019)\tLoss (Class) 0.0086 (0.0317)\tLoss (Regu) 0.0084 (0.0315)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.031 (0.019)\tLoss (Class) 0.0194 (0.0314)\tLoss (Regu) 0.0193 (0.0313)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.012 (0.018)\tLoss (Class) 0.0092 (0.0290)\tLoss (Regu) 0.0091 (0.0289)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.009 loss] Val [100.000 %, 100.000%, 0.030 loss] Best: 100.000 %\n",
            "Time for 26 / 90 6.97979736328125\n",
            "Learning rate:  0.03\n",
            "Epoch: [27][0/60]\tTime 0.220 (0.220)\tLoss (Class) 0.0082 (0.0082)\tLoss (Regu) 0.0081 (0.0081)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [27][50/60]\tTime 0.039 (0.043)\tLoss (Class) 0.0094 (0.0091)\tLoss (Regu) 0.0092 (0.0089)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.181 (0.181)\tLoss (Class) 0.0094 (0.0094)\tLoss (Regu) 0.0093 (0.0093)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.012 (0.020)\tLoss (Class) 0.0215 (0.0339)\tLoss (Regu) 0.0215 (0.0338)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.012 (0.018)\tLoss (Class) 0.0096 (0.0351)\tLoss (Regu) 0.0096 (0.0351)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.015 (0.017)\tLoss (Class) 0.0591 (0.0333)\tLoss (Regu) 0.0590 (0.0333)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.017 (0.017)\tLoss (Class) 0.0195 (0.0311)\tLoss (Regu) 0.0195 (0.0310)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.009 loss] Val [100.000 %, 100.000%, 0.030 loss] Best: 100.000 %\n",
            "Time for 27 / 90 6.521714687347412\n",
            "Learning rate:  0.03\n",
            "Epoch: [28][0/60]\tTime 0.219 (0.219)\tLoss (Class) 0.0087 (0.0087)\tLoss (Regu) 0.0085 (0.0085)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [28][50/60]\tTime 0.039 (0.045)\tLoss (Class) 0.0102 (0.0093)\tLoss (Regu) 0.0100 (0.0092)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.175 (0.175)\tLoss (Class) 0.1262 (0.1262)\tLoss (Regu) 0.1262 (0.1262)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.014 (0.020)\tLoss (Class) 0.0939 (0.0845)\tLoss (Regu) 0.0939 (0.0845)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.024 (0.018)\tLoss (Class) 0.1256 (0.0820)\tLoss (Regu) 0.1256 (0.0820)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.014 (0.018)\tLoss (Class) 0.0677 (0.0803)\tLoss (Regu) 0.0677 (0.0803)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.030 (0.018)\tLoss (Class) 0.0680 (0.0786)\tLoss (Regu) 0.0680 (0.0786)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.009 loss] Val [100.000 %, 100.000%, 0.079 loss] Best: 100.000 %\n",
            "Time for 28 / 90 6.786072015762329\n",
            "Learning rate:  0.03\n",
            "Epoch: [29][0/60]\tTime 0.206 (0.206)\tLoss (Class) 0.0085 (0.0085)\tLoss (Regu) 0.0083 (0.0083)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [29][50/60]\tTime 0.045 (0.043)\tLoss (Class) 0.0071 (0.0081)\tLoss (Regu) 0.0070 (0.0080)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.178 (0.178)\tLoss (Class) 0.0674 (0.0674)\tLoss (Regu) 0.0674 (0.0674)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.012 (0.019)\tLoss (Class) 0.0794 (0.0703)\tLoss (Regu) 0.0794 (0.0703)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.033 (0.018)\tLoss (Class) 0.0677 (0.0708)\tLoss (Regu) 0.0677 (0.0708)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.013 (0.018)\tLoss (Class) 0.0677 (0.0712)\tLoss (Regu) 0.0677 (0.0712)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.016 (0.018)\tLoss (Class) 0.0787 (0.0721)\tLoss (Regu) 0.0787 (0.0721)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.008 loss] Val [100.000 %, 100.000%, 0.072 loss] Best: 100.000 %\n",
            "Time for 29 / 90 6.786953449249268\n",
            "Learning rate:  0.003\n",
            "Epoch: [30][0/60]\tTime 0.215 (0.215)\tLoss (Class) 0.0077 (0.0077)\tLoss (Regu) 0.0075 (0.0075)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [30][50/60]\tTime 0.037 (0.042)\tLoss (Class) 0.0037 (0.0038)\tLoss (Regu) 0.0035 (0.0037)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.172 (0.172)\tLoss (Class) 0.0038 (0.0038)\tLoss (Regu) 0.0037 (0.0037)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.013 (0.020)\tLoss (Class) 0.0442 (0.0229)\tLoss (Regu) 0.0441 (0.0228)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.032 (0.018)\tLoss (Class) 0.0468 (0.0216)\tLoss (Regu) 0.0467 (0.0214)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.012 (0.017)\tLoss (Class) 0.0040 (0.0202)\tLoss (Regu) 0.0038 (0.0200)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.033 (0.017)\tLoss (Class) 0.0417 (0.0231)\tLoss (Regu) 0.0415 (0.0229)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.004 loss] Val [100.000 %, 100.000%, 0.022 loss] Best: 100.000 %\n",
            "Time for 30 / 90 6.462634325027466\n",
            "Learning rate:  0.003\n",
            "Epoch: [31][0/60]\tTime 0.212 (0.212)\tLoss (Class) 0.0047 (0.0047)\tLoss (Regu) 0.0045 (0.0045)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [31][50/60]\tTime 0.043 (0.042)\tLoss (Class) 0.0031 (0.0040)\tLoss (Regu) 0.0029 (0.0039)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.180 (0.180)\tLoss (Class) 0.0447 (0.0447)\tLoss (Regu) 0.0446 (0.0446)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.013 (0.021)\tLoss (Class) 0.0032 (0.0200)\tLoss (Regu) 0.0031 (0.0199)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.039 (0.020)\tLoss (Class) 0.0038 (0.0182)\tLoss (Regu) 0.0037 (0.0180)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.014 (0.019)\tLoss (Class) 0.0877 (0.0205)\tLoss (Regu) 0.0875 (0.0204)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.011 (0.018)\tLoss (Class) 0.0082 (0.0203)\tLoss (Regu) 0.0081 (0.0201)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.004 loss] Val [100.000 %, 100.000%, 0.020 loss] Best: 100.000 %\n",
            "Time for 31 / 90 6.827372074127197\n",
            "Learning rate:  0.003\n",
            "Epoch: [32][0/60]\tTime 0.205 (0.205)\tLoss (Class) 0.0037 (0.0037)\tLoss (Regu) 0.0036 (0.0036)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [32][50/60]\tTime 0.038 (0.042)\tLoss (Class) 0.0037 (0.0039)\tLoss (Regu) 0.0036 (0.0038)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.179 (0.179)\tLoss (Class) 0.0030 (0.0030)\tLoss (Regu) 0.0028 (0.0028)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.012 (0.020)\tLoss (Class) 0.0036 (0.0214)\tLoss (Regu) 0.0034 (0.0213)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.012 (0.018)\tLoss (Class) 0.0359 (0.0182)\tLoss (Regu) 0.0358 (0.0181)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.011 (0.017)\tLoss (Class) 0.0037 (0.0197)\tLoss (Regu) 0.0035 (0.0196)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.012 (0.017)\tLoss (Class) 0.0383 (0.0193)\tLoss (Regu) 0.0382 (0.0191)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.004 loss] Val [100.000 %, 100.000%, 0.019 loss] Best: 100.000 %\n",
            "Time for 32 / 90 6.593191862106323\n",
            "Learning rate:  0.003\n",
            "Epoch: [33][0/60]\tTime 0.228 (0.228)\tLoss (Class) 0.0038 (0.0038)\tLoss (Regu) 0.0037 (0.0037)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [33][50/60]\tTime 0.043 (0.044)\tLoss (Class) 0.0019 (0.0033)\tLoss (Regu) 0.0017 (0.0031)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.184 (0.184)\tLoss (Class) 0.0037 (0.0037)\tLoss (Regu) 0.0036 (0.0036)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.018 (0.020)\tLoss (Class) 0.0037 (0.0122)\tLoss (Regu) 0.0036 (0.0120)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.026 (0.018)\tLoss (Class) 0.0038 (0.0139)\tLoss (Regu) 0.0036 (0.0137)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.019 (0.017)\tLoss (Class) 0.0026 (0.0127)\tLoss (Regu) 0.0025 (0.0126)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.014 (0.017)\tLoss (Class) 0.0037 (0.0134)\tLoss (Regu) 0.0036 (0.0132)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.003 loss] Val [100.000 %, 100.000%, 0.013 loss] Best: 100.000 %\n",
            "Time for 33 / 90 6.63919734954834\n",
            "Learning rate:  0.003\n",
            "Epoch: [34][0/60]\tTime 0.214 (0.214)\tLoss (Class) 0.0020 (0.0020)\tLoss (Regu) 0.0019 (0.0019)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [34][50/60]\tTime 0.038 (0.043)\tLoss (Class) 0.0027 (0.0027)\tLoss (Regu) 0.0026 (0.0026)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.176 (0.176)\tLoss (Class) 0.0038 (0.0038)\tLoss (Regu) 0.0037 (0.0037)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.013 (0.018)\tLoss (Class) 0.0039 (0.0120)\tLoss (Regu) 0.0037 (0.0119)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.012 (0.018)\tLoss (Class) 0.0038 (0.0106)\tLoss (Regu) 0.0037 (0.0105)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.030 (0.017)\tLoss (Class) 0.0301 (0.0114)\tLoss (Regu) 0.0299 (0.0113)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.013 (0.017)\tLoss (Class) 0.0038 (0.0119)\tLoss (Regu) 0.0037 (0.0118)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.003 loss] Val [100.000 %, 100.000%, 0.012 loss] Best: 100.000 %\n",
            "Time for 34 / 90 6.556984186172485\n",
            "Learning rate:  0.003\n",
            "Epoch: [35][0/60]\tTime 0.240 (0.240)\tLoss (Class) 0.0044 (0.0044)\tLoss (Regu) 0.0042 (0.0042)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [35][50/60]\tTime 0.040 (0.045)\tLoss (Class) 0.0020 (0.0028)\tLoss (Regu) 0.0018 (0.0027)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.174 (0.174)\tLoss (Class) 0.0371 (0.0371)\tLoss (Regu) 0.0369 (0.0369)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.017 (0.019)\tLoss (Class) 0.0035 (0.0147)\tLoss (Regu) 0.0033 (0.0146)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.011 (0.018)\tLoss (Class) 0.0370 (0.0157)\tLoss (Regu) 0.0369 (0.0156)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.012 (0.017)\tLoss (Class) 0.0380 (0.0169)\tLoss (Regu) 0.0378 (0.0167)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.011 (0.017)\tLoss (Class) 0.0030 (0.0171)\tLoss (Regu) 0.0029 (0.0170)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.003 loss] Val [100.000 %, 100.000%, 0.017 loss] Best: 100.000 %\n",
            "Time for 35 / 90 6.774310111999512\n",
            "Learning rate:  0.003\n",
            "Epoch: [36][0/60]\tTime 0.219 (0.219)\tLoss (Class) 0.0030 (0.0030)\tLoss (Regu) 0.0029 (0.0029)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [36][50/60]\tTime 0.038 (0.043)\tLoss (Class) 0.0037 (0.0031)\tLoss (Regu) 0.0036 (0.0030)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.175 (0.175)\tLoss (Class) 0.0894 (0.0894)\tLoss (Regu) 0.0893 (0.0893)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.013 (0.018)\tLoss (Class) 0.1731 (0.0255)\tLoss (Regu) 0.1730 (0.0253)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.025 (0.017)\tLoss (Class) 0.0025 (0.0274)\tLoss (Regu) 0.0024 (0.0273)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.019 (0.017)\tLoss (Class) 0.0035 (0.0265)\tLoss (Regu) 0.0034 (0.0263)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.018 (0.016)\tLoss (Class) 0.0039 (0.0270)\tLoss (Regu) 0.0038 (0.0269)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.003 loss] Val [100.000 %, 100.000%, 0.029 loss] Best: 100.000 %\n",
            "Time for 36 / 90 6.5136377811431885\n",
            "Learning rate:  0.003\n",
            "Epoch: [37][0/60]\tTime 0.215 (0.215)\tLoss (Class) 0.0029 (0.0029)\tLoss (Regu) 0.0027 (0.0027)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [37][50/60]\tTime 0.039 (0.044)\tLoss (Class) 0.0041 (0.0031)\tLoss (Regu) 0.0040 (0.0029)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.180 (0.180)\tLoss (Class) 0.0023 (0.0023)\tLoss (Regu) 0.0022 (0.0022)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.012 (0.019)\tLoss (Class) 0.0288 (0.0142)\tLoss (Regu) 0.0287 (0.0141)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.011 (0.018)\tLoss (Class) 0.0037 (0.0125)\tLoss (Regu) 0.0035 (0.0123)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.012 (0.018)\tLoss (Class) 0.0036 (0.0130)\tLoss (Regu) 0.0035 (0.0129)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.013 (0.017)\tLoss (Class) 0.0562 (0.0131)\tLoss (Regu) 0.0561 (0.0130)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.003 loss] Val [100.000 %, 100.000%, 0.013 loss] Best: 100.000 %\n",
            "Time for 37 / 90 6.66576361656189\n",
            "Learning rate:  0.003\n",
            "Epoch: [38][0/60]\tTime 0.208 (0.208)\tLoss (Class) 0.0021 (0.0021)\tLoss (Regu) 0.0020 (0.0020)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [38][50/60]\tTime 0.038 (0.042)\tLoss (Class) 0.0039 (0.0029)\tLoss (Regu) 0.0037 (0.0028)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.174 (0.174)\tLoss (Class) 0.0036 (0.0036)\tLoss (Regu) 0.0034 (0.0034)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.015 (0.019)\tLoss (Class) 0.0287 (0.0128)\tLoss (Regu) 0.0285 (0.0126)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.011 (0.017)\tLoss (Class) 0.0035 (0.0125)\tLoss (Regu) 0.0033 (0.0124)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.017 (0.017)\tLoss (Class) 0.0036 (0.0118)\tLoss (Regu) 0.0035 (0.0117)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.021 (0.017)\tLoss (Class) 0.0030 (0.0118)\tLoss (Regu) 0.0028 (0.0116)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.003 loss] Val [100.000 %, 100.000%, 0.012 loss] Best: 100.000 %\n",
            "Time for 38 / 90 6.513542890548706\n",
            "Learning rate:  0.003\n",
            "Epoch: [39][0/60]\tTime 0.212 (0.212)\tLoss (Class) 0.0033 (0.0033)\tLoss (Regu) 0.0032 (0.0032)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [39][50/60]\tTime 0.038 (0.043)\tLoss (Class) 0.0037 (0.0029)\tLoss (Regu) 0.0036 (0.0028)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.174 (0.174)\tLoss (Class) 0.0033 (0.0033)\tLoss (Regu) 0.0032 (0.0032)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.025 (0.019)\tLoss (Class) 0.0363 (0.0129)\tLoss (Regu) 0.0362 (0.0128)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.012 (0.018)\tLoss (Class) 0.0080 (0.0133)\tLoss (Regu) 0.0078 (0.0132)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.012 (0.017)\tLoss (Class) 0.0029 (0.0167)\tLoss (Regu) 0.0028 (0.0165)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.013 (0.017)\tLoss (Class) 0.0037 (0.0159)\tLoss (Regu) 0.0036 (0.0157)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.003 loss] Val [100.000 %, 100.000%, 0.015 loss] Best: 100.000 %\n",
            "Time for 39 / 90 6.496572732925415\n",
            "Learning rate:  0.003\n",
            "Epoch: [40][0/60]\tTime 0.213 (0.213)\tLoss (Class) 0.0029 (0.0029)\tLoss (Regu) 0.0028 (0.0028)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [40][50/60]\tTime 0.038 (0.045)\tLoss (Class) 0.0029 (0.0037)\tLoss (Regu) 0.0027 (0.0036)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.182 (0.182)\tLoss (Class) 0.0252 (0.0252)\tLoss (Regu) 0.0251 (0.0251)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.011 (0.021)\tLoss (Class) 0.0040 (0.0133)\tLoss (Regu) 0.0039 (0.0132)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.017 (0.018)\tLoss (Class) 0.0034 (0.0143)\tLoss (Regu) 0.0033 (0.0142)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.011 (0.017)\tLoss (Class) 0.0040 (0.0142)\tLoss (Regu) 0.0039 (0.0141)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.012 (0.017)\tLoss (Class) 0.0041 (0.0134)\tLoss (Regu) 0.0039 (0.0133)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.004 loss] Val [100.000 %, 100.000%, 0.013 loss] Best: 100.000 %\n",
            "Time for 40 / 90 6.682716369628906\n",
            "Learning rate:  0.003\n",
            "Epoch: [41][0/60]\tTime 0.218 (0.218)\tLoss (Class) 0.0034 (0.0034)\tLoss (Regu) 0.0033 (0.0033)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [41][50/60]\tTime 0.038 (0.042)\tLoss (Class) 0.0037 (0.0036)\tLoss (Regu) 0.0035 (0.0035)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.191 (0.191)\tLoss (Class) 0.0030 (0.0030)\tLoss (Regu) 0.0029 (0.0029)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.015 (0.020)\tLoss (Class) 0.0039 (0.0141)\tLoss (Regu) 0.0037 (0.0140)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.017 (0.017)\tLoss (Class) 0.0341 (0.0129)\tLoss (Regu) 0.0340 (0.0127)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.013 (0.017)\tLoss (Class) 0.0039 (0.0131)\tLoss (Regu) 0.0037 (0.0130)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.025 (0.016)\tLoss (Class) 0.0034 (0.0127)\tLoss (Regu) 0.0033 (0.0126)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.004 loss] Val [100.000 %, 100.000%, 0.013 loss] Best: 100.000 %\n",
            "Time for 41 / 90 6.387138366699219\n",
            "Learning rate:  0.003\n",
            "Epoch: [42][0/60]\tTime 0.218 (0.218)\tLoss (Class) 0.0029 (0.0029)\tLoss (Regu) 0.0028 (0.0028)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [42][50/60]\tTime 0.038 (0.044)\tLoss (Class) 0.0037 (0.0033)\tLoss (Regu) 0.0036 (0.0032)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.177 (0.177)\tLoss (Class) 0.0031 (0.0031)\tLoss (Regu) 0.0030 (0.0030)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.017 (0.018)\tLoss (Class) 0.0031 (0.0123)\tLoss (Regu) 0.0030 (0.0122)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.028 (0.017)\tLoss (Class) 0.0504 (0.0108)\tLoss (Regu) 0.0503 (0.0107)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.015 (0.017)\tLoss (Class) 0.0031 (0.0109)\tLoss (Regu) 0.0030 (0.0108)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.021 (0.016)\tLoss (Class) 0.0023 (0.0115)\tLoss (Regu) 0.0022 (0.0114)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.003 loss] Val [100.000 %, 100.000%, 0.012 loss] Best: 100.000 %\n",
            "Time for 42 / 90 6.465336322784424\n",
            "Learning rate:  0.003\n",
            "Epoch: [43][0/60]\tTime 0.219 (0.219)\tLoss (Class) 0.0029 (0.0029)\tLoss (Regu) 0.0027 (0.0027)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [43][50/60]\tTime 0.038 (0.042)\tLoss (Class) 0.0033 (0.0028)\tLoss (Regu) 0.0032 (0.0027)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.179 (0.179)\tLoss (Class) 0.0023 (0.0023)\tLoss (Regu) 0.0022 (0.0022)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.021 (0.018)\tLoss (Class) 0.0036 (0.0114)\tLoss (Regu) 0.0035 (0.0112)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.012 (0.018)\tLoss (Class) 0.0028 (0.0108)\tLoss (Regu) 0.0027 (0.0107)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.025 (0.017)\tLoss (Class) 0.0037 (0.0112)\tLoss (Regu) 0.0035 (0.0111)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.014 (0.017)\tLoss (Class) 0.0029 (0.0116)\tLoss (Regu) 0.0027 (0.0114)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.003 loss] Val [100.000 %, 100.000%, 0.012 loss] Best: 100.000 %\n",
            "Time for 43 / 90 6.563913583755493\n",
            "Learning rate:  0.003\n",
            "Epoch: [44][0/60]\tTime 0.222 (0.222)\tLoss (Class) 0.0016 (0.0016)\tLoss (Regu) 0.0015 (0.0015)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [44][50/60]\tTime 0.038 (0.042)\tLoss (Class) 0.0037 (0.0027)\tLoss (Regu) 0.0035 (0.0026)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.177 (0.177)\tLoss (Class) 0.0822 (0.0822)\tLoss (Regu) 0.0820 (0.0820)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.011 (0.020)\tLoss (Class) 0.0020 (0.0213)\tLoss (Regu) 0.0019 (0.0212)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.030 (0.019)\tLoss (Class) 0.0022 (0.0207)\tLoss (Regu) 0.0020 (0.0206)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.011 (0.018)\tLoss (Class) 0.0033 (0.0189)\tLoss (Regu) 0.0031 (0.0188)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.030 (0.018)\tLoss (Class) 0.0454 (0.0189)\tLoss (Regu) 0.0453 (0.0188)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.003 loss] Val [100.000 %, 100.000%, 0.018 loss] Best: 100.000 %\n",
            "Time for 44 / 90 6.743533611297607\n",
            "Learning rate:  0.003\n",
            "Epoch: [45][0/60]\tTime 0.231 (0.231)\tLoss (Class) 0.0022 (0.0022)\tLoss (Regu) 0.0021 (0.0021)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [45][50/60]\tTime 0.038 (0.042)\tLoss (Class) 0.0031 (0.0025)\tLoss (Regu) 0.0030 (0.0024)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.175 (0.175)\tLoss (Class) 0.0034 (0.0034)\tLoss (Regu) 0.0033 (0.0033)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.021 (0.019)\tLoss (Class) 0.0035 (0.0111)\tLoss (Regu) 0.0033 (0.0110)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.012 (0.018)\tLoss (Class) 0.0034 (0.0103)\tLoss (Regu) 0.0033 (0.0102)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.013 (0.018)\tLoss (Class) 0.0034 (0.0111)\tLoss (Regu) 0.0033 (0.0110)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.025 (0.017)\tLoss (Class) 0.0030 (0.0114)\tLoss (Regu) 0.0028 (0.0113)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.003 loss] Val [100.000 %, 100.000%, 0.011 loss] Best: 100.000 %\n",
            "Time for 45 / 90 6.603027582168579\n",
            "Learning rate:  0.003\n",
            "Epoch: [46][0/60]\tTime 0.223 (0.223)\tLoss (Class) 0.0049 (0.0049)\tLoss (Regu) 0.0048 (0.0048)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [46][50/60]\tTime 0.044 (0.044)\tLoss (Class) 0.0025 (0.0028)\tLoss (Regu) 0.0024 (0.0027)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.173 (0.173)\tLoss (Class) 0.0020 (0.0020)\tLoss (Regu) 0.0019 (0.0019)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.033 (0.021)\tLoss (Class) 0.0035 (0.0078)\tLoss (Regu) 0.0034 (0.0077)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.014 (0.019)\tLoss (Class) 0.0035 (0.0090)\tLoss (Regu) 0.0034 (0.0089)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.020 (0.017)\tLoss (Class) 0.0225 (0.0099)\tLoss (Regu) 0.0224 (0.0097)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.011 (0.017)\tLoss (Class) 0.0222 (0.0103)\tLoss (Regu) 0.0221 (0.0102)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.003 loss] Val [100.000 %, 100.000%, 0.010 loss] Best: 100.000 %\n",
            "Time for 46 / 90 6.626769304275513\n",
            "Learning rate:  0.003\n",
            "Epoch: [47][0/60]\tTime 0.218 (0.218)\tLoss (Class) 0.0022 (0.0022)\tLoss (Regu) 0.0021 (0.0021)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [47][50/60]\tTime 0.038 (0.042)\tLoss (Class) 0.0024 (0.0025)\tLoss (Regu) 0.0023 (0.0024)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.189 (0.189)\tLoss (Class) 0.0026 (0.0026)\tLoss (Regu) 0.0025 (0.0025)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.030 (0.021)\tLoss (Class) 0.0023 (0.0141)\tLoss (Regu) 0.0022 (0.0139)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.013 (0.019)\tLoss (Class) 0.0023 (0.0155)\tLoss (Regu) 0.0021 (0.0154)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.027 (0.018)\tLoss (Class) 0.0404 (0.0159)\tLoss (Regu) 0.0403 (0.0158)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.013 (0.018)\tLoss (Class) 0.0030 (0.0166)\tLoss (Regu) 0.0029 (0.0164)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.003 loss] Val [100.000 %, 100.000%, 0.017 loss] Best: 100.000 %\n",
            "Time for 47 / 90 6.78593635559082\n",
            "Learning rate:  0.003\n",
            "Epoch: [48][0/60]\tTime 0.214 (0.214)\tLoss (Class) 0.0019 (0.0019)\tLoss (Regu) 0.0018 (0.0018)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [48][50/60]\tTime 0.041 (0.043)\tLoss (Class) 0.0022 (0.0023)\tLoss (Regu) 0.0021 (0.0022)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.176 (0.176)\tLoss (Class) 0.0133 (0.0133)\tLoss (Regu) 0.0132 (0.0132)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.024 (0.019)\tLoss (Class) 0.0026 (0.0078)\tLoss (Regu) 0.0025 (0.0076)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.013 (0.018)\tLoss (Class) 0.0326 (0.0086)\tLoss (Regu) 0.0325 (0.0085)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.013 (0.017)\tLoss (Class) 0.0026 (0.0086)\tLoss (Regu) 0.0025 (0.0084)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.011 (0.017)\tLoss (Class) 0.0030 (0.0085)\tLoss (Regu) 0.0029 (0.0083)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.002 loss] Val [100.000 %, 100.000%, 0.008 loss] Best: 100.000 %\n",
            "Time for 48 / 90 6.488559722900391\n",
            "Learning rate:  0.003\n",
            "Epoch: [49][0/60]\tTime 0.217 (0.217)\tLoss (Class) 0.0024 (0.0024)\tLoss (Regu) 0.0023 (0.0023)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [49][50/60]\tTime 0.042 (0.044)\tLoss (Class) 0.0062 (0.0026)\tLoss (Regu) 0.0061 (0.0025)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.189 (0.189)\tLoss (Class) 0.0020 (0.0020)\tLoss (Regu) 0.0019 (0.0019)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.011 (0.020)\tLoss (Class) 0.0030 (0.0068)\tLoss (Regu) 0.0028 (0.0067)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.033 (0.019)\tLoss (Class) 0.0265 (0.0094)\tLoss (Regu) 0.0263 (0.0093)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.015 (0.018)\tLoss (Class) 0.0253 (0.0107)\tLoss (Regu) 0.0252 (0.0106)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.025 (0.018)\tLoss (Class) 0.0028 (0.0114)\tLoss (Regu) 0.0027 (0.0112)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.003 loss] Val [100.000 %, 100.000%, 0.011 loss] Best: 100.000 %\n",
            "Time for 49 / 90 6.8117523193359375\n",
            "Learning rate:  0.003\n",
            "Epoch: [50][0/60]\tTime 0.218 (0.218)\tLoss (Class) 0.0032 (0.0032)\tLoss (Regu) 0.0031 (0.0031)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [50][50/60]\tTime 0.044 (0.044)\tLoss (Class) 0.0023 (0.0027)\tLoss (Regu) 0.0021 (0.0025)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.179 (0.179)\tLoss (Class) 0.0020 (0.0020)\tLoss (Regu) 0.0019 (0.0019)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.013 (0.019)\tLoss (Class) 0.0028 (0.0133)\tLoss (Regu) 0.0027 (0.0131)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.037 (0.018)\tLoss (Class) 0.0021 (0.0140)\tLoss (Regu) 0.0019 (0.0138)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.011 (0.017)\tLoss (Class) 0.0022 (0.0126)\tLoss (Regu) 0.0021 (0.0125)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.019 (0.017)\tLoss (Class) 0.0270 (0.0129)\tLoss (Regu) 0.0268 (0.0128)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.003 loss] Val [100.000 %, 100.000%, 0.013 loss] Best: 100.000 %\n",
            "Time for 50 / 90 6.6537024974823\n",
            "Learning rate:  0.003\n",
            "Epoch: [51][0/60]\tTime 0.223 (0.223)\tLoss (Class) 0.0030 (0.0030)\tLoss (Regu) 0.0029 (0.0029)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [51][50/60]\tTime 0.038 (0.044)\tLoss (Class) 0.0024 (0.0027)\tLoss (Regu) 0.0023 (0.0025)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.179 (0.179)\tLoss (Class) 0.0030 (0.0030)\tLoss (Regu) 0.0028 (0.0028)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.011 (0.019)\tLoss (Class) 0.0029 (0.0098)\tLoss (Regu) 0.0028 (0.0097)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.027 (0.017)\tLoss (Class) 0.0544 (0.0112)\tLoss (Regu) 0.0543 (0.0111)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.033 (0.017)\tLoss (Class) 0.0030 (0.0106)\tLoss (Regu) 0.0028 (0.0105)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.013 (0.017)\tLoss (Class) 0.0022 (0.0116)\tLoss (Regu) 0.0021 (0.0115)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.003 loss] Val [100.000 %, 100.000%, 0.012 loss] Best: 100.000 %\n",
            "Time for 51 / 90 6.5588531494140625\n",
            "Learning rate:  0.003\n",
            "Epoch: [52][0/60]\tTime 0.205 (0.205)\tLoss (Class) 0.0022 (0.0022)\tLoss (Regu) 0.0021 (0.0021)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [52][50/60]\tTime 0.039 (0.042)\tLoss (Class) 0.0018 (0.0023)\tLoss (Regu) 0.0017 (0.0022)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.175 (0.175)\tLoss (Class) 0.0020 (0.0020)\tLoss (Regu) 0.0019 (0.0019)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.015 (0.019)\tLoss (Class) 0.0031 (0.0100)\tLoss (Regu) 0.0029 (0.0099)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.013 (0.018)\tLoss (Class) 0.0227 (0.0096)\tLoss (Regu) 0.0226 (0.0095)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.022 (0.017)\tLoss (Class) 0.0031 (0.0105)\tLoss (Regu) 0.0030 (0.0103)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.014 (0.017)\tLoss (Class) 0.0031 (0.0104)\tLoss (Regu) 0.0030 (0.0103)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.002 loss] Val [100.000 %, 100.000%, 0.010 loss] Best: 100.000 %\n",
            "Time for 52 / 90 6.6156816482543945\n",
            "Learning rate:  0.003\n",
            "Epoch: [53][0/60]\tTime 0.228 (0.228)\tLoss (Class) 0.0017 (0.0017)\tLoss (Regu) 0.0016 (0.0016)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [53][50/60]\tTime 0.038 (0.042)\tLoss (Class) 0.0022 (0.0024)\tLoss (Regu) 0.0021 (0.0023)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.178 (0.178)\tLoss (Class) 0.0029 (0.0029)\tLoss (Regu) 0.0028 (0.0028)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.024 (0.018)\tLoss (Class) 0.0279 (0.0107)\tLoss (Regu) 0.0278 (0.0105)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.012 (0.017)\tLoss (Class) 0.0252 (0.0095)\tLoss (Regu) 0.0251 (0.0094)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.035 (0.017)\tLoss (Class) 0.0029 (0.0103)\tLoss (Regu) 0.0028 (0.0101)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.012 (0.017)\tLoss (Class) 0.0546 (0.0105)\tLoss (Regu) 0.0545 (0.0104)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.002 loss] Val [100.000 %, 100.000%, 0.011 loss] Best: 100.000 %\n",
            "Time for 53 / 90 6.474985361099243\n",
            "Learning rate:  0.003\n",
            "Epoch: [54][0/60]\tTime 0.209 (0.209)\tLoss (Class) 0.0023 (0.0023)\tLoss (Regu) 0.0022 (0.0022)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [54][50/60]\tTime 0.039 (0.042)\tLoss (Class) 0.0020 (0.0023)\tLoss (Regu) 0.0018 (0.0022)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.181 (0.181)\tLoss (Class) 0.0027 (0.0027)\tLoss (Regu) 0.0025 (0.0025)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.011 (0.020)\tLoss (Class) 0.0018 (0.0142)\tLoss (Regu) 0.0017 (0.0141)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.017 (0.019)\tLoss (Class) 0.0022 (0.0114)\tLoss (Regu) 0.0020 (0.0113)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.025 (0.018)\tLoss (Class) 0.0206 (0.0110)\tLoss (Regu) 0.0205 (0.0109)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.011 (0.017)\tLoss (Class) 0.0262 (0.0108)\tLoss (Regu) 0.0261 (0.0107)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.002 loss] Val [100.000 %, 100.000%, 0.011 loss] Best: 100.000 %\n",
            "Time for 54 / 90 6.588645696640015\n",
            "Learning rate:  0.003\n",
            "Epoch: [55][0/60]\tTime 0.208 (0.208)\tLoss (Class) 0.0019 (0.0019)\tLoss (Regu) 0.0018 (0.0018)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [55][50/60]\tTime 0.040 (0.042)\tLoss (Class) 0.0041 (0.0024)\tLoss (Regu) 0.0040 (0.0022)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.172 (0.172)\tLoss (Class) 0.0030 (0.0030)\tLoss (Regu) 0.0029 (0.0029)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.011 (0.019)\tLoss (Class) 0.0032 (0.0057)\tLoss (Regu) 0.0031 (0.0056)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.011 (0.017)\tLoss (Class) 0.0032 (0.0058)\tLoss (Regu) 0.0031 (0.0056)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.018 (0.017)\tLoss (Class) 0.0032 (0.0066)\tLoss (Regu) 0.0031 (0.0064)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.011 (0.016)\tLoss (Class) 0.0032 (0.0069)\tLoss (Regu) 0.0031 (0.0068)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.002 loss] Val [100.000 %, 100.000%, 0.007 loss] Best: 100.000 %\n",
            "Time for 55 / 90 6.422220706939697\n",
            "Learning rate:  0.003\n",
            "Epoch: [56][0/60]\tTime 0.223 (0.223)\tLoss (Class) 0.0033 (0.0033)\tLoss (Regu) 0.0032 (0.0032)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [56][50/60]\tTime 0.038 (0.045)\tLoss (Class) 0.0059 (0.0027)\tLoss (Regu) 0.0057 (0.0025)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.189 (0.189)\tLoss (Class) 0.0191 (0.0191)\tLoss (Regu) 0.0190 (0.0190)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.012 (0.020)\tLoss (Class) 0.0029 (0.0091)\tLoss (Regu) 0.0028 (0.0090)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.048 (0.019)\tLoss (Class) 0.0029 (0.0093)\tLoss (Regu) 0.0028 (0.0091)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.011 (0.018)\tLoss (Class) 0.0207 (0.0088)\tLoss (Regu) 0.0206 (0.0087)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.012 (0.017)\tLoss (Class) 0.0218 (0.0092)\tLoss (Regu) 0.0217 (0.0091)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.003 loss] Val [100.000 %, 100.000%, 0.009 loss] Best: 100.000 %\n",
            "Time for 56 / 90 6.788622140884399\n",
            "Learning rate:  0.003\n",
            "Epoch: [57][0/60]\tTime 0.230 (0.230)\tLoss (Class) 0.0029 (0.0029)\tLoss (Regu) 0.0028 (0.0028)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [57][50/60]\tTime 0.038 (0.043)\tLoss (Class) 0.0027 (0.0030)\tLoss (Regu) 0.0026 (0.0029)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.178 (0.178)\tLoss (Class) 0.0031 (0.0031)\tLoss (Regu) 0.0030 (0.0030)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.015 (0.019)\tLoss (Class) 0.0031 (0.0084)\tLoss (Regu) 0.0030 (0.0083)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.026 (0.017)\tLoss (Class) 0.0031 (0.0087)\tLoss (Regu) 0.0030 (0.0085)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.011 (0.016)\tLoss (Class) 0.0229 (0.0083)\tLoss (Regu) 0.0228 (0.0081)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.025 (0.016)\tLoss (Class) 0.0031 (0.0086)\tLoss (Regu) 0.0029 (0.0085)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.003 loss] Val [100.000 %, 100.000%, 0.008 loss] Best: 100.000 %\n",
            "Time for 57 / 90 6.355337619781494\n",
            "Learning rate:  0.003\n",
            "Epoch: [58][0/60]\tTime 0.223 (0.223)\tLoss (Class) 0.0022 (0.0022)\tLoss (Regu) 0.0020 (0.0020)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [58][50/60]\tTime 0.041 (0.043)\tLoss (Class) 0.0022 (0.0025)\tLoss (Regu) 0.0021 (0.0024)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.175 (0.175)\tLoss (Class) 0.0029 (0.0029)\tLoss (Regu) 0.0027 (0.0027)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.012 (0.020)\tLoss (Class) 0.0023 (0.0124)\tLoss (Regu) 0.0022 (0.0123)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.028 (0.017)\tLoss (Class) 0.0020 (0.0107)\tLoss (Regu) 0.0019 (0.0105)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.011 (0.017)\tLoss (Class) 0.0588 (0.0121)\tLoss (Regu) 0.0587 (0.0119)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.028 (0.016)\tLoss (Class) 0.0029 (0.0125)\tLoss (Regu) 0.0028 (0.0124)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.002 loss] Val [100.000 %, 100.000%, 0.012 loss] Best: 100.000 %\n",
            "Time for 58 / 90 6.464010000228882\n",
            "Learning rate:  0.003\n",
            "Epoch: [59][0/60]\tTime 0.229 (0.229)\tLoss (Class) 0.0018 (0.0018)\tLoss (Regu) 0.0017 (0.0017)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [59][50/60]\tTime 0.037 (0.043)\tLoss (Class) 0.0019 (0.0021)\tLoss (Regu) 0.0018 (0.0019)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.188 (0.188)\tLoss (Class) 0.0022 (0.0022)\tLoss (Regu) 0.0021 (0.0021)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.025 (0.019)\tLoss (Class) 0.0022 (0.0173)\tLoss (Regu) 0.0021 (0.0172)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.014 (0.017)\tLoss (Class) 0.0284 (0.0140)\tLoss (Regu) 0.0283 (0.0138)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.013 (0.017)\tLoss (Class) 0.0286 (0.0141)\tLoss (Regu) 0.0285 (0.0139)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.011 (0.017)\tLoss (Class) 0.0045 (0.0141)\tLoss (Regu) 0.0044 (0.0140)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.002 loss] Val [100.000 %, 100.000%, 0.014 loss] Best: 100.000 %\n",
            "Time for 59 / 90 6.652143955230713\n",
            "Learning rate:  0.00030000000000000003\n",
            "Epoch: [60][0/60]\tTime 0.211 (0.211)\tLoss (Class) 0.0029 (0.0029)\tLoss (Regu) 0.0028 (0.0028)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [60][50/60]\tTime 0.041 (0.042)\tLoss (Class) 0.0032 (0.0018)\tLoss (Regu) 0.0031 (0.0017)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.171 (0.171)\tLoss (Class) 0.0217 (0.0217)\tLoss (Regu) 0.0216 (0.0216)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.012 (0.019)\tLoss (Class) 0.0021 (0.0115)\tLoss (Regu) 0.0020 (0.0114)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.019 (0.017)\tLoss (Class) 0.0015 (0.0110)\tLoss (Regu) 0.0014 (0.0109)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.013 (0.017)\tLoss (Class) 0.0021 (0.0100)\tLoss (Regu) 0.0020 (0.0099)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.021 (0.017)\tLoss (Class) 0.0230 (0.0099)\tLoss (Regu) 0.0229 (0.0098)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.002 loss] Val [100.000 %, 100.000%, 0.010 loss] Best: 100.000 %\n",
            "Time for 60 / 90 6.4382078647613525\n",
            "Learning rate:  0.00030000000000000003\n",
            "Epoch: [61][0/60]\tTime 0.224 (0.224)\tLoss (Class) 0.0024 (0.0024)\tLoss (Regu) 0.0023 (0.0023)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [61][50/60]\tTime 0.038 (0.045)\tLoss (Class) 0.0018 (0.0017)\tLoss (Regu) 0.0017 (0.0015)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.181 (0.181)\tLoss (Class) 0.0022 (0.0022)\tLoss (Regu) 0.0020 (0.0020)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.011 (0.019)\tLoss (Class) 0.0022 (0.0070)\tLoss (Regu) 0.0020 (0.0069)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.012 (0.019)\tLoss (Class) 0.0210 (0.0073)\tLoss (Regu) 0.0209 (0.0072)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.014 (0.019)\tLoss (Class) 0.0207 (0.0080)\tLoss (Regu) 0.0206 (0.0079)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.012 (0.018)\tLoss (Class) 0.0022 (0.0082)\tLoss (Regu) 0.0020 (0.0081)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.002 loss] Val [100.000 %, 100.000%, 0.008 loss] Best: 100.000 %\n",
            "Time for 61 / 90 6.953862905502319\n",
            "Learning rate:  0.00030000000000000003\n",
            "Epoch: [62][0/60]\tTime 0.213 (0.213)\tLoss (Class) 0.0018 (0.0018)\tLoss (Regu) 0.0016 (0.0016)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [62][50/60]\tTime 0.040 (0.044)\tLoss (Class) 0.0012 (0.0016)\tLoss (Regu) 0.0011 (0.0015)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.177 (0.177)\tLoss (Class) 0.0033 (0.0033)\tLoss (Regu) 0.0031 (0.0031)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.013 (0.019)\tLoss (Class) 0.0013 (0.0102)\tLoss (Regu) 0.0011 (0.0101)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.026 (0.017)\tLoss (Class) 0.0022 (0.0097)\tLoss (Regu) 0.0020 (0.0095)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.019 (0.016)\tLoss (Class) 0.0022 (0.0098)\tLoss (Regu) 0.0021 (0.0097)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.027 (0.016)\tLoss (Class) 0.0022 (0.0095)\tLoss (Regu) 0.0020 (0.0094)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.002 loss] Val [100.000 %, 100.000%, 0.010 loss] Best: 100.000 %\n",
            "Time for 62 / 90 6.418159246444702\n",
            "Learning rate:  0.00030000000000000003\n",
            "Epoch: [63][0/60]\tTime 0.219 (0.219)\tLoss (Class) 0.0033 (0.0033)\tLoss (Regu) 0.0032 (0.0032)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [63][50/60]\tTime 0.038 (0.042)\tLoss (Class) 0.0025 (0.0017)\tLoss (Regu) 0.0024 (0.0016)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.195 (0.195)\tLoss (Class) 0.0021 (0.0021)\tLoss (Regu) 0.0020 (0.0020)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.019 (0.021)\tLoss (Class) 0.0020 (0.0080)\tLoss (Regu) 0.0018 (0.0079)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.012 (0.019)\tLoss (Class) 0.0181 (0.0082)\tLoss (Regu) 0.0179 (0.0081)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.022 (0.018)\tLoss (Class) 0.0177 (0.0076)\tLoss (Regu) 0.0176 (0.0075)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.013 (0.017)\tLoss (Class) 0.0021 (0.0073)\tLoss (Regu) 0.0020 (0.0071)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.002 loss] Val [100.000 %, 100.000%, 0.008 loss] Best: 100.000 %\n",
            "Time for 63 / 90 6.62452507019043\n",
            "Learning rate:  0.00030000000000000003\n",
            "Epoch: [64][0/60]\tTime 0.228 (0.228)\tLoss (Class) 0.0008 (0.0008)\tLoss (Regu) 0.0007 (0.0007)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [64][50/60]\tTime 0.046 (0.044)\tLoss (Class) 0.0014 (0.0017)\tLoss (Regu) 0.0013 (0.0015)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.188 (0.188)\tLoss (Class) 0.0021 (0.0021)\tLoss (Regu) 0.0020 (0.0020)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.011 (0.018)\tLoss (Class) 0.0016 (0.0121)\tLoss (Regu) 0.0015 (0.0120)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.027 (0.017)\tLoss (Class) 0.0276 (0.0116)\tLoss (Regu) 0.0275 (0.0114)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.011 (0.017)\tLoss (Class) 0.0019 (0.0147)\tLoss (Regu) 0.0018 (0.0146)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.012 (0.017)\tLoss (Class) 0.0657 (0.0150)\tLoss (Regu) 0.0656 (0.0149)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.002 loss] Val [100.000 %, 100.000%, 0.014 loss] Best: 100.000 %\n",
            "Time for 64 / 90 6.58243203163147\n",
            "Learning rate:  0.00030000000000000003\n",
            "Epoch: [65][0/60]\tTime 0.227 (0.227)\tLoss (Class) 0.0008 (0.0008)\tLoss (Regu) 0.0007 (0.0007)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [65][50/60]\tTime 0.039 (0.044)\tLoss (Class) 0.0018 (0.0017)\tLoss (Regu) 0.0017 (0.0016)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.173 (0.173)\tLoss (Class) 0.0022 (0.0022)\tLoss (Regu) 0.0020 (0.0020)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.017 (0.019)\tLoss (Class) 0.0021 (0.0134)\tLoss (Regu) 0.0020 (0.0132)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.012 (0.018)\tLoss (Class) 0.0291 (0.0118)\tLoss (Regu) 0.0290 (0.0117)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.020 (0.017)\tLoss (Class) 0.0022 (0.0120)\tLoss (Regu) 0.0020 (0.0119)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.017 (0.017)\tLoss (Class) 0.0021 (0.0124)\tLoss (Regu) 0.0020 (0.0122)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.002 loss] Val [100.000 %, 100.000%, 0.012 loss] Best: 100.000 %\n",
            "Time for 65 / 90 6.659722566604614\n",
            "Learning rate:  0.00030000000000000003\n",
            "Epoch: [66][0/60]\tTime 0.211 (0.211)\tLoss (Class) 0.0011 (0.0011)\tLoss (Regu) 0.0010 (0.0010)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [66][50/60]\tTime 0.037 (0.042)\tLoss (Class) 0.0018 (0.0016)\tLoss (Regu) 0.0017 (0.0015)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.181 (0.181)\tLoss (Class) 0.0018 (0.0018)\tLoss (Regu) 0.0017 (0.0017)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.012 (0.019)\tLoss (Class) 0.0022 (0.0105)\tLoss (Regu) 0.0021 (0.0104)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.023 (0.018)\tLoss (Class) 0.0258 (0.0101)\tLoss (Regu) 0.0256 (0.0100)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.012 (0.018)\tLoss (Class) 0.0015 (0.0122)\tLoss (Regu) 0.0014 (0.0121)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.020 (0.018)\tLoss (Class) 0.0541 (0.0132)\tLoss (Regu) 0.0540 (0.0131)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.002 loss] Val [100.000 %, 100.000%, 0.013 loss] Best: 100.000 %\n",
            "Time for 66 / 90 6.761073350906372\n",
            "Learning rate:  0.00030000000000000003\n",
            "Epoch: [67][0/60]\tTime 0.230 (0.230)\tLoss (Class) 0.0023 (0.0023)\tLoss (Regu) 0.0021 (0.0021)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [67][50/60]\tTime 0.040 (0.044)\tLoss (Class) 0.0009 (0.0016)\tLoss (Regu) 0.0008 (0.0015)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.173 (0.173)\tLoss (Class) 0.0139 (0.0139)\tLoss (Regu) 0.0138 (0.0138)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.011 (0.018)\tLoss (Class) 0.0189 (0.0091)\tLoss (Regu) 0.0188 (0.0090)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.011 (0.017)\tLoss (Class) 0.0036 (0.0081)\tLoss (Regu) 0.0034 (0.0080)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.035 (0.017)\tLoss (Class) 0.0022 (0.0082)\tLoss (Regu) 0.0021 (0.0081)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.013 (0.017)\tLoss (Class) 0.0179 (0.0080)\tLoss (Regu) 0.0177 (0.0079)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.002 loss] Val [100.000 %, 100.000%, 0.008 loss] Best: 100.000 %\n",
            "Time for 67 / 90 6.660948753356934\n",
            "Learning rate:  0.00030000000000000003\n",
            "Epoch: [68][0/60]\tTime 0.211 (0.211)\tLoss (Class) 0.0017 (0.0017)\tLoss (Regu) 0.0016 (0.0016)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [68][50/60]\tTime 0.039 (0.042)\tLoss (Class) 0.0019 (0.0017)\tLoss (Regu) 0.0017 (0.0016)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.177 (0.177)\tLoss (Class) 0.0020 (0.0020)\tLoss (Regu) 0.0019 (0.0019)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.012 (0.020)\tLoss (Class) 0.0020 (0.0072)\tLoss (Regu) 0.0019 (0.0071)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.012 (0.018)\tLoss (Class) 0.0011 (0.0096)\tLoss (Regu) 0.0010 (0.0095)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.013 (0.018)\tLoss (Class) 0.0269 (0.0099)\tLoss (Regu) 0.0267 (0.0098)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.032 (0.018)\tLoss (Class) 0.0020 (0.0093)\tLoss (Regu) 0.0019 (0.0092)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.002 loss] Val [100.000 %, 100.000%, 0.010 loss] Best: 100.000 %\n",
            "Time for 68 / 90 6.739479064941406\n",
            "Learning rate:  0.00030000000000000003\n",
            "Epoch: [69][0/60]\tTime 0.210 (0.210)\tLoss (Class) 0.0014 (0.0014)\tLoss (Regu) 0.0012 (0.0012)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [69][50/60]\tTime 0.042 (0.043)\tLoss (Class) 0.0012 (0.0016)\tLoss (Regu) 0.0011 (0.0014)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.173 (0.173)\tLoss (Class) 0.0015 (0.0015)\tLoss (Regu) 0.0014 (0.0014)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.012 (0.020)\tLoss (Class) 0.0030 (0.0129)\tLoss (Regu) 0.0029 (0.0128)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.030 (0.018)\tLoss (Class) 0.0012 (0.0116)\tLoss (Regu) 0.0011 (0.0114)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.012 (0.018)\tLoss (Class) 0.0016 (0.0117)\tLoss (Regu) 0.0015 (0.0115)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.020 (0.017)\tLoss (Class) 0.0035 (0.0114)\tLoss (Regu) 0.0033 (0.0113)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.002 loss] Val [100.000 %, 100.000%, 0.011 loss] Best: 100.000 %\n",
            "Time for 69 / 90 6.765586614608765\n",
            "Learning rate:  0.00030000000000000003\n",
            "Epoch: [70][0/60]\tTime 0.215 (0.215)\tLoss (Class) 0.0017 (0.0017)\tLoss (Regu) 0.0016 (0.0016)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [70][50/60]\tTime 0.038 (0.044)\tLoss (Class) 0.0016 (0.0016)\tLoss (Regu) 0.0015 (0.0015)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.173 (0.173)\tLoss (Class) 0.0392 (0.0392)\tLoss (Regu) 0.0391 (0.0391)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.018 (0.021)\tLoss (Class) 0.0017 (0.0086)\tLoss (Regu) 0.0016 (0.0085)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.012 (0.019)\tLoss (Class) 0.0021 (0.0091)\tLoss (Regu) 0.0019 (0.0090)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.012 (0.018)\tLoss (Class) 0.0203 (0.0086)\tLoss (Regu) 0.0202 (0.0084)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.018 (0.017)\tLoss (Class) 0.0194 (0.0087)\tLoss (Regu) 0.0193 (0.0086)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.002 loss] Val [100.000 %, 100.000%, 0.008 loss] Best: 100.000 %\n",
            "Time for 70 / 90 6.638853311538696\n",
            "Learning rate:  0.00030000000000000003\n",
            "Epoch: [71][0/60]\tTime 0.219 (0.219)\tLoss (Class) 0.0014 (0.0014)\tLoss (Regu) 0.0012 (0.0012)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [71][50/60]\tTime 0.038 (0.044)\tLoss (Class) 0.0011 (0.0015)\tLoss (Regu) 0.0009 (0.0014)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.183 (0.183)\tLoss (Class) 0.0341 (0.0341)\tLoss (Regu) 0.0340 (0.0340)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.027 (0.019)\tLoss (Class) 0.0016 (0.0104)\tLoss (Regu) 0.0015 (0.0103)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.012 (0.017)\tLoss (Class) 0.0022 (0.0113)\tLoss (Regu) 0.0020 (0.0112)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.024 (0.017)\tLoss (Class) 0.0014 (0.0124)\tLoss (Regu) 0.0013 (0.0122)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.013 (0.016)\tLoss (Class) 0.0013 (0.0127)\tLoss (Regu) 0.0012 (0.0126)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.002 loss] Val [100.000 %, 100.000%, 0.013 loss] Best: 100.000 %\n",
            "Time for 71 / 90 6.56687593460083\n",
            "Learning rate:  0.00030000000000000003\n",
            "Epoch: [72][0/60]\tTime 0.208 (0.208)\tLoss (Class) 0.0011 (0.0011)\tLoss (Regu) 0.0010 (0.0010)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [72][50/60]\tTime 0.038 (0.044)\tLoss (Class) 0.0012 (0.0015)\tLoss (Regu) 0.0011 (0.0014)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.175 (0.175)\tLoss (Class) 0.0424 (0.0424)\tLoss (Regu) 0.0423 (0.0423)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.022 (0.019)\tLoss (Class) 0.0021 (0.0140)\tLoss (Regu) 0.0020 (0.0138)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.013 (0.018)\tLoss (Class) 0.0020 (0.0138)\tLoss (Regu) 0.0018 (0.0136)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.013 (0.017)\tLoss (Class) 0.0016 (0.0147)\tLoss (Regu) 0.0015 (0.0146)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.024 (0.017)\tLoss (Class) 0.0013 (0.0151)\tLoss (Regu) 0.0012 (0.0150)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.002 loss] Val [100.000 %, 100.000%, 0.017 loss] Best: 100.000 %\n",
            "Time for 72 / 90 6.580082654953003\n",
            "Learning rate:  0.00030000000000000003\n",
            "Epoch: [73][0/60]\tTime 0.211 (0.211)\tLoss (Class) 0.0018 (0.0018)\tLoss (Regu) 0.0017 (0.0017)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [73][50/60]\tTime 0.037 (0.042)\tLoss (Class) 0.0008 (0.0016)\tLoss (Regu) 0.0007 (0.0014)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.179 (0.179)\tLoss (Class) 0.0022 (0.0022)\tLoss (Regu) 0.0020 (0.0020)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.012 (0.019)\tLoss (Class) 0.0730 (0.0174)\tLoss (Regu) 0.0729 (0.0172)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.011 (0.017)\tLoss (Class) 0.0015 (0.0141)\tLoss (Regu) 0.0014 (0.0139)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.012 (0.017)\tLoss (Class) 0.0778 (0.0151)\tLoss (Regu) 0.0777 (0.0150)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.018 (0.017)\tLoss (Class) 0.0365 (0.0151)\tLoss (Regu) 0.0364 (0.0150)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.002 loss] Val [100.000 %, 100.000%, 0.015 loss] Best: 100.000 %\n",
            "Time for 73 / 90 6.41867995262146\n",
            "Learning rate:  0.00030000000000000003\n",
            "Epoch: [74][0/60]\tTime 0.222 (0.222)\tLoss (Class) 0.0011 (0.0011)\tLoss (Regu) 0.0010 (0.0010)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [74][50/60]\tTime 0.038 (0.042)\tLoss (Class) 0.0014 (0.0015)\tLoss (Regu) 0.0013 (0.0014)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.176 (0.176)\tLoss (Class) 0.0021 (0.0021)\tLoss (Regu) 0.0020 (0.0020)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.011 (0.018)\tLoss (Class) 0.0743 (0.0150)\tLoss (Regu) 0.0742 (0.0148)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.021 (0.017)\tLoss (Class) 0.0336 (0.0145)\tLoss (Regu) 0.0335 (0.0144)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.011 (0.016)\tLoss (Class) 0.0021 (0.0141)\tLoss (Regu) 0.0020 (0.0140)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.012 (0.016)\tLoss (Class) 0.0021 (0.0137)\tLoss (Regu) 0.0019 (0.0136)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.001 loss] Val [100.000 %, 100.000%, 0.013 loss] Best: 100.000 %\n",
            "Time for 74 / 90 6.391395330429077\n",
            "Learning rate:  0.00030000000000000003\n",
            "Epoch: [75][0/60]\tTime 0.226 (0.226)\tLoss (Class) 0.0020 (0.0020)\tLoss (Regu) 0.0019 (0.0019)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [75][50/60]\tTime 0.038 (0.043)\tLoss (Class) 0.0011 (0.0015)\tLoss (Regu) 0.0010 (0.0014)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.176 (0.176)\tLoss (Class) 0.0288 (0.0288)\tLoss (Regu) 0.0286 (0.0286)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.016 (0.020)\tLoss (Class) 0.0022 (0.0118)\tLoss (Regu) 0.0020 (0.0117)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.025 (0.018)\tLoss (Class) 0.0016 (0.0142)\tLoss (Regu) 0.0015 (0.0141)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.012 (0.017)\tLoss (Class) 0.0299 (0.0134)\tLoss (Regu) 0.0298 (0.0132)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.017 (0.017)\tLoss (Class) 0.0021 (0.0124)\tLoss (Regu) 0.0020 (0.0122)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.001 loss] Val [100.000 %, 100.000%, 0.012 loss] Best: 100.000 %\n",
            "Time for 75 / 90 6.642628908157349\n",
            "Learning rate:  0.00030000000000000003\n",
            "Epoch: [76][0/60]\tTime 0.218 (0.218)\tLoss (Class) 0.0035 (0.0035)\tLoss (Regu) 0.0034 (0.0034)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [76][50/60]\tTime 0.039 (0.042)\tLoss (Class) 0.0009 (0.0016)\tLoss (Regu) 0.0008 (0.0014)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.182 (0.182)\tLoss (Class) 0.0115 (0.0115)\tLoss (Regu) 0.0114 (0.0114)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.011 (0.020)\tLoss (Class) 0.0022 (0.0091)\tLoss (Regu) 0.0020 (0.0090)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.013 (0.018)\tLoss (Class) 0.0021 (0.0081)\tLoss (Regu) 0.0020 (0.0080)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.039 (0.018)\tLoss (Class) 0.0021 (0.0073)\tLoss (Regu) 0.0020 (0.0072)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.012 (0.017)\tLoss (Class) 0.0016 (0.0076)\tLoss (Regu) 0.0015 (0.0075)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.002 loss] Val [100.000 %, 100.000%, 0.008 loss] Best: 100.000 %\n",
            "Time for 76 / 90 6.64400315284729\n",
            "Learning rate:  0.00030000000000000003\n",
            "Epoch: [77][0/60]\tTime 0.213 (0.213)\tLoss (Class) 0.0014 (0.0014)\tLoss (Regu) 0.0012 (0.0012)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [77][50/60]\tTime 0.038 (0.041)\tLoss (Class) 0.0011 (0.0015)\tLoss (Regu) 0.0010 (0.0014)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.173 (0.173)\tLoss (Class) 0.0296 (0.0296)\tLoss (Regu) 0.0295 (0.0295)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.013 (0.019)\tLoss (Class) 0.0021 (0.0112)\tLoss (Regu) 0.0020 (0.0111)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.020 (0.018)\tLoss (Class) 0.0016 (0.0116)\tLoss (Regu) 0.0015 (0.0115)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.012 (0.018)\tLoss (Class) 0.0021 (0.0105)\tLoss (Regu) 0.0020 (0.0103)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.011 (0.017)\tLoss (Class) 0.0022 (0.0120)\tLoss (Regu) 0.0020 (0.0119)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.001 loss] Val [100.000 %, 100.000%, 0.011 loss] Best: 100.000 %\n",
            "Time for 77 / 90 6.628472805023193\n",
            "Learning rate:  0.00030000000000000003\n",
            "Epoch: [78][0/60]\tTime 0.217 (0.217)\tLoss (Class) 0.0019 (0.0019)\tLoss (Regu) 0.0018 (0.0018)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [78][50/60]\tTime 0.038 (0.042)\tLoss (Class) 0.0011 (0.0016)\tLoss (Regu) 0.0010 (0.0015)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.180 (0.180)\tLoss (Class) 0.0012 (0.0012)\tLoss (Regu) 0.0011 (0.0011)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.012 (0.019)\tLoss (Class) 0.0012 (0.0107)\tLoss (Regu) 0.0011 (0.0106)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.012 (0.017)\tLoss (Class) 0.0297 (0.0116)\tLoss (Regu) 0.0296 (0.0115)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.012 (0.017)\tLoss (Class) 0.0027 (0.0129)\tLoss (Regu) 0.0026 (0.0128)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.015 (0.017)\tLoss (Class) 0.0351 (0.0127)\tLoss (Regu) 0.0350 (0.0125)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.002 loss] Val [100.000 %, 100.000%, 0.013 loss] Best: 100.000 %\n",
            "Time for 78 / 90 6.473956823348999\n",
            "Learning rate:  0.00030000000000000003\n",
            "Epoch: [79][0/60]\tTime 0.204 (0.204)\tLoss (Class) 0.0011 (0.0011)\tLoss (Regu) 0.0010 (0.0010)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [79][50/60]\tTime 0.038 (0.042)\tLoss (Class) 0.0011 (0.0016)\tLoss (Regu) 0.0009 (0.0015)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.181 (0.181)\tLoss (Class) 0.0145 (0.0145)\tLoss (Regu) 0.0144 (0.0144)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.030 (0.020)\tLoss (Class) 0.0020 (0.0059)\tLoss (Regu) 0.0019 (0.0057)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.014 (0.019)\tLoss (Class) 0.0135 (0.0061)\tLoss (Regu) 0.0134 (0.0060)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.022 (0.018)\tLoss (Class) 0.0103 (0.0064)\tLoss (Regu) 0.0101 (0.0063)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.011 (0.018)\tLoss (Class) 0.0013 (0.0058)\tLoss (Regu) 0.0012 (0.0057)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.002 loss] Val [100.000 %, 100.000%, 0.006 loss] Best: 100.000 %\n",
            "Time for 79 / 90 6.660553932189941\n",
            "Learning rate:  0.00030000000000000003\n",
            "Epoch: [80][0/60]\tTime 0.215 (0.215)\tLoss (Class) 0.0009 (0.0009)\tLoss (Regu) 0.0008 (0.0008)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [80][50/60]\tTime 0.038 (0.042)\tLoss (Class) 0.0014 (0.0015)\tLoss (Regu) 0.0012 (0.0014)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.179 (0.179)\tLoss (Class) 0.0021 (0.0021)\tLoss (Regu) 0.0020 (0.0020)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.013 (0.020)\tLoss (Class) 0.0021 (0.0095)\tLoss (Regu) 0.0019 (0.0094)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.031 (0.018)\tLoss (Class) 0.0022 (0.0102)\tLoss (Regu) 0.0021 (0.0101)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.011 (0.017)\tLoss (Class) 0.0021 (0.0105)\tLoss (Regu) 0.0020 (0.0104)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.012 (0.017)\tLoss (Class) 0.0021 (0.0106)\tLoss (Regu) 0.0019 (0.0105)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.001 loss] Val [100.000 %, 100.000%, 0.011 loss] Best: 100.000 %\n",
            "Time for 80 / 90 6.588146448135376\n",
            "Learning rate:  0.00030000000000000003\n",
            "Epoch: [81][0/60]\tTime 0.207 (0.207)\tLoss (Class) 0.0010 (0.0010)\tLoss (Regu) 0.0009 (0.0009)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [81][50/60]\tTime 0.038 (0.044)\tLoss (Class) 0.0011 (0.0015)\tLoss (Regu) 0.0010 (0.0014)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.172 (0.172)\tLoss (Class) 0.0648 (0.0648)\tLoss (Regu) 0.0647 (0.0647)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.026 (0.019)\tLoss (Class) 0.0021 (0.0147)\tLoss (Regu) 0.0020 (0.0145)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.013 (0.018)\tLoss (Class) 0.0597 (0.0141)\tLoss (Regu) 0.0595 (0.0140)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.025 (0.018)\tLoss (Class) 0.0020 (0.0139)\tLoss (Regu) 0.0019 (0.0138)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.012 (0.017)\tLoss (Class) 0.0013 (0.0136)\tLoss (Regu) 0.0012 (0.0135)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.001 loss] Val [100.000 %, 100.000%, 0.013 loss] Best: 100.000 %\n",
            "Time for 81 / 90 6.730925559997559\n",
            "Learning rate:  0.00030000000000000003\n",
            "Epoch: [82][0/60]\tTime 0.214 (0.214)\tLoss (Class) 0.0013 (0.0013)\tLoss (Regu) 0.0011 (0.0011)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [82][50/60]\tTime 0.041 (0.044)\tLoss (Class) 0.0025 (0.0016)\tLoss (Regu) 0.0024 (0.0015)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.181 (0.181)\tLoss (Class) 0.0306 (0.0306)\tLoss (Regu) 0.0305 (0.0305)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.011 (0.019)\tLoss (Class) 0.0021 (0.0144)\tLoss (Regu) 0.0020 (0.0143)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.012 (0.018)\tLoss (Class) 0.0309 (0.0120)\tLoss (Regu) 0.0308 (0.0119)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.012 (0.018)\tLoss (Class) 0.0297 (0.0133)\tLoss (Regu) 0.0296 (0.0132)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.013 (0.018)\tLoss (Class) 0.0335 (0.0125)\tLoss (Regu) 0.0334 (0.0124)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.002 loss] Val [100.000 %, 100.000%, 0.012 loss] Best: 100.000 %\n",
            "Time for 82 / 90 6.886134147644043\n",
            "Learning rate:  0.00030000000000000003\n",
            "Epoch: [83][0/60]\tTime 0.209 (0.209)\tLoss (Class) 0.0008 (0.0008)\tLoss (Regu) 0.0007 (0.0007)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [83][50/60]\tTime 0.037 (0.041)\tLoss (Class) 0.0042 (0.0015)\tLoss (Regu) 0.0041 (0.0014)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.173 (0.173)\tLoss (Class) 0.0016 (0.0016)\tLoss (Regu) 0.0015 (0.0015)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.013 (0.020)\tLoss (Class) 0.0211 (0.0069)\tLoss (Regu) 0.0210 (0.0068)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.032 (0.019)\tLoss (Class) 0.0204 (0.0082)\tLoss (Regu) 0.0203 (0.0081)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.011 (0.018)\tLoss (Class) 0.0214 (0.0087)\tLoss (Regu) 0.0213 (0.0085)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.017 (0.017)\tLoss (Class) 0.0020 (0.0089)\tLoss (Regu) 0.0018 (0.0088)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.002 loss] Val [100.000 %, 100.000%, 0.009 loss] Best: 100.000 %\n",
            "Time for 83 / 90 6.516384124755859\n",
            "Learning rate:  0.00030000000000000003\n",
            "Epoch: [84][0/60]\tTime 0.219 (0.219)\tLoss (Class) 0.0012 (0.0012)\tLoss (Regu) 0.0011 (0.0011)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [84][50/60]\tTime 0.037 (0.041)\tLoss (Class) 0.0015 (0.0015)\tLoss (Regu) 0.0013 (0.0014)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.179 (0.179)\tLoss (Class) 0.0619 (0.0619)\tLoss (Regu) 0.0618 (0.0618)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.012 (0.019)\tLoss (Class) 0.0020 (0.0121)\tLoss (Regu) 0.0019 (0.0119)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.016 (0.018)\tLoss (Class) 0.0020 (0.0119)\tLoss (Regu) 0.0019 (0.0117)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.011 (0.017)\tLoss (Class) 0.0020 (0.0120)\tLoss (Regu) 0.0019 (0.0119)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.018 (0.017)\tLoss (Class) 0.0616 (0.0122)\tLoss (Regu) 0.0615 (0.0120)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.002 loss] Val [100.000 %, 100.000%, 0.013 loss] Best: 100.000 %\n",
            "Time for 84 / 90 6.424069881439209\n",
            "Learning rate:  0.00030000000000000003\n",
            "Epoch: [85][0/60]\tTime 0.217 (0.217)\tLoss (Class) 0.0011 (0.0011)\tLoss (Regu) 0.0010 (0.0010)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [85][50/60]\tTime 0.039 (0.042)\tLoss (Class) 0.0012 (0.0014)\tLoss (Regu) 0.0011 (0.0013)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.179 (0.179)\tLoss (Class) 0.0362 (0.0362)\tLoss (Regu) 0.0361 (0.0361)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.011 (0.018)\tLoss (Class) 0.0326 (0.0162)\tLoss (Regu) 0.0325 (0.0161)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.011 (0.017)\tLoss (Class) 0.0021 (0.0147)\tLoss (Regu) 0.0019 (0.0146)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.011 (0.016)\tLoss (Class) 0.0653 (0.0142)\tLoss (Regu) 0.0652 (0.0140)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.015 (0.016)\tLoss (Class) 0.0020 (0.0132)\tLoss (Regu) 0.0019 (0.0131)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.001 loss] Val [100.000 %, 100.000%, 0.013 loss] Best: 100.000 %\n",
            "Time for 85 / 90 6.349266767501831\n",
            "Learning rate:  0.00030000000000000003\n",
            "Epoch: [86][0/60]\tTime 0.228 (0.228)\tLoss (Class) 0.0019 (0.0019)\tLoss (Regu) 0.0018 (0.0018)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [86][50/60]\tTime 0.044 (0.045)\tLoss (Class) 0.0015 (0.0013)\tLoss (Regu) 0.0014 (0.0012)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.177 (0.177)\tLoss (Class) 0.0020 (0.0020)\tLoss (Regu) 0.0019 (0.0019)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.014 (0.021)\tLoss (Class) 0.0021 (0.0118)\tLoss (Regu) 0.0020 (0.0117)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.018 (0.019)\tLoss (Class) 0.0020 (0.0115)\tLoss (Regu) 0.0019 (0.0114)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.012 (0.018)\tLoss (Class) 0.0021 (0.0110)\tLoss (Regu) 0.0020 (0.0109)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.011 (0.018)\tLoss (Class) 0.0254 (0.0107)\tLoss (Regu) 0.0253 (0.0106)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.001 loss] Val [100.000 %, 100.000%, 0.011 loss] Best: 100.000 %\n",
            "Time for 86 / 90 6.944092035293579\n",
            "Learning rate:  0.00030000000000000003\n",
            "Epoch: [87][0/60]\tTime 0.210 (0.210)\tLoss (Class) 0.0009 (0.0009)\tLoss (Regu) 0.0007 (0.0007)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [87][50/60]\tTime 0.038 (0.044)\tLoss (Class) 0.0013 (0.0013)\tLoss (Regu) 0.0012 (0.0012)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.179 (0.179)\tLoss (Class) 0.0020 (0.0020)\tLoss (Regu) 0.0019 (0.0019)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.012 (0.019)\tLoss (Class) 0.0012 (0.0153)\tLoss (Regu) 0.0010 (0.0151)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.031 (0.018)\tLoss (Class) 0.0050 (0.0150)\tLoss (Regu) 0.0049 (0.0149)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.012 (0.017)\tLoss (Class) 0.0496 (0.0159)\tLoss (Regu) 0.0494 (0.0157)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.028 (0.017)\tLoss (Class) 0.0053 (0.0163)\tLoss (Regu) 0.0052 (0.0162)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.001 loss] Val [100.000 %, 100.000%, 0.016 loss] Best: 100.000 %\n",
            "Time for 87 / 90 6.647123336791992\n",
            "Learning rate:  0.00030000000000000003\n",
            "Epoch: [88][0/60]\tTime 0.222 (0.222)\tLoss (Class) 0.0010 (0.0010)\tLoss (Regu) 0.0009 (0.0009)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [88][50/60]\tTime 0.040 (0.044)\tLoss (Class) 0.0007 (0.0014)\tLoss (Regu) 0.0006 (0.0013)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.177 (0.177)\tLoss (Class) 0.0015 (0.0015)\tLoss (Regu) 0.0014 (0.0014)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.013 (0.020)\tLoss (Class) 0.0249 (0.0093)\tLoss (Regu) 0.0248 (0.0092)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.012 (0.019)\tLoss (Class) 0.0021 (0.0109)\tLoss (Regu) 0.0019 (0.0107)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.012 (0.018)\tLoss (Class) 0.0248 (0.0111)\tLoss (Regu) 0.0247 (0.0110)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.013 (0.017)\tLoss (Class) 0.0245 (0.0108)\tLoss (Regu) 0.0244 (0.0107)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.001 loss] Val [100.000 %, 100.000%, 0.011 loss] Best: 100.000 %\n",
            "Time for 88 / 90 6.827299118041992\n",
            "Learning rate:  0.00030000000000000003\n",
            "Epoch: [89][0/60]\tTime 0.205 (0.205)\tLoss (Class) 0.0009 (0.0009)\tLoss (Regu) 0.0008 (0.0008)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Epoch: [89][50/60]\tTime 0.042 (0.044)\tLoss (Class) 0.0009 (0.0014)\tLoss (Regu) 0.0008 (0.0012)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [0/238]\tTime 0.176 (0.176)\tLoss (Class) 0.0021 (0.0021)\tLoss (Regu) 0.0020 (0.0020)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/238]\tTime 0.011 (0.020)\tLoss (Class) 0.0687 (0.0124)\tLoss (Regu) 0.0686 (0.0123)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [100/238]\tTime 0.026 (0.018)\tLoss (Class) 0.0019 (0.0121)\tLoss (Regu) 0.0018 (0.0120)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [150/238]\tTime 0.012 (0.017)\tLoss (Class) 0.0019 (0.0123)\tLoss (Regu) 0.0018 (0.0121)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [200/238]\tTime 0.019 (0.017)\tLoss (Class) 0.0020 (0.0129)\tLoss (Regu) 0.0019 (0.0128)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            " * Train[100.000 %, 100.000 %, 0.001 loss] Val [100.000 %, 100.000%, 0.013 loss] Best: 100.000 %\n",
            "Time for 89 / 90 6.680680274963379\n",
            "Best accuracy:  100.0\n"
          ]
        }
      ],
      "source": [
        "# set env var in this shell\n",
        "!export KTH_DATA_DIR=/content/DAWN_WACV2020/data\n",
        "\n",
        "# now run the exact KTH command from the README\n",
        "!python train.py --epochs 90 -b 16 --lr 0.03 --name dwnn_kth_3_dwnn_l5_16 --lrdecay 30 60 --database kth --traindir $KTH_DATA_DIR/KTH-TIPS2-b3/Test/ --valdir $KTH_DATA_DIR/KTH-TIPS2-b3/Train/ dawn --levels 5 --first_conv 16 --regu_details 0.1 --regu_approx 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ktqHAG9VNnle",
        "outputId": "15591369-6953-4577-f8ab-a7f5dcd29fbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ignore tensorboard logger\n",
            "Eval-only script launched\n",
            "Args: ['eval.py', '--database', 'kth', '--num_clases', '11', '--batch-size', '16', '--traindir', '/content/DAWN_WACV2020/data/KTH-TIPS2-b3/Train', '--valdir', '/content/DAWN_WACV2020/data/KTH-TIPS2-b3/Test', '--name', 'dwnn_kth_3_dwnn_l5_16', '--eval_only', '--eval_ckpt', '/content/DAWN_WACV2020/runs/dwnn_kth_3_dwnn_l5_16/model_best.pth.tar', 'dawn', '--levels', '5', '--first_conv', '16', '--regu_details', '0.1', '--regu_approx', '0.1']\n",
            "DAWN:\n",
            "- first conv: 16\n",
            "- image size: 224\n",
            "- nb levels : 5\n",
            "- levels U/P: [2, 1]\n",
            "- channels:  3\n",
            "Final channel: 256\n",
            "Final size   : 7\n",
            "Number of model parameters: 68667\n",
            "Number of trainable parameters: 68667\n",
            "=> loading checkpoint '/content/DAWN_WACV2020/runs/dwnn_kth_3_dwnn_l5_16/model_best.pth.tar'\n",
            "=> loaded checkpoint '/content/DAWN_WACV2020/runs/dwnn_kth_3_dwnn_l5_16/model_best.pth.tar' (epoch 1, best_prec1=100.000)\n",
            "Test: [0/60]\tTime 0.733 (0.733)\tLoss (Class) 0.4108 (0.4108)\tLoss (Regu) 0.4022 (0.4022)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Test: [50/60]\tTime 0.013 (0.029)\tLoss (Class) 0.6617 (0.5403)\tLoss (Regu) 0.6497 (0.5305)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "Evaluation done.\n",
            "Prec@1: 100.000 %, Prec@5: 100.000 %, Loss: 0.5342\n"
          ]
        }
      ],
      "source": [
        "!python eval.py \\\n",
        "--database kth \\\n",
        "--num_clases 11 \\\n",
        "--batch-size 16 \\\n",
        "--traindir /content/DAWN_WACV2020/data/KTH-TIPS2-b3/Train \\\n",
        "--valdir /content/DAWN_WACV2020/data/KTH-TIPS2-b3/Test \\\n",
        "--name dwnn_kth_3_dwnn_l5_16 \\\n",
        "--eval_only \\\n",
        "--eval_ckpt /content/DAWN_WACV2020/runs/dwnn_kth_3_dwnn_l5_16/model_best.pth.tar \\\n",
        "dawn \\\n",
        "--levels 5 \\\n",
        "--first_conv 16 \\\n",
        "--regu_details 0.1 \\\n",
        "--regu_approx 0.1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "67IiRvE4NnhL",
        "outputId": "71ce7c7d-9b3d-4318-c9e1-e7f20edcc499"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ignore tensorboard logger\n",
            "Eval-only script launched\n",
            "Args: ['eval.py', '--database', 'cifar-100', '--batch-size', '64', '--name', 'dawn_cifar100_eval', '--eval_only', '--eval_ckpt', '/content/DAWN_WACV2020/runs/dwnn_cifar-100_32/model_best.pth.tar', 'dawn', '--levels', '3', '--first_conv', '128', '--regu_details', '0.1', '--regu_approx', '0.1']\n",
            "DAWN:\n",
            "- first conv: 128\n",
            "- image size: 32\n",
            "- nb levels : 3\n",
            "- levels U/P: [2, 1]\n",
            "- channels:  3\n",
            "Final channel: 1280\n",
            "Final size   : 4\n",
            "Number of model parameters: 2646500\n",
            "Number of trainable parameters: 2646500\n",
            "=> loading checkpoint '/content/DAWN_WACV2020/runs/dwnn_cifar-100_32/model_best.pth.tar'\n",
            "=> loaded checkpoint '/content/DAWN_WACV2020/runs/dwnn_cifar-100_32/model_best.pth.tar' (epoch 136, best_prec1=66.040)\n",
            "Test: [0/157]\tTime 0.597 (0.597)\tLoss (Class) 1.4193 (1.4193)\tLoss (Regu) 0.2103 (0.2103)\tPrec@1 67.188 (67.188)\tPrec@5 92.188 (92.188)\n",
            "Test: [50/157]\tTime 0.008 (0.020)\tLoss (Class) 1.6353 (1.6657)\tLoss (Regu) 0.2185 (0.2159)\tPrec@1 68.750 (66.268)\tPrec@5 95.312 (89.675)\n",
            "Test: [100/157]\tTime 0.008 (0.014)\tLoss (Class) 1.2321 (1.6532)\tLoss (Regu) 0.2166 (0.2163)\tPrec@1 68.750 (66.043)\tPrec@5 95.312 (89.619)\n",
            "Test: [150/157]\tTime 0.007 (0.012)\tLoss (Class) 1.1553 (1.6428)\tLoss (Regu) 0.2123 (0.2165)\tPrec@1 70.312 (66.049)\tPrec@5 90.625 (89.683)\n",
            "Evaluation done.\n",
            "Prec@1: 66.030 %, Prec@5: 89.680 %, Loss: 1.6407\n"
          ]
        }
      ],
      "source": [
        "!python eval.py \\\n",
        "--database cifar-100 \\\n",
        "--batch-size 64 \\\n",
        "--name dawn_cifar100_eval \\\n",
        "--eval_only \\\n",
        "--eval_ckpt /content/DAWN_WACV2020/runs/dwnn_cifar-100_32/model_best.pth.tar \\\n",
        "dawn \\\n",
        "--levels 3 \\\n",
        "--first_conv 128 \\\n",
        "--regu_details 0.1 \\\n",
        "--regu_approx 0.1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnzpXxzzNneL",
        "outputId": "bef0df44-2574-4ea8-93c0-53fb72a4a9ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['epoch', 'state_dict', 'best_prec1'])\n"
          ]
        }
      ],
      "source": [
        "checkpoint_path=\"/content/DAWN_WACV2020/runs/dwnn_cifar-100_32/model_best.pth.tar\"\n",
        "\n",
        "checkpoint = torch.load(checkpoint_path)\n",
        "print(checkpoint.keys())\n",
        "# Common keys: 'epoch', 'state_dict', 'optimizer', 'best_prec1', etc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mF-CpcENnbY",
        "outputId": "b00b6b64-06b0-4bad-c5b8-fec5be8e81b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DAWN:\n",
            "- first conv: 128\n",
            "- image size: 224\n",
            "- nb levels : 3\n",
            "- levels U/P: [2, 1]\n",
            "- channels:  3\n",
            "Final channel: 1280\n",
            "Final size   : 28\n"
          ]
        }
      ],
      "source": [
        "import models.dawn as dawn\n",
        "\n",
        "NUM_CLASS = 100\n",
        "first_conv = 128\n",
        "number_levels = 3\n",
        "regu_details = 0.1\n",
        "regu_approx = 0.1\n",
        "kernel_size = 3  # default if unchanged\n",
        "classifier = 'mode1'  # default\n",
        "no_bootleneck = False\n",
        "share_weights = False\n",
        "simple_lifting = False\n",
        "haar_wavelet = False\n",
        "COLOR = True  # Because CIFAR-100 is color images\n",
        "\n",
        "model = dawn.DAWN(\n",
        "    NUM_CLASS,\n",
        "    big_input=True,     # CIFAR input size 32 may count as small or large input depending on your code (check INPUT_SIZE)\n",
        "    first_conv=first_conv,\n",
        "    number_levels=number_levels,\n",
        "    kernel_size=kernel_size,\n",
        "    no_bootleneck=no_bootleneck,\n",
        "    classifier=classifier,\n",
        "    share_weights=share_weights,\n",
        "    simple_lifting=simple_lifting,\n",
        "    COLOR=COLOR,\n",
        "    regu_details=regu_details,\n",
        "    regu_approx=regu_approx,\n",
        "    haar_wavelet=haar_wavelet\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEPCRlUiZutF",
        "outputId": "875e6e6e-deaf-4110-fbe0-00d4d899848c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "state_dict = checkpoint['state_dict']\n",
        "from collections import OrderedDict\n",
        "new_state_dict = OrderedDict()\n",
        "for k, v in state_dict.items():\n",
        "    name = k\n",
        "    if k.startswith('module.'):\n",
        "        name = k[7:]  # remove 'module.' prefix\n",
        "    new_state_dict[name] = v\n",
        "model.load_state_dict(new_state_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cP6rmAX-u563"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!cp -r /content/myfolder /content/drive/MyDrive/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v19VXbhQwZqO",
        "outputId": "d8c419d0-7917-4a45-94dc-8b90c76a0b44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "0O9hBaB2u53b"
      },
      "outputs": [],
      "source": [
        "!cp -r /content/DAWN_WACV2020 /content/drive/MyDrive/ECE_251C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-72JiTrhu50u"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "trial",
      "language": "python",
      "name": "trial"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
